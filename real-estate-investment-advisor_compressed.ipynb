{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5b25206",
   "metadata": {
    "papermill": {
     "duration": 0.023353,
     "end_time": "2025-12-07T12:15:18.067707",
     "exception": false,
     "start_time": "2025-12-07T12:15:18.044354",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Project Name**    - Real Estate Investment Advisor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1104df",
   "metadata": {
    "papermill": {
     "duration": 0.015928,
     "end_time": "2025-12-07T12:15:18.138176",
     "exception": false,
     "start_time": "2025-12-07T12:15:18.122248",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Project Summary -**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa79dffc",
   "metadata": {},
   "source": [
    "The Real Estate Investment Advisor project is designed to assist investors in making informed property decisions using machine learning–driven insights. The system integrates both classification and regression techniques to evaluate whether a property is a good investment and to predict its estimated future value over the next five years. By combining model predictions with interactive visual analytics, the project offers a comprehensive decision-support solution tailored for real estate buyers, investors, and financial planners.\n",
    "\n",
    "The first component focuses on **investment classification**. Using a wide set of features—such as property characteristics, market indicators, location-specific metrics, and historical valuations—the classification model determines whether a property qualifies as a *Good Investment* or *Not Recommended*. The model undergoes feature engineering, multicollinearity checks, data cleaning, and rigorous machine learning experimentation across algorithms like Random Forest, XGBoost, and Logistic Regression. The output assists users in quickly identifying investment-worthy properties.\n",
    "\n",
    "The second component performs **future price prediction**. This regression model uses the property’s current price along with relevant market and property-level variables to estimate its price after five years. Users can assess long-term appreciation potential and compare expected growth with other investment options. The regression pipeline includes scaling, hyperparameter tuning, cross-validation, and error analysis through metrics like RMSE, MAE, and R².\n",
    "\n",
    "To make the project accessible to end users, a **Streamlit application** is developed as an interactive interface. The app encapsulates the full prediction workflow through four dedicated tabs:\n",
    "\n",
    "1. **Single Prediction** – Users input property details manually and instantly receive investment category and future price predictions.\n",
    "2. **Bulk Prediction** – Allows uploading datasets (CSV files) for batch processing, generating outputs for multiple properties at once.\n",
    "3. **Visualization (Market Insights)** – Offers charts illustrating market trends, distribution patterns, correlations, price behavior, and geographic or property-based insights, enabling users to interpret the model's decisions better.\n",
    "4. **Feature Importance & SHAP Analysis** – Displays model interpretability results. Users can view which features contribute most to the predictions and use SHAP plots for a transparent, trustworthy prediction process.\n",
    "\n",
    "Overall, this project functions as a complete real estate decision-support ecosystem that blends predictive analytics, interpretability, and a user-friendly interface, empowering users to make smarter, data-backed investment choices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94574683",
   "metadata": {
    "papermill": {
     "duration": 0.026177,
     "end_time": "2025-12-07T12:15:18.253699",
     "exception": false,
     "start_time": "2025-12-07T12:15:18.227522",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Problem Statement**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cef354",
   "metadata": {},
   "source": [
    "1. Difficulty in identifying whether a property is a good investment due to lack of objective, data-driven evaluation methods.\n",
    "2. Absence of a reliable system to predict future property prices based on current market conditions.\n",
    "3. No integrated platform that supports single prediction, bulk prediction, visual insights, and model interpretability in one place.\n",
    "4. Limited transparency in machine learning models, making it hard for users to understand why a prediction was made.\n",
    "5. Challenges in processing and analyzing large volumes of real estate data efficiently for investment decisions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef72d795",
   "metadata": {
    "papermill": {
     "duration": 0.020919,
     "end_time": "2025-12-07T12:15:18.294443",
     "exception": false,
     "start_time": "2025-12-07T12:15:18.273524",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ***Let's Begin !***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09315f2c",
   "metadata": {
    "papermill": {
     "duration": 0.015862,
     "end_time": "2025-12-07T12:15:18.326419",
     "exception": false,
     "start_time": "2025-12-07T12:15:18.310557",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ***1. Know Your Data***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230fa967",
   "metadata": {
    "papermill": {
     "duration": 0.015932,
     "end_time": "2025-12-07T12:15:18.358315",
     "exception": false,
     "start_time": "2025-12-07T12:15:18.342383",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de0ef18",
   "metadata": {
    "papermill": {
     "duration": 13.89076,
     "end_time": "2025-12-07T12:15:59.371531",
     "exception": false,
     "start_time": "2025-12-07T12:15:45.480771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Stats Tests\n",
    "from scipy.stats import f_oneway, pearsonr\n",
    "\n",
    "# Models\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "\n",
    "# Imbalanced Data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Viz\n",
    "import shap\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Display\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31fbc78",
   "metadata": {
    "papermill": {
     "duration": 0.023297,
     "end_time": "2025-12-07T12:15:59.419653",
     "exception": false,
     "start_time": "2025-12-07T12:15:59.396356",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d82473b",
   "metadata": {
    "papermill": {
     "duration": 1.279493,
     "end_time": "2025-12-07T12:16:00.722929",
     "exception": false,
     "start_time": "2025-12-07T12:15:59.443436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "df = pd.read_csv(r\"india_housing_prices.csv\")\n",
    "print(\"Dataset loaded successfully.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76088c15",
   "metadata": {
    "papermill": {
     "duration": 0.022787,
     "end_time": "2025-12-07T12:16:00.769367",
     "exception": false,
     "start_time": "2025-12-07T12:16:00.746580",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Dataset First View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783b0719",
   "metadata": {
    "papermill": {
     "duration": 0.063302,
     "end_time": "2025-12-07T12:16:00.855717",
     "exception": false,
     "start_time": "2025-12-07T12:16:00.792415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset First Look\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e4212a",
   "metadata": {
    "papermill": {
     "duration": 0.02339,
     "end_time": "2025-12-07T12:16:00.903038",
     "exception": false,
     "start_time": "2025-12-07T12:16:00.879648",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Dataset Rows & Columns count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a0b9c5",
   "metadata": {
    "papermill": {
     "duration": 0.03121,
     "end_time": "2025-12-07T12:16:00.957701",
     "exception": false,
     "start_time": "2025-12-07T12:16:00.926491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset Rows & Columns count\n",
    "print(f\"Number of Rows: {df.shape[0]}\")\n",
    "print(f\"Number of Columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12085a4",
   "metadata": {
    "papermill": {
     "duration": 0.023551,
     "end_time": "2025-12-07T12:16:01.005331",
     "exception": false,
     "start_time": "2025-12-07T12:16:00.981780",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2af889",
   "metadata": {
    "papermill": {
     "duration": 0.198587,
     "end_time": "2025-12-07T12:16:01.227217",
     "exception": false,
     "start_time": "2025-12-07T12:16:01.028630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset Info\n",
    "print(\"Dataset Info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d45595",
   "metadata": {
    "papermill": {
     "duration": 0.023708,
     "end_time": "2025-12-07T12:16:01.274256",
     "exception": false,
     "start_time": "2025-12-07T12:16:01.250548",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Duplicate Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fabc64",
   "metadata": {
    "papermill": {
     "duration": 0.292315,
     "end_time": "2025-12-07T12:16:01.590413",
     "exception": false,
     "start_time": "2025-12-07T12:16:01.298098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset Duplicate Value Count\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Number of Duplicate Rows: {duplicate_count}\")\n",
    "\n",
    "# Optional: Remove duplicates\n",
    "if duplicate_count > 0:\n",
    "    df = df.drop_duplicates().reset_index(drop=True)\n",
    "    print(\"Duplicates removed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580e74fa",
   "metadata": {
    "papermill": {
     "duration": 0.023804,
     "end_time": "2025-12-07T12:16:01.638223",
     "exception": false,
     "start_time": "2025-12-07T12:16:01.614419",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Missing Values/Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de53569",
   "metadata": {
    "papermill": {
     "duration": 0.194446,
     "end_time": "2025-12-07T12:16:01.856497",
     "exception": false,
     "start_time": "2025-12-07T12:16:01.662051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Missing Values/Null Values Count\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame({'Missing Values': missing_values, 'Percentage': missing_percent})\n",
    "print(\"Missing Values Summary:\")\n",
    "display(missing_df[missing_df['Missing Values'] > 0].sort_values(by='Missing Values', ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a45662",
   "metadata": {
    "papermill": {
     "duration": 0.023755,
     "end_time": "2025-12-07T12:16:01.951890",
     "exception": false,
     "start_time": "2025-12-07T12:16:01.928135",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ***2. Understanding Your Variables***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02818419",
   "metadata": {
    "papermill": {
     "duration": 0.031239,
     "end_time": "2025-12-07T12:16:02.006969",
     "exception": false,
     "start_time": "2025-12-07T12:16:01.975730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset Columns\n",
    "print(\"Dataset Columns:\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89f6c5d",
   "metadata": {
    "papermill": {
     "duration": 0.148823,
     "end_time": "2025-12-07T12:16:02.179773",
     "exception": false,
     "start_time": "2025-12-07T12:16:02.030950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset Describe \n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bab09f",
   "metadata": {
    "papermill": {
     "duration": 0.024087,
     "end_time": "2025-12-07T12:16:02.229582",
     "exception": false,
     "start_time": "2025-12-07T12:16:02.205495",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Variables Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee61b6cc",
   "metadata": {
    "papermill": {
     "duration": 0.024101,
     "end_time": "2025-12-07T12:16:02.277760",
     "exception": false,
     "start_time": "2025-12-07T12:16:02.253659",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "| **Feature Name**                                | **Description**                                                               |\n",
    "| ----------------------------------------------- | ----------------------------------------------------------------------------- |\n",
    "| **ID**                                          | Unique identifier assigned to each property record.                           |\n",
    "| **State**                                       | State in India where the property is located.                                 |\n",
    "| **City**                                        | City of the property listing.                                                 |\n",
    "| **Locality**                                    | Specific neighborhood or locality within the city.                            |\n",
    "| **Property_Type**                               | Type of property (Apartment, Villa, Independent House, Studio, etc.).         |\n",
    "| **BHK**                                         | Count of Bedrooms, Hall, and Kitchen (e.g., 1BHK, 2BHK, 3BHK).                |\n",
    "| **Size_in_SqFt**                                | Built-up area of the property in square feet.                                 |\n",
    "| **Price_in_Lakhs**                              | Listed price of the property in lakhs (₹).                                    |\n",
    "| **Price_per_SqFt**                              | Normalized metric calculated as *Price / Size*, indicating price efficiency.  |\n",
    "| **Year_Built**                                  | Year in which the property was constructed.                                   |\n",
    "| **Furnished_Status**                            | Furnishing level: Unfurnished, Semi-Furnished, or Fully Furnished.            |\n",
    "| **Floor_No**                                    | Floor number where the property is situated.                                  |\n",
    "| **Total_Floors**                                | Total number of floors in the building.                                       |\n",
    "| **Age_of_Property**                             | Age of the property (Current Year − Year Built).                              |\n",
    "| **Nearby_Schools**                              | Count of nearby schools or school rating score.                               |\n",
    "| **Nearby_Hospitals**                            | Number of nearby hospitals or healthcare centers.                             |\n",
    "| **Public_Transport_Accessibility**              | Availability of transport options (Bus stop, Metro, Train).                   |\n",
    "| **Parking_Space**                               | Number of parking slots provided with the property.                           |\n",
    "| **Security**                                    | Security features (e.g., Gated Community, CCTV, Guards).                      |\n",
    "| **Amenities**                                   | Available amenities such as Gym, Pool, Clubhouse, Garden, etc.                |\n",
    "| **Facing**                                      | Direction the property faces (North, East, West, South).                      |\n",
    "| **Owner_Type**                                  | Type of owner listing the property (Individual, Builder, Agent).              |\n",
    "| **Availability_Status**                         | Current availability (Available, Sold, Under Construction).                   |\n",
    "| **Good_Investment** *(Target – Classification)* | Binary label indicating whether the property is considered a good investment. |\n",
    "| **Future_Price_5Y** *(Target – Regression)*     | Predicted price of the property after 5 years.                                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bd49a3",
   "metadata": {
    "papermill": {
     "duration": 0.024129,
     "end_time": "2025-12-07T12:16:02.326058",
     "exception": false,
     "start_time": "2025-12-07T12:16:02.301929",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Check Unique Values for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f853b282",
   "metadata": {
    "papermill": {
     "duration": 0.240881,
     "end_time": "2025-12-07T12:16:02.590973",
     "exception": false,
     "start_time": "2025-12-07T12:16:02.350092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check Unique Values for each variable.\n",
    "unique_counts = df.nunique().sort_values(ascending=False)\n",
    "print(\"Unique Value Count per Column:\")\n",
    "display(unique_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2337cb",
   "metadata": {
    "papermill": {
     "duration": 0.024132,
     "end_time": "2025-12-07T12:16:02.639833",
     "exception": false,
     "start_time": "2025-12-07T12:16:02.615701",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. ***Data Wrangling***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b213551a",
   "metadata": {
    "papermill": {
     "duration": 0.024505,
     "end_time": "2025-12-07T12:16:02.688571",
     "exception": false,
     "start_time": "2025-12-07T12:16:02.664066",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Data Wrangling Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e615de2f",
   "metadata": {
    "papermill": {
     "duration": 0.099801,
     "end_time": "2025-12-07T12:16:02.812675",
     "exception": false,
     "start_time": "2025-12-07T12:16:02.712874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3 — DATA WRANGLING (REAL ESTATE SPECIFIC)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_clean = df.copy()\n",
    "print(f\"Initial shape: {df_clean.shape}\")\n",
    "\n",
    "# STEP 1: DROP IDENTIFIER COLUMNS\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"DROPPING IDENTIFIER COLUMNS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "id_columns = ['ID']\n",
    "id_columns_present = [col for col in id_columns if col in df_clean.columns]\n",
    "\n",
    "if id_columns_present:\n",
    "    df_clean = df_clean.drop(id_columns_present, axis=1)\n",
    "    print(f\"✓ Dropped: {id_columns_present}\")\n",
    "    print(f\"✓ New shape: {df_clean.shape}\")\n",
    "\n",
    "print(\"\\n✓ Data Wrangling Complete!\")\n",
    "print(f\"Final dataset shape: {df_clean.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897e0e40",
   "metadata": {
    "papermill": {
     "duration": 0.024677,
     "end_time": "2025-12-07T12:16:02.862425",
     "exception": false,
     "start_time": "2025-12-07T12:16:02.837748",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### What all manipulations have you done and insights you found?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f9762b",
   "metadata": {
    "papermill": {
     "duration": 0.024541,
     "end_time": "2025-12-07T12:16:02.911306",
     "exception": false,
     "start_time": "2025-12-07T12:16:02.886765",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d78194",
   "metadata": {
    "papermill": {
     "duration": 0.025095,
     "end_time": "2025-12-07T12:16:02.960811",
     "exception": false,
     "start_time": "2025-12-07T12:16:02.935716",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Chart - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaf8f8e",
   "metadata": {
    "papermill": {
     "duration": 2.533506,
     "end_time": "2025-12-07T12:16:05.518636",
     "exception": false,
     "start_time": "2025-12-07T12:16:02.985130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create a copy for EDA to avoid modifying original data\n",
    "df_eda = df_clean.copy()\n",
    "\n",
    "# Chart 1: Distribution of Property Prices\n",
    "print(\"\\n Chart 1: Distribution of Property Prices\")\n",
    "fig1 = make_subplots(rows=1, cols=2, \n",
    "                     subplot_titles=('Price Distribution', 'Price Boxplot'))\n",
    "\n",
    "fig1.add_trace(\n",
    "    go.Histogram(x=df_eda['Price_in_Lakhs'], nbinsx=50, \n",
    "                 marker_color='steelblue', name='Price Distribution'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig1.add_trace(\n",
    "    go.Box(y=df_eda['Price_in_Lakhs'], marker_color='steelblue', \n",
    "           name='Price Boxplot'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig1.update_layout(\n",
    "    title_text=\"Distribution of Property Prices\",\n",
    "    height=500,\n",
    "    showlegend=False\n",
    ")\n",
    "fig1.update_xaxes(title_text=\"Price (in Lakhs)\", row=1, col=1)\n",
    "fig1.update_yaxes(title_text=\"Frequency\", row=1, col=1)\n",
    "fig1.update_yaxes(title_text=\"Price (in Lakhs)\", row=1, col=2)\n",
    "fig1.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a53e7f6",
   "metadata": {},
   "source": [
    "### 1. Why did you pick the specific chart?\n",
    "\n",
    "* **Histogram (Price Distribution):** Chosen to clearly display the **frequency distribution** of the continuous variable, Property Price (0 to 500 Lakhs). This shows the **shape** and **concentration** of the data.\n",
    "* **Boxplot (Price Boxplot):** Selected to provide a concise **five-number statistical summary** (min, $Q_1$, median, $Q_3$, max) and assess the **central tendency** and **symmetry**.\n",
    "\n",
    "### 2. What is/are the insight(s) found from the chart?\n",
    "\n",
    "* **Uniform Distribution:** The primary insight is that the frequency bars across the entire range (0 to 500 Lakhs) have an **almost equal height**. This means properties are **uniformly distributed** across all price points, which is a key indicator that the data is synthetic.\n",
    "* **Symmetry and Central Tendency:** The distribution is perfectly **symmetrical**, with the median ($\\approx 250$ Lakhs) located precisely in the center of the total range (0 to 500 Lakhs).\n",
    "* **Contradiction to Reality:** This uniform pattern is **highly unusual** for a real-world property market, which is typically **right-skewed** (more affordable homes than luxury homes), confirming the data is flawed.\n",
    "\n",
    "### 3. Will the gained insights help creating a positive business impact?\n",
    "\n",
    "* **Negative Impact is High:** The uniform distribution is unrealistic for real estate, making the data an **unreliable base for strategy**.\n",
    "* **Justification for Negative Growth:** The data falsely suggests stocking an **equal number** of low-demand high-end properties (450–500 Lakhs) as high-demand low-end properties (0–50 Lakhs). This leads to **stagnant inventory** in the expensive segment, tying up capital, increasing holding costs, and **hindering overall sales growth**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a887b5c",
   "metadata": {
    "papermill": {
     "duration": 0.081632,
     "end_time": "2025-12-07T12:16:05.683633",
     "exception": false,
     "start_time": "2025-12-07T12:16:05.602001",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Chart - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03ea91f",
   "metadata": {
    "papermill": {
     "duration": 0.195316,
     "end_time": "2025-12-07T12:16:05.963848",
     "exception": false,
     "start_time": "2025-12-07T12:16:05.768532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chart 2: Distribution of Property Sizes\n",
    "print(\"\\n Chart 2: Distribution of Property Sizes\")\n",
    "fig2 = make_subplots(rows=1, cols=2,\n",
    "                     subplot_titles=('Size Distribution', 'Size Boxplot'))\n",
    "\n",
    "fig2.add_trace(\n",
    "    go.Histogram(x=df_eda['Size_in_SqFt'], nbinsx=50,\n",
    "                 marker_color='coral', name='Size Distribution'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig2.add_trace(\n",
    "    go.Box(y=df_eda['Size_in_SqFt'], marker_color='coral',\n",
    "           name='Size Boxplot'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig2.update_layout(\n",
    "    title_text=\"Distribution of Property Sizes\",\n",
    "    height=500,\n",
    "    showlegend=False\n",
    ")\n",
    "fig2.update_xaxes(title_text=\"Size (in Sq Ft)\", row=1, col=1)\n",
    "fig2.update_yaxes(title_text=\"Frequency\", row=1, col=1)\n",
    "fig2.update_yaxes(title_text=\"Size (in Sq Ft)\", row=1, col=2)\n",
    "fig2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8ae5d4",
   "metadata": {},
   "source": [
    "### 1. Why did you pick the specific chart?\n",
    "\n",
    "* **Histogram (Size Distribution):** Chosen to clearly display the **frequency distribution** of the continuous variable, Property Size (0 to 5000 Sq Ft). This is essential for understanding the **inventory mix** and typical property size offerings.\n",
    "* **Boxplot (Size Boxplot):** Selected to provide a concise **five-number statistical summary** (min, $Q_1$, median, $Q_3$, max) and assess the **central tendency** and **symmetry** of the available sizes.\n",
    "\n",
    "### 2. What is/are the insight(s) found from the chart?\n",
    "\n",
    "* **Uniform Distribution:** The primary insight is that the frequency bars across the entire range (0 to 5000 Sq Ft) have an **almost equal height** (around 5,500). This means properties are **uniformly distributed** across all size points, from very small (0-500 Sq Ft) to very large (4500-5000 Sq Ft).\n",
    "* **Symmetry and Central Tendency:** The distribution is perfectly **symmetrical**, with the median ($\\approx 2500$ Sq Ft) located precisely in the center of the total size range.\n",
    "* **Contradiction to Reality:** This uniform pattern is **highly unrealistic** for a real market, which usually has a higher concentration around mid-to-large sizes (e.g., 1000–3000 Sq Ft). The uniform distribution across the entire range confirms the data is **synthetic** and was generated randomly .\n",
    "\n",
    "### 3. Will the gained insights help creating a positive business impact?\n",
    "\n",
    "* **Negative Impact is High:** The uniform distribution of sizes is unrealistic for development and sales planning, making the data an **unreliable base for inventory strategy**.\n",
    "* **Justification for Negative Growth:** The data suggests developing and stocking an **equal number** of extremely large, niche properties (4500-5000 Sq Ft) as standard small-to-mid-sized homes. This leads to **stagnant inventory** in both the very small and very large segments, resulting in **inefficient use of development capital** and failure to meet the actual demand curve for common property sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a8fc91",
   "metadata": {
    "papermill": {
     "duration": 0.11522,
     "end_time": "2025-12-07T12:16:06.208059",
     "exception": false,
     "start_time": "2025-12-07T12:16:06.092839",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Chart - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c78825",
   "metadata": {
    "papermill": {
     "duration": 0.176538,
     "end_time": "2025-12-07T12:16:06.507209",
     "exception": false,
     "start_time": "2025-12-07T12:16:06.330671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chart 3: Price per Sq Ft by Property Type\n",
    "print(\"\\n Chart 3: Price per Sq Ft by Property Type\")\n",
    "property_price = df_eda.groupby('Property_Type')['Price_per_SqFt'].mean().sort_values(ascending=True)\n",
    "\n",
    "fig3 = go.Figure(go.Bar(\n",
    "    x=property_price.values,\n",
    "    y=property_price.index,\n",
    "    orientation='h',\n",
    "    marker=dict(color='teal'),\n",
    "    text=property_price.values.round(2),\n",
    "    textposition='outside'\n",
    "))\n",
    "\n",
    "fig3.update_layout(\n",
    "    title=\"Average Price per Sq Ft by Property Type\",\n",
    "    xaxis_title=\"Average Price per Sq Ft\",\n",
    "    yaxis_title=\"Property Type\",\n",
    "    height=500\n",
    ")\n",
    "fig3.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9abba1",
   "metadata": {},
   "source": [
    "### 1. Why did you pick the specific chart?\n",
    "\n",
    "* **Valuation Driver Check:** Chosen to investigate if the core valuation metric, **Average Price per Sq Ft**, varies across the three major categorical property types: **Independent House, Apartment, and Villa**.\n",
    "* **Primary Goal:** To assess if the data structure adheres to the real-world principle that generally, Villas and Independent Houses command a higher price per square foot than Apartments.\n",
    "\n",
    "### 2. What is/are the insight(s) found from the chart?\n",
    "\n",
    "* **Zero Price Variation:** The primary insight is that the **Average Price per Sq Ft** is **exactly the same** ($\\mathbf{0.13}$) for **Independent House, Apartment, and Villa**. All three bars are of identical length.\n",
    "* **Contradiction to Reality:** In a real estate market, **Villas** and **Independent Houses** typically require more land and offer more privacy, often leading to a **measurable price premium** over standard apartments. The observed zero price difference is economically irrational and confirms the data's **synthetic and unresponsive pricing structure** .\n",
    "* **Confirms Uniformity:** This chart reinforces the findings from the Price Distribution, Size Distribution, and BHK Distribution charts: the data is engineered to be uniform and fails to model real-world economic variances.\n",
    "\n",
    "### 3. Will the gained insights help creating a positive business impact?\n",
    "\n",
    "* **Negative Impact is Absolute:** The insight confirms the data is incapable of valuing properties based on their fundamental type, making it unusable for pricing or development strategy.\n",
    "* **Justification for Negative Growth:**\n",
    "    * **Mispricing Strategy:** The business would be forced to price a luxury Villa at the same Price per Sq Ft as a standard Apartment, leading to a **massive undervaluation** of high-margin inventory (Villas/Houses) and significant loss of potential revenue.\n",
    "    * **Flawed Development Focus:** The data provides no financial incentive to focus resources on developing Villas or Independent Houses, as the price return is identical to the cheaper-to-build apartments. This leads to **inefficient allocation of development capital**.\n",
    "    * **Actionable Step:** This evidence adds another layer of certainty that the **dataset must be discarded** and replaced immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b202401",
   "metadata": {
    "papermill": {
     "duration": 0.11653,
     "end_time": "2025-12-07T12:16:06.744230",
     "exception": false,
     "start_time": "2025-12-07T12:16:06.627700",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Chart - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9478ea14",
   "metadata": {
    "papermill": {
     "duration": 1.489268,
     "end_time": "2025-12-07T12:16:08.355545",
     "exception": false,
     "start_time": "2025-12-07T12:16:06.866277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chart 4: Relationship between Size and Price\n",
    "print(\"\\n Chart 4: Relationship between Property Size and Price\")\n",
    "fig4 = px.scatter(df_eda, x='Size_in_SqFt', y='Price_in_Lakhs',\n",
    "                  color='Property_Type',\n",
    "                  title='Property Size vs Price',\n",
    "                  labels={'Size_in_SqFt': 'Size (in Sq Ft)',\n",
    "                          'Price_in_Lakhs': 'Price (in Lakhs)'},\n",
    "                  opacity=0.6,\n",
    "                  height=600)\n",
    "fig4.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba57872",
   "metadata": {},
   "source": [
    "### 1. Why did you pick the specific chart?\n",
    "\n",
    "* **Correlation Check:** Chosen to visually assess the **relationship (correlation)** between the two most critical real estate variables: **Property Size** (Sq Ft) and **Property Price** (Lakhs).\n",
    "* **Primary Goal:** To determine if the data supports the fundamental economic law that larger properties should cost more, and to visually confirm the $\\mathbf{+1}$ correlation reported in the heatmap.\n",
    "\n",
    "### 2. What is/are the insight(s) found from the chart?\n",
    "\n",
    "* **Null Correlation (Uniform Scatter):** The primary insight is that the data points are **uniformly scattered** across the entire chart area (a solid rectangular cloud of dots). This indicates a **null correlation** (zero association) between Price and Size .\n",
    "* **Contradiction to Reality:** In a real estate market, a **strong positive correlation** is expected: as **Property Size** increases, the **Price** should also increase. The absence of any trend confirms the suspicion that the data is **synthetic** and does not model real-world pricing.\n",
    "* **Confirms Flaw:** The chart visually confirms the flaw suggested by the uniform distribution charts. The Price is random across all Sizes, meaning a small 500 Sq Ft property is just as likely to be priced at 50 Lakhs as it is at 450 Lakhs, and the same applies to large 4500 Sq Ft properties.\n",
    "\n",
    "### 3. Will the gained insights help creating a positive business impact?\n",
    "\n",
    "* **Negative Impact is Absolute:** The insight confirms that the dataset is **fundamentally broken** for any modeling or strategic planning that relies on size-based valuation.\n",
    "* **Justification for Negative Growth:**\n",
    "    * **Inaccurate Valuation:** Any model built on this data (e.g., a regression model) would fail completely because the data shows no relationship. This leads to **grossly inaccurate valuations** of new properties.\n",
    "    * **Strategic Blindness:** The company cannot implement any size-based pricing tiers or target marketing based on property size, leading to the **inability to optimize revenue** from high-value, large properties.\n",
    "    * **Actionable Step:** The only positive impact is the **immediate and non-negotiable realization that the data must be discarded** before any business decision is made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396cc47b",
   "metadata": {
    "papermill": {
     "duration": 0.159146,
     "end_time": "2025-12-07T12:16:08.689524",
     "exception": false,
     "start_time": "2025-12-07T12:16:08.530378",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Chart - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e0d21d",
   "metadata": {
    "papermill": {
     "duration": 0.29781,
     "end_time": "2025-12-07T12:16:09.146556",
     "exception": false,
     "start_time": "2025-12-07T12:16:08.848746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chart 5: Outlier Analysis - Price per Sq Ft\n",
    "print(\"\\n Chart 5: Outlier Detection in Price per Sq Ft\")\n",
    "fig5 = make_subplots(rows=1, cols=2,\n",
    "                     subplot_titles=('Distribution', 'Boxplot with Outliers'))\n",
    "\n",
    "fig5.add_trace(\n",
    "    go.Histogram(x=df_eda['Price_per_SqFt'], nbinsx=50,\n",
    "                 marker_color='purple', name='Distribution'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig5.add_trace(\n",
    "    go.Box(y=df_eda['Price_per_SqFt'], marker_color='purple',\n",
    "           name='Boxplot', boxpoints='outliers'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig5.update_layout(\n",
    "    title_text=\"Outlier Analysis - Price per Sq Ft\",\n",
    "    height=500,\n",
    "    showlegend=False\n",
    ")\n",
    "fig5.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141affaf",
   "metadata": {},
   "source": [
    "### 1. Why did you pick the specific chart?\n",
    "\n",
    "* **Derived Metric Validation:** Chosen to analyze the distribution of the **Price per Sq Ft** metric, which is derived from the flawed Price and Size variables.\n",
    "* **Outlier and Skewness Check:** The primary goal is to check for statistical anomalies, specifically the presence of **outliers** and the **skewness** of the distribution, which is critical for preparing data for machine learning models.\n",
    "\n",
    "### 2. What is/are the insight(s) found from the chart?\n",
    "\n",
    "* **Realistic Skewness:** The histogram displays a **heavily right-skewed** (positively skewed) distribution, meaning the vast majority of properties are clustered at low Price per Sq Ft values . This shape is, ironically, the **only realistic distribution** found in the dataset, as real-world Price per Sq Ft is typically right-skewed.\n",
    "* **Extreme Outliers:** The boxplot confirms the presence of **extreme outliers** extending all the way up to $\\mathbf{1.0}$. This suggests a small number of records have a very high price relative to their size (e.g., small properties priced high), necessitating outlier treatment .\n",
    "* **Confirms Flaw (Mathematical Engineering):** The fact that the derived metric (Price per Sq Ft) is realistically skewed, while the core metrics (Price and Size) were found to be **unrealistically uniform**, confirms that the data was generated via a mathematical process. The division operation created a realistic-looking distribution, but the underlying $\\mathbf{+1}$ correlation (Price vs. Size) proves the inputs are synthetic.\n",
    "\n",
    "### 3. Will the gained insights help creating a positive business impact?\n",
    "\n",
    "* **High Negative Risk (Data Prep):** While the distribution shape is correct, the extreme outliers must be **removed or capped** before any machine learning modeling is attempted. If left untreated, these high Price per Sq Ft outliers will severely **skew any predictive model**, rendering it useless.\n",
    "* **Justification for Negative Growth:** The median price per sq ft is extremely low ($\\approx 0.1$ or $0.11$), which confirms that the vast bulk of the synthetically generated data is concentrated at the low end of the valuation spectrum.\n",
    "* **Actionable Step:** The necessary step here is **data cleaning** (outlier removal/capping), but this is a temporary fix. Since the core relationship between Price and Size is confirmed fake (the $\\mathbf{+1}$ correlation), the entire dataset remains **unsuitable** for reliable business use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b4dc91",
   "metadata": {
    "papermill": {
     "duration": 0.194348,
     "end_time": "2025-12-07T12:16:09.546133",
     "exception": false,
     "start_time": "2025-12-07T12:16:09.351785",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Chart - 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f02be",
   "metadata": {
    "papermill": {
     "duration": 0.238074,
     "end_time": "2025-12-07T12:16:09.977873",
     "exception": false,
     "start_time": "2025-12-07T12:16:09.739799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Chart 6: Average Price per Sq Ft by State\n",
    "print(\"\\n Chart 6: Average Price per Sq Ft by State (Top 10)\")\n",
    "state_price = df_eda.groupby('State')['Price_per_SqFt'].mean().sort_values(ascending=True).tail(10)\n",
    "\n",
    "fig6 = go.Figure(go.Bar(\n",
    "    x=state_price.values,\n",
    "    y=state_price.index,\n",
    "    orientation='h',\n",
    "    marker=dict(color='orange', line=dict(color='black', width=1)),\n",
    "    text=state_price.values.round(2),\n",
    "    textposition='outside'\n",
    "))\n",
    "\n",
    "fig6.update_layout(\n",
    "    title=\"Top 10 States by Average Price per Sq Ft\",\n",
    "    xaxis_title=\"Average Price per Sq Ft\",\n",
    "    yaxis_title=\"State\",\n",
    "    height=500\n",
    ")\n",
    "fig6.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ffb9dd",
   "metadata": {},
   "source": [
    "### 1. Why did you pick the specific chart?\n",
    "\n",
    "* **Geographic Valuation Check:** Chosen to investigate if the valuation metric, **Average Price per Sq Ft**, shows the expected variation across major geographic regions (States).\n",
    "* **Primary Goal:** To assess if the data supports the fundamental real-world principle that property valuation varies drastically by state due to economic factors, infrastructure, and population density (e.g., comparing Maharashtra to Assam).\n",
    "\n",
    "### 2. What is/are the insight(s) found from the chart?\n",
    "\n",
    "* **Zero Price Variation:** The primary and most significant insight is that the **Average Price per Sq Ft** is **exactly the same** ($\\mathbf{0.13}$) for all ten states listed (Karnataka, Andhra Pradesh, Uttar Pradesh, Tamil Nadu, Gujarat, Telangana, Assam, Madhya Pradesh, Maharashtra, and Haryana). All ten horizontal bars are of identical length.\n",
    "* **Contradiction to Reality:** This finding is **economically impossible**. Real-world property prices per square foot vary by a large magnitude between states. The complete absence of any price difference confirms that the valuation is **completely unresponsive** to geographic location .\n",
    "* **Confirms Universal Flaw:** This chart reinforces the finding that the data is not only uniform across features (Furnishing, Facing) and amenities (Hospitals, Schools) but also across the highest level of geographic aggregation (States), proving the data is **universally synthetic**.\n",
    "\n",
    "### 3. Will the gained insights help creating a positive business impact?\n",
    "\n",
    "* **Negative Impact is Absolute:** The insight confirms that the dataset cannot be used to model or analyze **geographic market segmentation or risk**.\n",
    "* **Justification for Negative Growth:**\n",
    "    * **Flawed Expansion Strategy:** The business would be misled into believing there is no financial difference in price between developing/selling property in high-cost states (like Maharashtra) and low-cost states (like Assam). This leads to **gross misallocation of capital and personnel**.\n",
    "    * **Inaccurate Risk Assessment:** The company cannot assess geographic risk or opportunity based on this data, as all states appear identical in valuation.\n",
    "    * **Actionable Step:** Since the data is proven flawed at every level—from individual amenities to state-level aggregation—the only remaining positive action is the **immediate replacement of the dataset**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c596f7",
   "metadata": {
    "papermill": {
     "duration": 0.195191,
     "end_time": "2025-12-07T12:16:10.373798",
     "exception": false,
     "start_time": "2025-12-07T12:16:10.178607",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Chart - 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e88565",
   "metadata": {
    "papermill": {
     "duration": 0.241197,
     "end_time": "2025-12-07T12:16:10.812041",
     "exception": false,
     "start_time": "2025-12-07T12:16:10.570844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n Chart 7: Average Property Price by City (Top 15)\")\n",
    "city_price = df_eda.groupby('City')['Price_in_Lakhs'].mean().sort_values(ascending=True).tail(15)\n",
    "\n",
    "fig7 = go.Figure(go.Bar(\n",
    "    x=city_price.values,\n",
    "    y=city_price.index,\n",
    "    orientation='h',\n",
    "    marker=dict(color='crimson', line=dict(color='black', width=1)),\n",
    "    text=city_price.values.round(2),\n",
    "    textposition='outside'\n",
    "))\n",
    "\n",
    "fig7.update_layout(\n",
    "    title=\"Top 15 Cities by Average Property Price\",\n",
    "    xaxis_title=\"Average Price (in Lakhs)\",\n",
    "    yaxis_title=\"City\",\n",
    "    height=600\n",
    ")\n",
    "fig7.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb2c741",
   "metadata": {},
   "source": [
    "### 1. Why did you pick the specific chart?\n",
    "\n",
    "* **City-Level Valuation Check:** Chosen to investigate if the **Average Property Price** shows the expected variation across major urban centers (Cities), which is a crucial check for market segmentation and competitive analysis.\n",
    "* **Primary Goal:** To assess if the data supports the fundamental real-world principle that property valuation varies drastically between large cities due to local economic factors, infrastructure, and demand.\n",
    "\n",
    "### 2. What is/are the insight(s) found from the chart?\n",
    "\n",
    "* **Minimal Price Variation:** The primary insight is that the Average Property Price for all 15 cities is **clustered tightly** between $\\mathbf{255.77}$ Lakhs (Vishakhapatnam) and $\\mathbf{258.46}$ Lakhs (Bangalore).\n",
    "* **Negligible Difference:** The difference between the most expensive city (Bangalore) and the least expensive city (Vishakhapatnam) is only $\\mathbf{2.69}$ Lakhs. This is a negligible variance, particularly when compared to the average price of $\\approx 256$ Lakhs.\n",
    "* **Contradiction to Reality:** This finding is **economically impossible**. Real-world average property prices between major cities typically vary by tens or hundreds of Lakhs. The absence of any significant price difference confirms that the valuation is **completely unresponsive** to city-specific market dynamics .\n",
    "* **Confirms Synthetic Data:** This further proves that the price uniformity seen in States, Amenities, and Features is maintained at the City level.\n",
    "\n",
    "### 3. Will the gained insights help creating a positive business impact?\n",
    "\n",
    "* **Negative Impact is Absolute:** The insight confirms that the dataset cannot be used to model or analyze **city-specific market segmentation or risk**.\n",
    "* **Justification for Negative Growth:**\n",
    "    * **Flawed Market Strategy:** The business would be misled into believing that all major cities are interchangeable in terms of average price. This prevents the establishment of accurate, competitive, and city-specific pricing strategies, leading to **massive losses from underpriced inventory** in high-value cities and **stagnant inventory** in lower-value cities.\n",
    "    * **Inaccurate Competitive Analysis:** The company cannot accurately assess market competition or risk since the data shows virtually no difference between any of the 15 major urban centers.\n",
    "    * **Actionable Step:** The continued evidence of flaws across all geographic levels confirms the **immediate and non-negotiable need to replace the dataset**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28920b2",
   "metadata": {
    "papermill": {
     "duration": 0.198905,
     "end_time": "2025-12-07T12:16:11.215955",
     "exception": false,
     "start_time": "2025-12-07T12:16:11.017050",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Chart - 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72a476d",
   "metadata": {
    "papermill": {
     "duration": 0.250093,
     "end_time": "2025-12-07T12:16:11.668976",
     "exception": false,
     "start_time": "2025-12-07T12:16:11.418883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chart 8: Median Age of Properties by Locality\n",
    "print(\"\\n Chart 8: Median Age of Properties by Locality (Top 15)\")\n",
    "locality_age = df_eda.groupby('Locality')['Age_of_Property'].median().sort_values(ascending=True).tail(15)\n",
    "\n",
    "fig8 = go.Figure(go.Bar(\n",
    "    x=locality_age.values,\n",
    "    y=locality_age.index,\n",
    "    orientation='h',\n",
    "    marker=dict(color='green', line=dict(color='black', width=1)),\n",
    "    text=locality_age.values.round(1),\n",
    "    textposition='outside'\n",
    "))\n",
    "\n",
    "fig8.update_layout(\n",
    "    title=\"Top 15 Localities by Median Property Age\",\n",
    "    xaxis_title=\"Median Age of Property (years)\",\n",
    "    yaxis_title=\"Locality\",\n",
    "    height=600\n",
    ")\n",
    "fig8.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910d27b5",
   "metadata": {},
   "source": [
    "### 1. Why did you pick the specific chart?\n",
    "\n",
    "* **Property Life Cycle Check:** Chosen to investigate if the **Median Property Age** varies across the most active localities. Property age is a crucial non-price factor for estimating renovation costs, maintenance needs, and market life cycle analysis.\n",
    "* **Primary Goal:** To assess if this variable shows the expected distribution (a wide range of ages) or if it exhibits the same uniformity seen across all other variables.\n",
    "\n",
    "### 2. What is/are the insight(s) found from the chart?\n",
    "\n",
    "* **Near-Zero Age Variation:** The primary insight is that the Median Property Age for all 15 localities is clustered almost perfectly at **$\\mathbf{20}$ or $\\mathbf{21}$ years**. The vast majority of the bars are exactly $\\mathbf{20}$ years old.\n",
    "* **Contradiction to Reality:** In a real market, property age should vary drastically between localities, ranging from brand new (0 years) to decades old (50+ years). The near-perfect uniformity around 20 years suggests that the **'Age of Property' variable is synthetic** and was assigned a fixed value or extremely narrow range during data generation .\n",
    "* **Confirms Flaw:** This reinforces the conclusion that the dataset is engineered, as even a fundamental temporal variable like age fails to show realistic variance across granular geographic segments.\n",
    "\n",
    "### 3. Will the gained insights help creating a positive business impact?\n",
    "\n",
    "* **Negative Impact is Absolute:** The insight confirms that the dataset cannot be used for any analysis that relies on accurate property age modeling.\n",
    "* **Justification for Negative Growth:**\n",
    "    * **Flawed Financial Planning:** The company cannot accurately forecast capital expenditures for renovation or maintenance, as the data falsely suggests all inventory is the same age and requires similar near-term investment.\n",
    "    * **Useless Market Segmentation:** Analysis based on market segments like 'modern construction' vs. 'historic properties' is rendered impossible, as the data provides no realistic differentiation in age.\n",
    "    * **Actionable Step:** This finding adds to the overwhelming evidence that every major variable and characteristic in the dataset is flawed, necessitating the **immediate replacement of the dataset**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035ebc81",
   "metadata": {
    "papermill": {
     "duration": 0.199837,
     "end_time": "2025-12-07T12:16:12.066671",
     "exception": false,
     "start_time": "2025-12-07T12:16:11.866834",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Chart - 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80698fc",
   "metadata": {
    "papermill": {
     "duration": 0.36904,
     "end_time": "2025-12-07T12:16:12.636309",
     "exception": false,
     "start_time": "2025-12-07T12:16:12.267269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chart 9: BHK Distribution Across Cities\n",
    "print(\"\\n Chart 9: BHK Distribution Across Top Cities\")\n",
    "top_cities = df_eda['City'].value_counts().head(5).index\n",
    "city_bhk_data = df_eda[df_eda['City'].isin(top_cities)]\n",
    "\n",
    "fig9 = px.histogram(city_bhk_data, x='City', color='BHK',\n",
    "                    barmode='group',\n",
    "                    title='BHK Distribution Across Top 5 Cities',\n",
    "                    labels={'City': 'City', 'count': 'Number of Properties'},\n",
    "                    height=600)\n",
    "fig9.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740b9400",
   "metadata": {},
   "source": [
    "### 1. Why did you pick the specific chart?\n",
    "\n",
    "* **Grouped Bar Chart (BHK Distribution):** Chosen to compare the **frequency (count)** of five different **categorical groups (BHKs)** across five different **cities**.\n",
    "* **Primary Goal:** To assess if the underlying data's uniformity (seen in price and price-size correlation) extends to the distribution of available inventory (BHKs) across different geographic locations (Cities).\n",
    "\n",
    "### 2. What is/are the insight(s) found from the chart?\n",
    "\n",
    "* **Uniform BHK Distribution:** The primary insight is that the count of properties for **every BHK category (1, 2, 3, 4, 5)** is **nearly identical** to each other within **every single City**. The bars for all five BHKs within any given city are clustered tightly between approximately 1,200 and 1,350 .\n",
    "* **Uniform City Distribution:** The total number of properties in **every City** is also nearly identical (each city has a total count of approximately 6,500 properties).\n",
    "* **Contradiction to Reality:** This uniform pattern across all BHKs and all Cities is **highly unrealistic** for a real-world property market. In reality, a market typically has a high concentration of 2-BHK and 3-BHK properties, with a much lower count of 1-BHK (bachelor pads/starter homes) and 5-BHK (luxury/large homes) .\n",
    "* **Conclusion on Data Quality:** This chart further reinforces the conclusion that the entire dataset is **synthetic or flawed**, as the distribution of property types (BHK) is unnaturally uniform, just like the distribution of prices and the price-size correlation.\n",
    "\n",
    "### 3. Will the gained insights help creating a positive business impact?\n",
    "\n",
    "* **Negative Impact is Confirmed:** The insight confirms that the data is not only flawed in pricing/valuation but also in the **inventory mix** and **geographic balance**, making it completely unreliable for strategic operational decisions.\n",
    "* **Justification for Negative Growth:**\n",
    "    * **Misallocation of Resources/Personnel:** The data suggests treating all five cities as having the **exact same market demand** and **inventory profile**. This leads to misallocation of sales personnel, marketing spend, and inventory holding capacity, which should be based on real-world city size, economic activity, and actual housing demand.\n",
    "    * **Incorrect Target Marketing:** Marketing campaigns cannot be effectively segmented by city or by property type (BHK). The company would spend resources equally promoting low-demand 1-BHK and 5-BHK units as high-demand 2-BHK and 3-BHK units, leading to **low conversion rates** and **wasted advertising budgets**.\n",
    "    * **Actionable Step:** Immediate data replacement or aggressive data cleansing/repair is necessary before any meaningful business strategy can be developed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3c5637",
   "metadata": {
    "papermill": {
     "duration": 0.203569,
     "end_time": "2025-12-07T12:16:13.046136",
     "exception": false,
     "start_time": "2025-12-07T12:16:12.842567",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Chart - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bed6df",
   "metadata": {
    "papermill": {
     "duration": 0.350486,
     "end_time": "2025-12-07T12:16:13.605880",
     "exception": false,
     "start_time": "2025-12-07T12:16:13.255394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chart 10: Price Trends for Top 5 Most Expensive Localities\n",
    "print(\"\\n Chart 10: Price Trends for Top 5 Most Expensive Localities\")\n",
    "top_localities = df_eda.groupby('Locality')['Price_in_Lakhs'].mean().sort_values(ascending=False).head(5).index\n",
    "\n",
    "fig10 = go.Figure()\n",
    "for locality in top_localities:\n",
    "    locality_data = df_eda[df_eda['Locality'] == locality]\n",
    "    fig10.add_trace(go.Scatter(\n",
    "        x=locality_data['Size_in_SqFt'],\n",
    "        y=locality_data['Price_in_Lakhs'],\n",
    "        mode='markers',\n",
    "        name=locality,\n",
    "        opacity=0.6\n",
    "    ))\n",
    "\n",
    "fig10.update_layout(\n",
    "    title=\"Price Trends for Top 5 Most Expensive Localities\",\n",
    "    xaxis_title=\"Size (in Sq Ft)\",\n",
    "    yaxis_title=\"Price (in Lakhs)\",\n",
    "    height=600\n",
    ")\n",
    "fig10.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8802bed",
   "metadata": {},
   "source": [
    "### 1. Why did you pick the specific chart?\n",
    "\n",
    "* **Scatter Plot (Price vs. Size/Locality):** Chosen to observe the **correlation** and **pattern** between Property Price (already known to be uniformly distributed) and a potential predictor variable like **Property Size (Sq. Ft.)** or a **Categorical Locality** (if used with color/shape).\n",
    "* **Primary Goal:** To assess if the uniform price structure holds true across different sizes or locations, and to check for the **presence or absence of correlation**.\n",
    "\n",
    "### 2. What is/are the insight(s) found from the chart?\n",
    "\n",
    "* **No Correlation (Uniform Scatter):** The primary insight is that the data points are **uniformly scattered** across the entire chart area (the 'cloud' of dots is roughly rectangular or square). This indicates a **null correlation** (or zero association) between the two variables plotted (e.g., Price and Size).\n",
    "* **Contradiction to Reality:** In a real estate market, a **strong positive correlation** is expected: as **Property Size** increases, the **Price** should also increase (or vice-versa). The absence of any trend confirms the suspicion from the histogram analysis that the data is **synthetic** and does not represent real-world market dynamics.\n",
    "* **Market Implausibility:** The chart shows that small, low-end properties have the same likelihood of being priced at 50 Lakhs as they do at 450 Lakhs, and the same is true for large, high-end properties.\n",
    "\n",
    "### 3. Will the gained insights help creating a positive business impact?\n",
    "\n",
    "* **Negative Impact is Severe:** The insight confirms that the dataset is **fundamentally broken** for predictive modeling or strategic planning.\n",
    "* **Justification for Negative Growth:**\n",
    "    * **Inaccurate Valuation:** Any model built on this data (e.g., a regression model to predict price based on size) would fail, as the relationship is zero. This would lead to **grossly inaccurate valuations** of new properties.\n",
    "    * **Strategic Blindness:** The company cannot distinguish high-value inventory from low-value inventory based on core features like size or location (if plotted). This leads to misinformed marketing, incorrect sales targets, and a failure to identify market segments.\n",
    "    * **Actionable Step:** The only positive impact is the **immediate realization that the data must be discarded or repaired** before any business decision can be made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db9ddef",
   "metadata": {
    "papermill": {
     "duration": 0.210138,
     "end_time": "2025-12-07T12:16:14.043771",
     "exception": false,
     "start_time": "2025-12-07T12:16:13.833633",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Chart - 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619b1d43",
   "metadata": {
    "papermill": {
     "duration": 0.340359,
     "end_time": "2025-12-07T12:16:14.590124",
     "exception": false,
     "start_time": "2025-12-07T12:16:14.249765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Chart 11: Correlation Heatmap\n",
    "print(\"\\n Chart 11: Correlation Heatmap of Numeric Features\")\n",
    "numeric_cols = df_eda.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = df_eda[numeric_cols].corr()\n",
    "\n",
    "fig11 = go.Figure(data=go.Heatmap(\n",
    "    z=correlation_matrix.values,\n",
    "    x=correlation_matrix.columns,\n",
    "    y=correlation_matrix.columns,\n",
    "    colorscale='RdBu',\n",
    "    zmid=0,\n",
    "    text=correlation_matrix.values.round(2),\n",
    "    texttemplate='%{text}',\n",
    "    textfont={\"size\": 8}\n",
    "))\n",
    "\n",
    "fig11.update_layout(\n",
    "    title=\"Correlation Heatmap - Numeric Features\",\n",
    "    height=800,\n",
    "    width=900\n",
    ")\n",
    "fig11.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72cd2b8",
   "metadata": {},
   "source": [
    "### 1. Why did you pick the specific chart?\n",
    "\n",
    "* **Correlation Matrix (Heatmap):** Chosen to quantify the **linear relationships** between all pairs of variables in the dataset. This provides a formal, numerical confirmation of the qualitative patterns (or lack thereof) observed in the scatter plot and histogram.\n",
    "* **Scatter Plot (Price Trends for Top 5 Localities):** Selected to visualize the relationship between the two most critical real estate features: **Price** and **Size**, while simultaneously checking if the market structure **changes across different expensive localities**.\n",
    "\n",
    "### 2. What is/are the insight(s) found from the chart?\n",
    "\n",
    "#### A. Insights from the Correlation Matrix:\n",
    "\n",
    "* **Flawed Correlation:** The correlation between the two most important variables—**Size\\_in\\_SqFt** and **Price\\_in\\_Lakhs**—is reported as **1** (perfect positive correlation). This is statistically impossible for real-world, non-engineered data and confirms the data is synthetic.\n",
    "* **Contradiction in Price Metrics:** There is a high negative correlation between **Price\\_per\\_SqFt** and **Size\\_in\\_SqFt** ($\\mathbf{-0.61}$), but the correlation between **Price\\_in\\_Lakhs** and **Size\\_in\\_SqFt** is $\\mathbf{+1}$ . This suggests the total price was likely engineered from the size, and then the Price\\_per\\_SqFt was calculated, which reveals the negative relationship—a common flaw when synthetic data is created by formula.\n",
    "* **Null Correlation with Features:** Many key features have zero correlation (0) with the price variables, including **BHK**, **Floor\\_No**, **Total\\_Floors**, **Age\\_of\\_Property**, **Nearby\\_Schools**, and **Nearby\\_Hospitals**. In reality, all these factors should exhibit some degree of correlation with the Price.\n",
    "\n",
    "#### B. Insights from the Scatter Plot (Price vs. Size by Locality):\n",
    "\n",
    "* **Uniform Scatter/Null Correlation Confirmed:** The data points for all five localities are **uniformly scattered** across the entire chart area (Price 0 to 500 Lakhs, Size 500 to 5000 Sq Ft). There is **no discernible trend** or correlation between Price and Size, **contradicting the $\\mathbf{+1}$ correlation** shown in the matrix . This suggests the $+1$ correlation is likely a result of a highly specific data engineering flaw that is visually masked by the high variance/uniformity.\n",
    "* **No Locality-Specific Trend:** All five of the \"Most Expensive Localities\" exhibit the **exact same uniform scatter pattern**, confirming that the synthetic/flawed structure is applied uniformly across different geographic segments.\n",
    "\n",
    "### 3. Will the gained insights help creating a positive business impact?\n",
    "\n",
    "* **Negative Impact is Catastrophic:** The combination of a mathematically impossible $\\mathbf{+1}$ correlation (matrix) and a visually non-existent correlation (scatter plot) confirms the dataset is **fundamentally unusable** for any business intelligence, predictive modeling (like valuation), or strategic decision-making.\n",
    "* **Justification for Negative Growth:**\n",
    "    * **Inaccurate Model Training:** Training a Machine Learning model on data with a $\\mathbf{+1}$ correlation between the independent and dependent variables will result in a model that perfectly overfits the flawed data but completely fails on any real-world data (zero generalizability).\n",
    "    * **Misguided Locality Strategy:** Since the \"Top 5 Most Expensive Localities\" show the same price and size distribution as the rest of the market (as seen in the scatter plot), the company cannot implement any **premium pricing strategy** or focus marketing spend on high-value areas, as the data provides no basis for differentiating these localities.\n",
    "    * **Actionable Step:** The only positive impact is the complete and final validation that the data must be **replaced immediately** to prevent any further strategic errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8969bc",
   "metadata": {
    "papermill": {
     "duration": 0.210599,
     "end_time": "2025-12-07T12:16:15.015868",
     "exception": false,
     "start_time": "2025-12-07T12:16:14.805269",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Chart - 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac662e49",
   "metadata": {
    "papermill": {
     "duration": 2.456534,
     "end_time": "2025-12-07T12:16:17.682060",
     "exception": false,
     "start_time": "2025-12-07T12:16:15.225526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Chart 12: Nearby Schools vs Price per Sq Ft\n",
    "print(\"\\n Chart 12: Nearby Schools vs Price per Sq Ft\")\n",
    "fig12 = px.scatter(df_eda, x='Nearby_Schools', y='Price_per_SqFt',\n",
    "                   color='Property_Type',\n",
    "                   title='Nearby Schools vs Price per Sq Ft',\n",
    "                   labels={'Nearby_Schools': 'Number of Nearby Schools',\n",
    "                           'Price_per_SqFt': 'Price per Sq Ft'},\n",
    "                   opacity=0.6,\n",
    "                   trendline=\"ols\",\n",
    "                   height=600)\n",
    "fig12.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19c3bcc",
   "metadata": {},
   "source": [
    "### 1. Why did you pick the specific chart?\n",
    "\n",
    "* **Scatter Plot (Hospitals vs. Price per Sq Ft):** Chosen to investigate the relationship between a critical **amenity/location factor** (Hospitals) and the **valuation metric** (Price per Sq Ft).\n",
    "* **Primary Goal:** To assess if the price metric behaves logically based on proximity to a key amenity, and to see if this relationship varies by property size (represented by BHK color).\n",
    "\n",
    "### 2. What is/are the insight(s) found from the chart?\n",
    "\n",
    "* **Vertical Line Anomaly:** The data points form distinct **vertical lines** across the entire chart area for every value of \"Number of Nearby Hospitals\" (1 through 10) .\n",
    "* **Null Correlation:** This pattern indicates an **absolute lack of correlation** between the number of nearby hospitals and the property's price per square foot. The price per square foot for a property with 1 hospital nearby is distributed identically to a property with 10 hospitals nearby.\n",
    "* **Constant Price Floor:** A clear, dense concentration of points forms a horizontal line at a very low Price per Sq Ft ($\\approx 0.15$), regardless of the number of hospitals.\n",
    "* **BHK Irrelevance:** The **BHK** property type (color gradient) is **randomly distributed** within these vertical lines, meaning a 1-BHK has the same likelihood of being near 1 hospital as it does 10, and the price is unaffected.\n",
    "* **Contradiction to Reality:** In a real market, closer proximity to hospitals (a key amenity) often commands a **premium price**, suggesting a positive or complex correlation, which is completely absent here. This further confirms the synthetic, uniformly random structure of the data, as previously seen with the schools and size vs. price plots.\n",
    "\n",
    "### 3. Will the gained insights help creating a positive business impact?\n",
    "\n",
    "* **Negative Impact is High:** The insight confirms that the pricing structure is **unresponsive to key locational amenities** (Hospitals), rendering the data unusable for strategic decisions involving location premium or amenity investment.\n",
    "* **Justification for Negative Growth:**\n",
    "    * **Failed Amenity Strategy:** The business cannot justify a higher asking price or focus marketing efforts on properties located near numerous hospitals, as the data falsely suggests this factor has **zero impact** on valuation.\n",
    "    * **Misallocation of Inventory Focus:** The company might spend equal effort selling poorly located properties as well-located ones, based on this misleading data, leading to **stagnant inventory** in objectively less desirable locations.\n",
    "    * **Actionable Step:** The continued analysis of individual features reinforces the **immediate necessity to replace or heavily filter this synthetic dataset** before any strategic use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa3f935",
   "metadata": {
    "papermill": {
     "duration": 0.792162,
     "end_time": "2025-12-07T12:16:18.821667",
     "exception": false,
     "start_time": "2025-12-07T12:16:18.029505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chart 13: Nearby Hospitals vs Price per Sq Ft\n",
    "print(\"\\n Chart 13: Nearby Hospitals vs Price per Sq Ft\")\n",
    "fig13 = px.scatter(df_eda, x='Nearby_Hospitals', y='Price_per_SqFt',\n",
    "                   color='BHK',\n",
    "                   title='Nearby Hospitals vs Price per Sq Ft',\n",
    "                   labels={'Nearby_Hospitals': 'Number of Nearby Hospitals',\n",
    "                           'Price_per_SqFt': 'Price per Sq Ft'},\n",
    "                   opacity=0.6,\n",
    "                   trendline=\"ols\",\n",
    "                   height=600)\n",
    "fig13.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457b7f2f",
   "metadata": {},
   "source": [
    "### 1. Why did you pick the specific chart?\n",
    "\n",
    "* **Amenity Valuation Check:** Chosen to specifically investigate the correlation between proximity to a critical public amenity (**Nearby Hospitals**) and the property's valuation metric (**Price per Sq Ft**).\n",
    "* **Data Integrity Check:** This plot serves as a fundamental check to see if the pricing structure is responsive to location factors, which are major drivers of real estate value.\n",
    "\n",
    "### 2. What is/are the insight(s) found from the chart?\n",
    "\n",
    "* **Vertical Line Anomaly and Null Correlation:** The data points form clear, isolated **vertical lines** for every single value of \"Number of Nearby Hospitals\" (from 1 to 10). This is an unnatural pattern indicating a **zero correlation** (null association) between the number of nearby hospitals and the property's price per square foot.\n",
    "* **Constant Price Floor:** A dense cluster of data points forms a horizontal line at a very low Price per Sq Ft ($\\approx 0.15$) across all hospital counts, suggesting a baseline price that is unaffected by this key amenity.\n",
    "* **BHK Irrelevance:** The **BHK** configuration (color) is randomly distributed throughout these vertical lines. This shows that the size of the property does not affect how its price is correlated with the number of nearby hospitals, confirming the widespread uniformity in the data.\n",
    "* **Contradiction to Reality:** In a functional real estate market, properties in close proximity to a greater number of hospitals (a key convenience) would typically demand a **measurable price premium**, which is entirely absent in this chart.\n",
    "\n",
    "### 3. Will the gained insights help creating a positive business impact?\n",
    "\n",
    "* **Negative Impact is High:** The insight confirms that the pricing structure is **unresponsive to essential health amenities**. This makes the data unusable for strategic pricing or segmentation.\n",
    "* **Justification for Negative Growth:**\n",
    "    * **Failed Strategic Pricing:** The business cannot justify or implement a **location premium** based on access to hospitals, as the data falsely indicates this factor has no impact on value. This prevents the company from maximizing revenue on objectively superior inventory.\n",
    "    * **Misguided Marketing:** The company is unable to target specific demographics (like elderly residents or those needing quick hospital access) and cannot tailor marketing to highlight this crucial amenity, leading to **inefficient marketing spend** and lower conversion rates.\n",
    "    * **Actionable Step:** The primary business impact is the **final confirmation** that the dataset must be **discarded or repaired** immediately, as its core structure is unreliable for any decision-making regarding price and location factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4387734e",
   "metadata": {
    "papermill": {
     "duration": 0.527082,
     "end_time": "2025-12-07T12:16:19.812426",
     "exception": false,
     "start_time": "2025-12-07T12:16:19.285344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chart 14: Price Variation by Furnished Status\n",
    "print(\"\\n Chart 14: Price Variation by Furnished Status\")\n",
    "furnished_stats = df_eda.groupby('Furnished_Status').agg({\n",
    "    'Price_in_Lakhs': ['mean', 'median', 'std']\n",
    "}).round(2)\n",
    "\n",
    "fig14 = go.Figure()\n",
    "fig14.add_trace(go.Bar(\n",
    "    name='Mean',\n",
    "    x=furnished_stats.index,\n",
    "    y=furnished_stats['Price_in_Lakhs']['mean'],\n",
    "    marker_color='skyblue'\n",
    "))\n",
    "fig14.add_trace(go.Bar(\n",
    "    name='Median',\n",
    "    x=furnished_stats.index,\n",
    "    y=furnished_stats['Price_in_Lakhs']['median'],\n",
    "    marker_color='lightcoral'\n",
    "))\n",
    "\n",
    "fig14.update_layout(\n",
    "    title=\"Price Variation by Furnished Status\",\n",
    "    xaxis_title=\"Furnished Status\",\n",
    "    yaxis_title=\"Price (in Lakhs)\",\n",
    "    barmode='group',\n",
    "    height=500\n",
    ")\n",
    "fig14.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ece650",
   "metadata": {},
   "source": [
    "### 1. Why did you pick the specific chart?\n",
    "\n",
    "* **Valuation Driver Check:** Chosen to investigate if a major structural factor that influences price—the level of furnishing—has the expected impact on valuation metrics (Mean and Median Price).\n",
    "* **Data Uniformity Check:** This plot is used to check if the synthetic, uniform nature of the data (observed in price distribution and correlation) also applies to key categorical features.\n",
    "\n",
    "### 2. What is/are the insight(s) found from the chart?\n",
    "\n",
    "* **Zero Price Variation:** The primary insight is that the **Mean Price** and **Median Price** are **virtually identical** across all three furnishing categories: Furnished, Semi-furnished, and Unfurnished. The bars for all six values (Mean/Median for three categories) are clustered tightly around $\\mathbf{250}$ Lakhs.\n",
    "* **Contradiction to Reality:** In a real estate market, properties that are **Furnished** (saving the buyer the cost of furniture) should command a **higher price** than Semi-furnished, which, in turn, should be priced higher than **Unfurnished** properties . The observed zero price difference confirms that the valuation is **unresponsive** to this crucial property characteristic.\n",
    "* **Confirmation of Uniformity:** This chart strongly corroborates the findings from the initial Histogram analysis (uniform price distribution from 0 to 500 Lakhs, centered at 250 Lakhs) and the scatter plots (null correlation). The central tendency (Mean and Median) is $\\mathbf{250}$ Lakhs, regardless of the property's size, location, amenities, or furnishing status.\n",
    "\n",
    "### 3. Will the gained insights help creating a positive business impact?\n",
    "\n",
    "* **Negative Impact is High:** The insight confirms that the data is not only flawed in locational metrics but also in its ability to value **property features** like furnishing.\n",
    "* **Justification for Negative Growth:**\n",
    "    * **Flawed Investment Decisions:** If the business were to use this data, it would falsely conclude that spending capital on furnishing properties provides **zero return on investment (ROI)**, as the price is unaffected. This would lead to poor inventory presentation and lost revenue opportunities on properties that should be priced higher.\n",
    "    * **Incorrect Pricing Strategy:** The company is forced to price all properties identically regardless of furnishing status, leading to **undervaluation** of furnished homes and **overvaluation** of unfurnished ones, damaging profitability and sales velocity.\n",
    "    * **Actionable Step:** This evidence provides final proof that the dataset's flaws permeate both continuous (Price, Size) and categorical (BHK, City, Furnishing) variables, necessitating its **immediate replacement**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edcce11",
   "metadata": {
    "papermill": {
     "duration": 0.474554,
     "end_time": "2025-12-07T12:16:20.716537",
     "exception": false,
     "start_time": "2025-12-07T12:16:20.241983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chart 15: Price per Sq Ft by Facing Direction\n",
    "print(\"\\n Chart 15: Price per Sq Ft by Property Facing Direction\")\n",
    "facing_price = df_eda.groupby('Facing')['Price_per_SqFt'].mean().sort_values(ascending=True)\n",
    "\n",
    "fig15 = go.Figure(go.Bar(\n",
    "    x=facing_price.values,\n",
    "    y=facing_price.index,\n",
    "    orientation='h',\n",
    "    marker=dict(color='gold', line=dict(color='black', width=1)),\n",
    "    text=facing_price.values.round(2),\n",
    "    textposition='outside'\n",
    "))\n",
    "\n",
    "fig15.update_layout(\n",
    "    title=\"Average Price per Sq Ft by Facing Direction\",\n",
    "    xaxis_title=\"Average Price per Sq Ft\",\n",
    "    yaxis_title=\"Facing Direction\",\n",
    "    height=500\n",
    ")\n",
    "fig15.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fafd697",
   "metadata": {},
   "source": [
    "### 1. Why did you pick the specific chart?\n",
    "\n",
    "* **Vastu/Directional Premium Check:** Chosen to investigate if a commonly recognized categorical factor in real estate, particularly in certain markets (like India, due to Vastu/Feng Shui), impacts the valuation metric (Average Price per Sq Ft).\n",
    "* **Data Uniformity Check:** This is the final check to see if the synthetic uniformity observed in all previous variables (Price, Size, BHK, Furnishing, Amenities) also extends to property orientation.\n",
    "\n",
    "### 2. What is/are the insight(s) found from the chart?\n",
    "\n",
    "* **Zero Price Variation:** The primary insight is that the **Average Price per Sq Ft** is **exactly the same** ($\\mathbf{0.13}$) for properties facing **West, South, East, and North**. All four horizontal bars are of identical length.\n",
    "* **Contradiction to Reality:** In many real estate markets, especially those influenced by Vastu Shastra, an **East or North-facing** property often commands a measurable **price premium** over South or West-facing properties . The observed zero price difference confirms that the valuation is **completely unresponsive** to property orientation.\n",
    "* **Final Confirmation of Synthetic Data:** This result provides the final piece of evidence corroborating all previous findings. The price per square foot remains constant and uniformly distributed regardless of continuous factors (Size, Amenities) or categorical factors (BHK, Furnishing, **Facing Direction**).\n",
    "\n",
    "### 3. Will the gained insights help creating a positive business impact?\n",
    "\n",
    "* **Negative Impact is Absolute:** The insight confirms that the dataset cannot model price variation based on fundamental property attributes like its orientation.\n",
    "* **Justification for Negative Growth:**\n",
    "    * **Flawed Marketing/Sales Strategy:** The business is unable to strategically price East-facing or North-facing homes higher, losing out on potential revenue from buyers willing to pay a premium for favorable direction.\n",
    "    * **Incorrect Inventory Management:** The company cannot prioritize the sale or acquisition of favorably oriented properties, treating all inventory equally despite clear differences in real-world demand.\n",
    "    * **Actionable Step:** Given that every chart presented (Price Distribution, Price vs. Size, Price vs. Amenities, Price vs. Furnishing, and Price vs. Facing Direction) has demonstrated an unnatural, uniform, or impossible pattern, the only positive business impact is the **irrefutable conclusion that the data is unusable and must be replaced immediately** to avoid catastrophic strategic errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc4a39a",
   "metadata": {
    "papermill": {
     "duration": 0.496893,
     "end_time": "2025-12-07T12:16:21.650194",
     "exception": false,
     "start_time": "2025-12-07T12:16:21.153301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chart 16: Properties by Owner Type\n",
    "print(\"\\n Chart 16: Distribution of Properties by Owner Type\")\n",
    "owner_counts = df_eda['Owner_Type'].value_counts()\n",
    "\n",
    "fig16 = go.Figure(data=[go.Pie(\n",
    "    labels=owner_counts.index,\n",
    "    values=owner_counts.values,\n",
    "    hole=0.3,\n",
    "    marker=dict(colors=px.colors.qualitative.Set2)\n",
    ")])\n",
    "\n",
    "fig16.update_layout(\n",
    "    title=\"Distribution of Properties by Owner Type\",\n",
    "    height=500\n",
    ")\n",
    "fig16.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e510982b",
   "metadata": {},
   "source": [
    "### 1. Why did you pick the specific chart?\n",
    "\n",
    "* **Market Representation Check:** Chosen to investigate the distribution of property listings based on who is selling them. This is a crucial metric for understanding market segmentation, commission structures, and sales focus.\n",
    "* **Final Uniformity Check:** This serves as a final check to see if the synthetic uniformity observed across all other continuous and categorical variables also applies to the market participant distribution.\n",
    "\n",
    "### 2. What is/are the insight(s) found from the chart?\n",
    "\n",
    "* **Perfectly Uniform Distribution:** The primary insight is that the distribution of properties by Owner Type is **perfectly uniform**:\n",
    "    * **Broker:** $\\mathbf{33.4\\%}$\n",
    "    * **Owner:** $\\mathbf{33.3\\%}$\n",
    "    * **Builder:** $\\mathbf{33.3\\%}$\n",
    "* **Contradiction to Reality:** A real-world property market rarely, if ever, exhibits a perfectly equal distribution of listings among these three parties . Most markets are typically dominated by **Brokers/Agents** (higher proportion) and **Builders** (in new construction markets), with a smaller share held by direct **Owners**.\n",
    "* **Final Confirmation of Synthetic Data:** This provides the last piece of overwhelming evidence. The data creator has applied an unnatural, uniform distribution to every single variable analyzed so far, including the proportion of market participants.\n",
    "\n",
    "### 3. Will the gained insights help creating a positive business impact?\n",
    "\n",
    "* **Negative Impact is Absolute:** This chart confirms that the dataset cannot be trusted to model or analyze market dynamics, as the representation of market participants is entirely unrealistic.\n",
    "* **Justification for Negative Growth:**\n",
    "    * **Flawed Sales and Commission Strategy:** The business would be led to believe that sales resources and commission structures should be allocated equally to all three channels (Broker, Owner, Builder). In reality, targeting the dominant channel (usually Broker or Builder) is essential for sales efficiency.\n",
    "    * **Inaccurate Market Segmentation:** The company cannot identify true market competition or opportunities. For example, relying on the Builder segment (which is perfectly $\\mathbf{33.3\\%}$) might be disastrous if the real-world builder market share is only $\\mathbf{10\\%}$.\n",
    "    * **Actionable Step:** Given that every single chart presented has demonstrated an unnatural, uniform, or mathematically impossible pattern, the ultimate positive impact is the **irrefutable conclusion that the data is unusable and must be replaced immediately** to avoid catastrophic strategic errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4921145f",
   "metadata": {
    "papermill": {
     "duration": 0.467443,
     "end_time": "2025-12-07T12:16:22.556990",
     "exception": false,
     "start_time": "2025-12-07T12:16:22.089547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chart 17: Properties by Availability Status\n",
    "print(\"\\n Chart 17: Properties by Availability Status\")\n",
    "availability_counts = df_eda['Availability_Status'].value_counts()\n",
    "\n",
    "fig17 = go.Figure(go.Bar(\n",
    "    x=availability_counts.index,\n",
    "    y=availability_counts.values,\n",
    "    marker=dict(color='salmon', line=dict(color='black', width=1)),\n",
    "    text=availability_counts.values,\n",
    "    textposition='outside'\n",
    "))\n",
    "\n",
    "fig17.update_layout(\n",
    "    title=\"Distribution of Properties by Availability Status\",\n",
    "    xaxis_title=\"Availability Status\",\n",
    "    yaxis_title=\"Number of Properties\",\n",
    "    height=500\n",
    ")\n",
    "fig17.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfbaa33",
   "metadata": {},
   "source": [
    "### 1. Why did you pick the specific chart?\n",
    "\n",
    "* **Inventory Status Check:** Chosen to investigate the mix of inventory based on completion status. This is critical for sales strategies, financing, and forecasting future revenue streams (e.g., immediate sales vs. future commitments).\n",
    "* **Final Uniformity Check:** This serves as a final, comprehensive check to see if the **unnatural uniformity** observed across all other aspects of the data (price, size, BHK, amenities, owner type) also extends to the inventory's availability status.\n",
    "\n",
    "### 2. What is/are the insight(s) found from the chart?\n",
    "\n",
    "* **Perfectly Equal Distribution:** The primary insight is that the number of properties **Under\\_Construction** ($\\mathbf{125,035}$) is **virtually identical** to the number of properties **Ready\\_to\\_Move** ($\\mathbf{124,965}$). The two categories are split almost exactly $\\mathbf{50\\%}$ / $\\mathbf{50\\%}$.\n",
    "* **Contradiction to Reality:** In a real-world market, the distribution between these two categories is dynamic and rarely this precisely equal. It is usually driven by economic cycles, new project launches, and absorption rates, leading to an unequal split. A $50/50$ split strongly suggests that the data creator simply split the records exactly in half.\n",
    "* **Confirms Synthetic Data Structure:** This finding provides the definitive capstone evidence that the dataset is **synthetic**. Every chart analyzed—from price distribution and correlations to categorical factors like furnishing, facing direction, owner type, and now availability status—has exhibited an unnatural, uniform, or impossible distribution, confirming that the data was generated and is not reflective of a real market.\n",
    "\n",
    "### 3. Will the gained insights help creating a positive business impact?\n",
    "\n",
    "* **Negative Impact is Absolute:** The insight confirms that the inventory balance is artificial and cannot be used for any operational or financial planning.\n",
    "* **Justification for Negative Growth:**\n",
    "    * **Flawed Sales and Marketing Focus:** The business would be led to believe that sales efforts should be split 50/50 between the two segments. This fails to account for real market demand, which might heavily favor one over the other (e.g., a tight market favors Ready-to-Move, while an optimistic market favors Under-Construction for better pricing).\n",
    "    * **Inaccurate Financial Forecasting:** The company cannot accurately forecast cash flow or risk, as the data provides a false sense of balance between immediate revenue (Ready-to-Move) and future commitment/risk (Under-Construction).\n",
    "    * **Actionable Step:** Since every variable checked is flawed, the only positive impact is the **final confirmation** that the dataset is **irrecoverable for business use** and its replacement is the single most urgent task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d91361",
   "metadata": {
    "papermill": {
     "duration": 0.500347,
     "end_time": "2025-12-07T12:16:23.489021",
     "exception": false,
     "start_time": "2025-12-07T12:16:22.988674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chart 18: Parking Space Effect on Price\n",
    "print(\"\\n Chart 18: Parking Space vs Property Price\")\n",
    "parking_price = df_eda.groupby('Parking_Space')['Price_in_Lakhs'].mean()\n",
    "\n",
    "fig18 = go.Figure()\n",
    "fig18.add_trace(go.Scatter(\n",
    "    x=parking_price.index,\n",
    "    y=parking_price.values,\n",
    "    mode='lines+markers',\n",
    "    marker=dict(size=10, color='navy'),\n",
    "    line=dict(width=3, color='navy')\n",
    "))\n",
    "\n",
    "fig18.update_layout(\n",
    "    title=\"Effect of Parking Space on Property Price\",\n",
    "    xaxis_title=\"Number of Parking Spaces\",\n",
    "    yaxis_title=\"Average Price (in Lakhs)\",\n",
    "    height=500\n",
    ")\n",
    "fig18.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e18e29",
   "metadata": {},
   "source": [
    "### 1. Why did you pick the specific chart?\n",
    "\n",
    "* **Feature Valuation Check:** Chosen to investigate the impact of a fundamental and high-value property feature, **Parking Space**, on the average property price. Parking is typically a significant price driver, especially in urban areas.\n",
    "* **Data Uniformity Check:** This serves as a final, micro-level check to see if the overall price is unresponsive to even small, binary features, confirming the comprehensive nature of the data flaw.\n",
    "\n",
    "### 2. What is/are the insight(s) found from the chart?\n",
    "\n",
    "* **Minimal Price Difference:** There is an *extremely* minor difference in the Average Price:\n",
    "    * **No Parking:** $\\approx 254.435$ Lakhs\n",
    "    * **Yes Parking:** $\\approx 254.745$ Lakhs\n",
    "* **The price difference is $\\mathbf{0.31}$ Lakhs (or $\\mathbf{31,000}$ Rupees)**, which is negligible given the average price is over 254 Lakhs.\n",
    "* **Contradiction to Reality:** In a real-world market, the presence of a parking space (especially a dedicated one) typically results in a **significant and substantial price premium**, often ranging from 1 to 10 Lakhs or more, depending on the location . A price difference of only $\\mathbf{31,000}$ Rupees is economically irrational.\n",
    "* **Confirms Synthetic Data Structure:** While the line slopes slightly upward (suggesting *some* influence), the magnitude of the effect is so trivial that it reinforces the central finding: the pricing structure is **almost perfectly uniform** and unresponsive to real-world value drivers. This is consistent with the lack of effect seen with Furnishing, Facing Direction, and Amenities.\n",
    "\n",
    "### 3. Will the gained insights help creating a positive business impact?\n",
    "\n",
    "* **Negative Impact is Absolute:** The insight confirms that the dataset fails to model the financial impact of essential, high-value property features like parking.\n",
    "* **Justification for Negative Growth:**\n",
    "    * **Flawed Cost-Benefit Analysis (CBA):** The business would conclude that the cost of providing a parking space is not justified by the minimal price increase, leading to investment decisions that fail to meet market demand for properties with parking.\n",
    "    * **Misleading Pricing Strategy:** Sales teams would not be able to justify any meaningful premium for a property with dedicated parking, leading to **undervaluation** of superior inventory and loss of potential revenue.\n",
    "    * **Actionable Step:** Every single chart provided across the entire analysis (price, size, amenities, furnishing, owner type, availability, and parking) has now been proven flawed. The only remaining positive action is the **immediate halting of all business strategy based on this irrecoverable dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca9152",
   "metadata": {
    "papermill": {
     "duration": 0.496651,
     "end_time": "2025-12-07T12:16:24.444944",
     "exception": false,
     "start_time": "2025-12-07T12:16:23.948293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Chart 19: Amenities Effect on Price per Sq Ft\n",
    "print(\"\\n Chart 19: Amenities vs Price per Sq Ft (Top 10)\")\n",
    "amenities_price = df_eda.groupby('Amenities')['Price_per_SqFt'].mean().sort_values(ascending=True).tail(10)\n",
    "\n",
    "fig19 = go.Figure(go.Bar(\n",
    "    x=amenities_price.values,\n",
    "    y=amenities_price.index,\n",
    "    orientation='h',\n",
    "    marker=dict(color='lime', line=dict(color='black', width=1)),\n",
    "    text=amenities_price.values.round(2),\n",
    "    textposition='outside'\n",
    "))\n",
    "\n",
    "fig19.update_layout(\n",
    "    title=\"Top 10 Amenities by Average Price per Sq Ft\",\n",
    "    xaxis_title=\"Average Price per Sq Ft\",\n",
    "    yaxis_title=\"Amenities\",\n",
    "    height=600\n",
    ")\n",
    "fig19.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31038da8",
   "metadata": {},
   "source": [
    "### 1. Why did you pick the specific chart?\n",
    "\n",
    "* **Amenity Package Valuation:** Chosen to investigate if complex combinations of high-value amenities (like a pool, gym, and clubhouse) drive a price premium, which is expected in real estate.\n",
    "* **Final Uniformity Check:** This serves as the ultimate check for the dataset's flaws, examining if the price is unresponsive to the presence of multiple, desirable luxury features.\n",
    "\n",
    "### 2. What is/are the insight(s) found from the chart?\n",
    "\n",
    "* **Near-Zero Price Variation:** The primary insight is that the **Average Price per Sq Ft** for all ten different combinations of high-end amenities is **virtually identical**.\n",
    "    * The top three combinations are priced at $\\mathbf{0.15}$.\n",
    "    * The remaining seven combinations are priced at $\\mathbf{0.14}$.\n",
    "* **Contradiction to Reality:** In a real-world market, a property offering a **Clubhouse, Pool, Gym, and Garden** should command a substantially and measurably higher price per square foot than one offering a basic set of amenities . The difference between the highest and lowest price points here ($\\mathbf{0.01}$) is negligible and economically irrational.\n",
    "* **Confirms Synthetic Data Structure:** This chart is the final piece of overwhelming evidence. It shows that even luxury amenity packages—which should be major price drivers—have almost no impact on the price per square foot. This confirms the **uniform and unresponsive pricing structure** that permeates the entire dataset, a hallmark of synthetic data.\n",
    "\n",
    "### 3. Will the gained insights help creating a positive business impact?\n",
    "\n",
    "* **Negative Impact is Absolute:** The insight confirms that the dataset fails to model the financial impact of essential, high-value amenity packages.\n",
    "* **Justification for Negative Growth:**\n",
    "    * **Flawed Investment in Luxury:** The business would be misled to believe that investing substantial capital in high-end amenities (pools, gyms) provides **no return in price premium**, leading to a failure to develop competitive, high-value properties.\n",
    "    * **Misleading Pricing Strategy:** Sales teams cannot justify any meaningful premium for a property based on its luxury amenity package, resulting in the **undervaluation** of premium inventory and a loss of potential revenue.\n",
    "    * **Actionable Step:** Having now analyzed every single variable across all provided charts, the **absolute and final conclusion** is that the data is irrecoverable for business use. The only remaining positive action is to **immediately replace the dataset** to prevent catastrophic strategic errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9f8621",
   "metadata": {
    "papermill": {
     "duration": 0.477602,
     "end_time": "2025-12-07T12:16:25.355605",
     "exception": false,
     "start_time": "2025-12-07T12:16:24.878003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chart 20: Public Transport Accessibility vs Price\n",
    "print(\"\\n Chart 20: Public Transport Accessibility vs Price per Sq Ft\")\n",
    "transport_price = df_eda.groupby('Public_Transport_Accessibility')['Price_per_SqFt'].mean()\n",
    "\n",
    "fig20 = go.Figure(go.Bar(\n",
    "    x=transport_price.index,\n",
    "    y=transport_price.values,\n",
    "    marker=dict(color='purple', line=dict(color='black', width=1)),\n",
    "    text=transport_price.values.round(2),\n",
    "    textposition='outside'\n",
    "))\n",
    "\n",
    "fig20.update_layout(\n",
    "    title=\"Public Transport Accessibility vs Price per Sq Ft\",\n",
    "    xaxis_title=\"Public Transport Accessibility Rating\",\n",
    "    yaxis_title=\"Average Price per Sq Ft\",\n",
    "    height=500\n",
    ")\n",
    "fig20.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7464ac5a",
   "metadata": {},
   "source": [
    "### 1. Why did you pick the specific chart?\n",
    "\n",
    "* **Location Valuation Check:** Chosen to investigate the impact of **transport accessibility**—a primary driver of real estate value in urban and suburban markets—on the valuation metric (Average Price per Sq Ft).\n",
    "* **Final Uniformity Check:** This serves as the conclusive test for the dataset's flaw, examining if the price is responsive to a critical infrastructure factor.\n",
    "\n",
    "### 2. What is/are the insight(s) found from the chart?\n",
    "\n",
    "* **Zero Price Variation:** The primary and most significant insight is that the **Average Price per Sq Ft** is **exactly the same** ($\\mathbf{0.13}$) for properties rated as **High, Low, and Medium** in Public Transport Accessibility. All three bars are of identical height.\n",
    "* **Contradiction to Reality:** In a functional market, properties with **High** public transport accessibility (e.g., near metro stations, major bus routes) should command a **measurable price premium** over properties with **Low** accessibility . The complete absence of any price difference confirms that the valuation is entirely **unresponsive** to a key location factor.\n",
    "* **Confirms Synthetic Data Structure:** This chart completes the picture: the pricing structure exhibits $\\mathbf{0\\%}$ variation across amenities, furnishing status, directional facing, and now, public transport accessibility. This irrefutably confirms the data is **synthetic** and does not model real-world market dynamics.\n",
    "\n",
    "### 3. Will the gained insights help creating a positive business impact?\n",
    "\n",
    "* **Negative Impact is Absolute:** The insight confirms that the dataset cannot be used to model any strategic pricing or investment decisions related to location and infrastructure.\n",
    "* **Justification for Negative Growth:**\n",
    "    * **Flawed Investment in Location:** The business would be misled into believing there is no financial benefit to acquiring or listing properties near highly accessible public transport. This would lead to a failure to capitalize on market segments willing to pay a premium for convenience.\n",
    "    * **Misleading Pricing Strategy:** Sales teams cannot justify differential pricing based on transport access, resulting in the **undervaluation** of superiorly located inventory.\n",
    "    * **Actionable Step:** Every single variable across every chart provided (Price, Size, Amenities, Furnishing, Availability, Owner Type, and now Public Transport Accessibility) has been proven flawed. The only remaining positive action is to **immediately replace the dataset** to prevent strategic errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178286a1",
   "metadata": {
    "papermill": {
     "duration": 0.427565,
     "end_time": "2025-12-07T12:16:26.223542",
     "exception": false,
     "start_time": "2025-12-07T12:16:25.795977",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ***5. Hypothesis Testing***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eb5939",
   "metadata": {
    "papermill": {
     "duration": 0.441933,
     "end_time": "2025-12-07T12:16:27.095063",
     "exception": false,
     "start_time": "2025-12-07T12:16:26.653130",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Hypothetical Statement - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe18673",
   "metadata": {
    "papermill": {
     "duration": 0.437889,
     "end_time": "2025-12-07T12:16:27.972624",
     "exception": false,
     "start_time": "2025-12-07T12:16:27.534735",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c2c16a",
   "metadata": {
    "papermill": {
     "duration": 0.449098,
     "end_time": "2025-12-07T12:16:28.880374",
     "exception": false,
     "start_time": "2025-12-07T12:16:28.431276",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Null Hypothesis (H0): Property Type does NOT affect Price per Sq Ft\n",
    "\n",
    "Alternative Hypothesis (H1): Property Type DOES affect Price per Sq Ft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48af9a2c",
   "metadata": {
    "papermill": {
     "duration": 0.447804,
     "end_time": "2025-12-07T12:16:29.781673",
     "exception": false,
     "start_time": "2025-12-07T12:16:29.333869",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 2. Perform an appropriate statistical test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36401126",
   "metadata": {
    "papermill": {
     "duration": 0.729596,
     "end_time": "2025-12-07T12:16:30.948366",
     "exception": false,
     "start_time": "2025-12-07T12:16:30.218770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare data - drop missing values\n",
    "df_hypothesis = df_clean.copy() \n",
    "\n",
    "df_h1 = df_hypothesis.dropna(subset=['Property_Type', 'Price_per_SqFt'])\n",
    "\n",
    "# Group data by property type\n",
    "property_groups = [group['Price_per_SqFt'].values \n",
    "                   for name, group in df_h1.groupby('Property_Type')]\n",
    "\n",
    "# Perform ANOVA\n",
    "f_stat, p_value = f_oneway(*property_groups)\n",
    "\n",
    "print(f\"\\n Results:\")\n",
    "print(f\"   F-statistic: {f_stat:.4f}\")\n",
    "print(f\"   P-value: {p_value:.6f}\")\n",
    "print(f\"   Alpha (α): 0.05\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"\\n Decision: REJECT H0 (p-value = {p_value:.6f} < 0.05)\")\n",
    "    print(\"Conclusion: Property Type SIGNIFICANTLY affects Price per Sq Ft\")\n",
    "    print(\"Business Insight: Different property types have statistically different pricing.\")\n",
    "else:\n",
    "    print(f\"\\n Decision: FAIL TO REJECT H0 (p-value = {p_value:.6f} >= 0.05)\")\n",
    "    print(\"Conclusion: No significant difference in Price per Sq Ft across Property Types\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n Summary Statistics by Property Type:\")\n",
    "summary_h1 = df_h1.groupby('Property_Type')['Price_per_SqFt'].agg(['mean', 'median', 'std', 'count'])\n",
    "print(summary_h1.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dc83c3",
   "metadata": {
    "papermill": {
     "duration": 0.433787,
     "end_time": "2025-12-07T12:16:31.838891",
     "exception": false,
     "start_time": "2025-12-07T12:16:31.405104",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### Which statistical test have you done to obtain P-Value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0ede33",
   "metadata": {
    "papermill": {
     "duration": 0.443975,
     "end_time": "2025-12-07T12:16:32.723160",
     "exception": false,
     "start_time": "2025-12-07T12:16:32.279185",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Statistical Test: One-Way ANOVA\n",
    "\n",
    "Significance Level: α = 0.05\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991d43e6",
   "metadata": {
    "papermill": {
     "duration": 0.436132,
     "end_time": "2025-12-07T12:16:33.597150",
     "exception": false,
     "start_time": "2025-12-07T12:16:33.161018",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### Why did you choose the specific statistical test?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147626cb",
   "metadata": {
    "papermill": {
     "duration": 0.44108,
     "end_time": "2025-12-07T12:16:34.477376",
     "exception": false,
     "start_time": "2025-12-07T12:16:34.036296",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Compares continuous variable (Price per Sq Ft) across multiple categorical groups (Property Types). Efficiently tests if at least one group mean differs in a single test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1849acb",
   "metadata": {
    "papermill": {
     "duration": 0.440482,
     "end_time": "2025-12-07T12:16:35.350555",
     "exception": false,
     "start_time": "2025-12-07T12:16:34.910073",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Hypothetical Statement - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372b84ff",
   "metadata": {
    "papermill": {
     "duration": 0.433432,
     "end_time": "2025-12-07T12:16:36.221633",
     "exception": false,
     "start_time": "2025-12-07T12:16:35.788201",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6216b9ab",
   "metadata": {
    "papermill": {
     "duration": 0.442218,
     "end_time": "2025-12-07T12:16:37.096379",
     "exception": false,
     "start_time": "2025-12-07T12:16:36.654161",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Null Hypothesis (H0): Furnished Status does NOT affect Property Price\n",
    "\n",
    "Alternative Hypothesis (H1): Furnished Status DOES affect Property Price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6892db0a",
   "metadata": {
    "papermill": {
     "duration": 0.440713,
     "end_time": "2025-12-07T12:16:37.982486",
     "exception": false,
     "start_time": "2025-12-07T12:16:37.541773",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 2. Perform an appropriate statistical test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27eda03",
   "metadata": {
    "papermill": {
     "duration": 0.596838,
     "end_time": "2025-12-07T12:16:39.041182",
     "exception": false,
     "start_time": "2025-12-07T12:16:38.444344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "df_h2 = df_hypothesis.dropna(subset=['Furnished_Status', 'Price_in_Lakhs'])\n",
    "\n",
    "# Group data\n",
    "furnished_groups = [group['Price_in_Lakhs'].values \n",
    "                    for name, group in df_h2.groupby('Furnished_Status')]\n",
    "\n",
    "# Perform ANOVA\n",
    "f_stat_h2, p_value_h2 = f_oneway(*furnished_groups)\n",
    "\n",
    "print(f\"\\n Results:\")\n",
    "print(f\"   F-statistic: {f_stat_h2:.4f}\")\n",
    "print(f\"   P-value: {p_value_h2:.6f}\")\n",
    "print(f\"   Alpha (α): 0.05\")\n",
    "\n",
    "if p_value_h2 < 0.05:\n",
    "    print(f\"\\n Decision: REJECT H0 (p-value = {p_value_h2:.6f} < 0.05)\")\n",
    "    print(\"Conclusion: Furnished Status SIGNIFICANTLY affects Property Price\")\n",
    "    print(\"Business Insight: Furnishing level impacts property pricing significantly.\")\n",
    "else:\n",
    "    print(f\"\\n Decision: FAIL TO REJECT H0 (p-value = {p_value_h2:.6f} >= 0.05)\")\n",
    "    print(\"Conclusion: No significant difference in Price across Furnished Status\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n Summary Statistics by Furnished Status:\")\n",
    "summary_h2 = df_h2.groupby('Furnished_Status')['Price_in_Lakhs'].agg(['mean', 'median', 'std', 'count'])\n",
    "print(summary_h2.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e068566",
   "metadata": {
    "papermill": {
     "duration": 0.441462,
     "end_time": "2025-12-07T12:16:39.933952",
     "exception": false,
     "start_time": "2025-12-07T12:16:39.492490",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### Which statistical test have you done to obtain P-Value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998645a6",
   "metadata": {
    "papermill": {
     "duration": 0.440381,
     "end_time": "2025-12-07T12:16:40.812070",
     "exception": false,
     "start_time": "2025-12-07T12:16:40.371689",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Statistical Test: One-Way ANOVA\n",
    "\n",
    "Significance Level: α = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800cfad0",
   "metadata": {
    "papermill": {
     "duration": 0.434834,
     "end_time": "2025-12-07T12:16:41.690347",
     "exception": false,
     "start_time": "2025-12-07T12:16:41.255513",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### Why did you choose the specific statistical test?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82c9579",
   "metadata": {
    "papermill": {
     "duration": 0.447928,
     "end_time": "2025-12-07T12:16:42.576529",
     "exception": false,
     "start_time": "2025-12-07T12:16:42.128601",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Tests if Property Price differs across three Furnished Status categories. Handles multiple group comparisons simultaneously while controlling for Type I error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696be2c0",
   "metadata": {
    "papermill": {
     "duration": 0.438709,
     "end_time": "2025-12-07T12:16:43.468386",
     "exception": false,
     "start_time": "2025-12-07T12:16:43.029677",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Hypothetical Statement - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb5828c",
   "metadata": {
    "papermill": {
     "duration": 0.444545,
     "end_time": "2025-12-07T12:16:44.346086",
     "exception": false,
     "start_time": "2025-12-07T12:16:43.901541",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df16ea0",
   "metadata": {
    "papermill": {
     "duration": 0.433309,
     "end_time": "2025-12-07T12:16:45.222622",
     "exception": false,
     "start_time": "2025-12-07T12:16:44.789313",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Null Hypothesis (H0): No correlation between Nearby Schools and Price per Sq Ft (ρ = 0)\n",
    "\n",
    "Alternative Hypothesis (H1): There IS a correlation (ρ ≠ 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868f40da",
   "metadata": {
    "papermill": {
     "duration": 0.443626,
     "end_time": "2025-12-07T12:16:46.098362",
     "exception": false,
     "start_time": "2025-12-07T12:16:45.654736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 2. Perform an appropriate statistical test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5036ce52",
   "metadata": {
    "papermill": {
     "duration": 0.481696,
     "end_time": "2025-12-07T12:16:47.023760",
     "exception": false,
     "start_time": "2025-12-07T12:16:46.542064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HYPOTHESIS 3: Correlation between Nearby Schools and Price per Sq Ft\n",
    "\n",
    "# Prepare data\n",
    "df_h3 = df_hypothesis.dropna(subset=['Nearby_Schools', 'Price_per_SqFt'])\n",
    "\n",
    "# Perform correlation test\n",
    "correlation_coef, p_value_h3 = pearsonr(df_h3['Nearby_Schools'], \n",
    "                                        df_h3['Price_per_SqFt'])\n",
    "\n",
    "print(f\"\\n Results:\")\n",
    "print(f\"   Correlation Coefficient (r): {correlation_coef:.4f}\")\n",
    "print(f\"   P-value: {p_value_h3:.6f}\")\n",
    "print(f\"   Alpha (α): 0.05\")\n",
    "\n",
    "if p_value_h3 < 0.05:\n",
    "    print(f\"\\n Decision: REJECT H0 (p-value = {p_value_h3:.6f} < 0.05)\")\n",
    "    if correlation_coef > 0:\n",
    "        print(f\"Conclusion: POSITIVE correlation exists (r = {correlation_coef:.4f})\")\n",
    "        print(\"Business Insight: More nearby schools → Higher price per sq ft\")\n",
    "    else:\n",
    "        print(f\"Conclusion: NEGATIVE correlation exists (r = {correlation_coef:.4f})\")\n",
    "        print(\"Business Insight: More nearby schools → Lower price per sq ft\")\n",
    "else:\n",
    "    print(f\"\\n Decision: FAIL TO REJECT H0 (p-value = {p_value_h3:.6f} >= 0.05)\")\n",
    "    print(\"Conclusion: No significant correlation between Nearby Schools and Price\")\n",
    "\n",
    "# Interpretation of correlation strength\n",
    "abs_corr = abs(correlation_coef)\n",
    "if abs_corr < 0.3:\n",
    "    strength = \"Weak\"\n",
    "elif abs_corr < 0.7:\n",
    "    strength = \"Moderate\"\n",
    "else:\n",
    "    strength = \"Strong\"\n",
    "print(f\"Correlation Strength: {strength}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e6bbea",
   "metadata": {
    "papermill": {
     "duration": 0.432243,
     "end_time": "2025-12-07T12:16:47.901244",
     "exception": false,
     "start_time": "2025-12-07T12:16:47.469001",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### Which statistical test have you done to obtain P-Value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41379cce",
   "metadata": {
    "papermill": {
     "duration": 0.442374,
     "end_time": "2025-12-07T12:16:48.803668",
     "exception": false,
     "start_time": "2025-12-07T12:16:48.361294",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Pearson Correlation Test\n",
    "\n",
    "Significance Level: α = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a945ec3",
   "metadata": {
    "papermill": {
     "duration": 0.445136,
     "end_time": "2025-12-07T12:16:49.701129",
     "exception": false,
     "start_time": "2025-12-07T12:16:49.255993",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### Why did you choose the specific statistical test?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40509547",
   "metadata": {
    "papermill": {
     "duration": 0.445364,
     "end_time": "2025-12-07T12:16:50.578149",
     "exception": false,
     "start_time": "2025-12-07T12:16:50.132785",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Both variables are continuous numerical, measuring strength and direction of their linear relationship. Provides correlation coefficient and significance test for linear associations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5faa9e",
   "metadata": {
    "papermill": {
     "duration": 0.439651,
     "end_time": "2025-12-07T12:16:51.460425",
     "exception": false,
     "start_time": "2025-12-07T12:16:51.020774",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ***6. Feature Engineering & Data Pre-processing***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4bc009",
   "metadata": {
    "papermill": {
     "duration": 0.44923,
     "end_time": "2025-12-07T12:16:52.342112",
     "exception": false,
     "start_time": "2025-12-07T12:16:51.892882",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1. Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9631717b",
   "metadata": {
    "papermill": {
     "duration": 2.187453,
     "end_time": "2025-12-07T12:16:54.969113",
     "exception": false,
     "start_time": "2025-12-07T12:16:52.781660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_features(df):\n",
    "    \"\"\"Create ENHANCED features for 90%+ accuracy - NO PRICE LEAKAGE\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 1: ENHANCED FEATURE ENGINEERING (90%+ TARGET)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    df_fe = df.copy()\n",
    "    initial_cols = df_fe.shape[1]\n",
    "    \n",
    "    # Convert numeric columns\n",
    "    print(\"\\n1. Converting numeric columns...\")\n",
    "    numeric_cols = ['Age_of_Property', 'Size_in_SqFt', 'Price_in_Lakhs', \n",
    "                    'Nearby_Schools', 'Nearby_Hospitals', 'Floor_No', 'Total_Floors', 'BHK']\n",
    "    for col in numeric_cols:\n",
    "        if col in df_fe.columns:\n",
    "            df_fe[col] = pd.to_numeric(df_fe[col], errors='coerce').fillna(0)\n",
    "    print(f\"   ✓ Converted {len(numeric_cols)} columns to numeric\")\n",
    "    \n",
    "    # Infrastructure Score\n",
    "    print(\"\\n2. Infrastructure Score (Enhanced)\")\n",
    "    transport_map = {'High': 2, 'Medium': 1, 'Low': 0}\n",
    "    df_fe['Transport_Score'] = df_fe['Public_Transport_Accessibility'].map(transport_map).fillna(0)\n",
    "    df_fe['Infrastructure_Score'] = (\n",
    "        df_fe['Nearby_Schools'] * 0.4 + \n",
    "        df_fe['Nearby_Hospitals'] * 0.3 + \n",
    "        df_fe['Transport_Score'] * 0.3\n",
    "    )\n",
    "    df_fe['Total_Infrastructure'] = df_fe['Nearby_Schools'] + df_fe['Nearby_Hospitals']\n",
    "    print(f\"   ✓ Created 3 infrastructure features\")\n",
    "    \n",
    "    # Location Quality (NO PRICE)\n",
    "    print(\"\\n3. Location Quality Features (Price-Independent)\")\n",
    "    city_avg_size = df_fe.groupby('City')['Size_in_SqFt'].transform('mean')\n",
    "    state_avg_size = df_fe.groupby('State')['Size_in_SqFt'].transform('mean')\n",
    "    \n",
    "    df_fe['City_Size_Level'] = city_avg_size\n",
    "    df_fe['State_Size_Level'] = state_avg_size\n",
    "    \n",
    "    city_infrastructure = df_fe.groupby('City')['Infrastructure_Score'].transform('mean')\n",
    "    df_fe['Location_Infrastructure_Quality'] = city_infrastructure\n",
    "    print(f\"   ✓ Created 3 location features (price-independent)\")\n",
    "    \n",
    "    # Property Value Indicators\n",
    "    print(\"\\n4. Property Value Indicators (Enhanced)\")\n",
    "    df_fe['Size_per_BHK'] = df_fe['Size_in_SqFt'] / (df_fe['BHK'] + 1)\n",
    "    df_fe['Floor_Position_Ratio'] = df_fe['Floor_No'] / (df_fe['Total_Floors'] + 1)\n",
    "    df_fe['School_Density'] = df_fe['Nearby_Schools'] / (df_fe['Age_of_Property'] + 1)\n",
    "    df_fe['Hospital_Density'] = df_fe['Nearby_Hospitals'] / (df_fe['Age_of_Property'] + 1)\n",
    "    print(f\"   ✓ Created 4 value indicators\")\n",
    "    \n",
    "    # Amenities Features\n",
    "    print(\"\\n5. Amenities Features (Enhanced)\")\n",
    "    df_fe['Amenities_Count'] = df_fe['Amenities'].str.split(',').str.len().fillna(0)\n",
    "    df_fe['Has_Pool'] = df_fe['Amenities'].str.contains('Pool', case=False, na=False).astype(int)\n",
    "    df_fe['Has_Gym'] = df_fe['Amenities'].str.contains('Gym', case=False, na=False).astype(int)\n",
    "    df_fe['Has_Clubhouse'] = df_fe['Amenities'].str.contains('Clubhouse', case=False, na=False).astype(int)\n",
    "    df_fe['Premium_Amenities'] = df_fe['Has_Pool'] + df_fe['Has_Gym'] + df_fe['Has_Clubhouse']\n",
    "    print(f\"   ✓ Created 5 amenity features\")\n",
    "    \n",
    "    # Boolean Flags\n",
    "    print(\"\\n6. Boolean Flags (Enhanced)\")\n",
    "    df_fe['Has_Parking'] = (df_fe['Parking_Space'] == 'Yes').astype(int)\n",
    "    df_fe['Has_Security'] = (df_fe['Security'] == 'Yes').astype(int)\n",
    "    df_fe['Is_New_Property'] = (df_fe['Age_of_Property'] <= 5).astype(int)\n",
    "    df_fe['Is_Mid_Age'] = ((df_fe['Age_of_Property'] > 5) & (df_fe['Age_of_Property'] <= 15)).astype(int)\n",
    "    df_fe['Is_Top_Floor'] = (df_fe['Floor_No'] == df_fe['Total_Floors']).astype(int)\n",
    "    df_fe['Is_Ground_Floor'] = (df_fe['Floor_No'] == 0).astype(int)\n",
    "    df_fe['Is_Ready_to_Move'] = (df_fe['Availability_Status'] == 'Ready_to_Move').astype(int)\n",
    "    df_fe['Is_Large_Property'] = (df_fe['Size_in_SqFt'] > df_fe['Size_in_SqFt'].median()).astype(int)\n",
    "    df_fe['Is_High_BHK'] = (df_fe['BHK'] >= 3).astype(int)\n",
    "    print(f\"   ✓ Created 9 boolean flags\")\n",
    "    \n",
    "    # Interaction Features\n",
    "    print(\"\\n7. Interaction Features\")\n",
    "    df_fe['BHK_x_Size'] = df_fe['BHK'] * df_fe['Size_in_SqFt']\n",
    "    df_fe['Age_x_Infrastructure'] = df_fe['Age_of_Property'] * df_fe['Infrastructure_Score']\n",
    "    df_fe['BHK_x_Amenities'] = df_fe['BHK'] * df_fe['Amenities_Count']\n",
    "    print(f\"   ✓ Created 3 interaction features\")\n",
    "    \n",
    "    # Advanced Features\n",
    "    print(\"\\n8. Advanced Location & Quality Features\")\n",
    "    city_bhk_avg = df_fe.groupby('City')['BHK'].transform('mean')\n",
    "    city_size_avg = df_fe.groupby('City')['Size_in_SqFt'].transform('mean')\n",
    "    \n",
    "    df_fe['City_BHK_Level'] = city_bhk_avg\n",
    "    df_fe['Property_vs_City_BHK'] = df_fe['BHK'] / (city_bhk_avg + 1)\n",
    "    df_fe['Property_vs_City_Size'] = df_fe['Size_in_SqFt'] / (city_size_avg + 1)\n",
    "    \n",
    "    locality_infra = df_fe.groupby('Locality')['Infrastructure_Score'].transform('mean')\n",
    "    df_fe['Locality_Quality'] = locality_infra\n",
    "    df_fe['Property_vs_Locality_Quality'] = df_fe['Infrastructure_Score'] / (locality_infra + 1)\n",
    "    \n",
    "    df_fe['Space_per_Person'] = df_fe['Size_in_SqFt'] / ((df_fe['BHK'] * 2) + 1)\n",
    "    df_fe['Modern_Property_Score'] = (\n",
    "        df_fe['Is_New_Property'] * 3 + \n",
    "        df_fe['Premium_Amenities'] * 2 + \n",
    "        df_fe['Has_Security'] * 1 +\n",
    "        df_fe['Has_Parking'] * 1\n",
    "    )\n",
    "    \n",
    "    df_fe['Is_Premium_Property'] = (\n",
    "        (df_fe['BHK'] >= 3) & \n",
    "        (df_fe['Premium_Amenities'] >= 2) & \n",
    "        (df_fe['Has_Security'] == 1)\n",
    "    ).astype(int)\n",
    "    \n",
    "    df_fe['Is_Budget_Property'] = (\n",
    "        (df_fe['BHK'] <= 2) & \n",
    "        (df_fe['Premium_Amenities'] == 0) & \n",
    "        (df_fe['Age_of_Property'] > 15)\n",
    "    ).astype(int)\n",
    "    \n",
    "    print(f\"   ✓ Created 9 advanced features\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FEATURE ENGINEERING COMPLETE\")\n",
    "    print(f\"   New features: {df_fe.shape[1] - initial_cols}\")\n",
    "    print(f\"   Final shape: {df_fe.shape}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return df_fe\n",
    "\n",
    "# Execute Feature Engineering\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"EXECUTING: Feature Engineering\")\n",
    "print(\"=\"*40)\n",
    "df_featured = create_features(df_clean)\n",
    "print(f\"\\n Features created: {df_featured.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a984fdf",
   "metadata": {},
   "source": [
    "### 2. Target Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e830bfbf",
   "metadata": {
    "papermill": {
     "duration": 0.757809,
     "end_time": "2025-12-07T12:16:56.194595",
     "exception": false,
     "start_time": "2025-12-07T12:16:55.436786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_realistic_targets(df, base_appreciation_rate=0.08, years=5):\n",
    "    \"\"\"Create targets with realistic market noise and uncertainty\"\"\"\n",
    "    print(\"\\n Creating realistic targets with market variability...\")\n",
    "    \n",
    "    df_targets = df.copy()\n",
    "    \n",
    "    # Calculate feature-based appreciation (from previous version)\n",
    "    infra_score_norm = df_targets['Infrastructure_Score'] / df_targets['Infrastructure_Score'].max()\n",
    "    infra_boost = infra_score_norm * 0.15\n",
    "    \n",
    "    age_penalty = np.where(df_targets['Age_of_Property'] > 20, -0.10,\n",
    "                  np.where(df_targets['Age_of_Property'] > 10, -0.05,\n",
    "                  np.where(df_targets['Age_of_Property'] <= 5, 0.05, 0)))\n",
    "    \n",
    "    bhk_boost = np.where(df_targets['BHK'] >= 4, 0.10,\n",
    "                np.where(df_targets['BHK'] == 3, 0.05, 0))\n",
    "    \n",
    "    premium_boost = (df_targets['Premium_Amenities'] / 3) * 0.08\n",
    "    modern_boost = df_targets['Is_New_Property'] * 0.05\n",
    "    \n",
    "    city_infra_avg = df_targets.groupby('City')['Infrastructure_Score'].transform('mean')\n",
    "    city_quality = (city_infra_avg / df_targets['Infrastructure_Score'].max())\n",
    "    location_boost = (city_quality - 0.5) * 0.20\n",
    "    \n",
    "    base_multiplier = 1.0 + infra_boost + age_penalty + bhk_boost + premium_boost + modern_boost + location_boost\n",
    "    base_multiplier = base_multiplier.clip(0.70, 1.30)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # ADD REALISTIC MARKET NOISE (This prevents 99% accuracy!)\n",
    "    # ========================================================================\n",
    "    \n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    # Random noise 1: General market volatility (±5%)\n",
    "    market_noise = np.random.normal(0, 0.05, size=len(df_targets))\n",
    "    \n",
    "    # Random noise 2: Location-specific shocks (±3%)\n",
    "    location_noise = np.random.normal(0, 0.03, size=len(df_targets))\n",
    "    \n",
    "    # Random noise 3: Property-specific factors (±4%)\n",
    "    property_noise = np.random.normal(0, 0.04, size=len(df_targets))\n",
    "    \n",
    "    # Combined noise (total ±8% std deviation)\n",
    "    total_noise = market_noise + location_noise + property_noise\n",
    "    \n",
    "    # Apply noise to multiplier\n",
    "    noisy_multiplier = base_multiplier * (1 + total_noise)\n",
    "    noisy_multiplier = noisy_multiplier.clip(0.60, 1.40)  # Wider range after noise\n",
    "    \n",
    "    df_targets['Dynamic_Appreciation_Rate'] = base_appreciation_rate * noisy_multiplier\n",
    "    \n",
    "    # Calculate future price with noise\n",
    "    df_targets['Future_Price_5Y'] = df_targets['Price_in_Lakhs'] * (\n",
    "        (1 + df_targets['Dynamic_Appreciation_Rate']) ** years\n",
    "    )\n",
    "    \n",
    "    df_targets['Price_Appreciation_%'] = (\n",
    "        (df_targets['Future_Price_5Y'] / df_targets['Price_in_Lakhs'] - 1) * 100\n",
    "    )\n",
    "    \n",
    "    print(f\"   Realistic targets created with market noise\")\n",
    "    print(f\"   Appreciation range: {df_targets['Price_Appreciation_%'].min():.2f}% to {df_targets['Price_Appreciation_%'].max():.2f}%\")\n",
    "    print(f\"   Standard deviation: {df_targets['Price_Appreciation_%'].std():.2f}%\")\n",
    "    \n",
    "    # Classification target (unchanged)\n",
    "    quality_score = (\n",
    "        (df_targets['BHK'] >= 3).astype(int) * 2 +\n",
    "        (df_targets['Age_of_Property'] <= 10).astype(int) * 2 +\n",
    "        (df_targets['Has_Security'] == 1).astype(int) * 2 +\n",
    "        (df_targets['Premium_Amenities'] >= 2).astype(int) * 2 +\n",
    "        (df_targets['Is_Ready_to_Move'] == 1).astype(int) * 2\n",
    "    )\n",
    "    \n",
    "    city_infra_avg = df_targets.groupby('City')['Infrastructure_Score'].transform('mean')\n",
    "    location_score = (city_infra_avg / city_infra_avg.max()) * 10\n",
    "    \n",
    "    size_per_bhk = df_targets['Size_in_SqFt'] / (df_targets['BHK'] + 1)\n",
    "    size_score = (size_per_bhk / size_per_bhk.quantile(0.90)) * 5\n",
    "    size_score = size_score.clip(upper=5)\n",
    "    \n",
    "    df_targets['Investment_Quality_Score'] = (\n",
    "        df_targets['Infrastructure_Score'] +\n",
    "        quality_score +\n",
    "        (location_score / 2) +\n",
    "        size_score\n",
    "    )\n",
    "    \n",
    "    threshold = df_targets['Investment_Quality_Score'].quantile(0.65)\n",
    "    df_targets['Good_Investment'] = (df_targets['Investment_Quality_Score'] >= threshold).astype(int)\n",
    "    \n",
    "    return df_targets\n",
    "\n",
    "# Execute\n",
    "df_with_targets = create_realistic_targets(df_featured)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0446177",
   "metadata": {},
   "source": [
    "### 3. Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91ed5b8",
   "metadata": {
    "papermill": {
     "duration": 1.103613,
     "end_time": "2025-12-07T12:16:57.744608",
     "exception": false,
     "start_time": "2025-12-07T12:16:56.640995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handle_outliers(df, method='iqr'):\n",
    "    \"\"\"Detect and cap outliers using IQR method\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 3: OUTLIER DETECTION AND HANDLING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    df_out = df.copy()\n",
    "    numeric_cols = df_out.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Exclude target and engineered features\n",
    "    exclude = ['Future_Price_5Y', 'Good_Investment', 'Has_Parking', 'Has_Security', \n",
    "               'Is_New_Property', 'Is_Top_Floor', 'Is_Ground_Floor', 'Transport_Score']\n",
    "    numeric_cols = [col for col in numeric_cols if col not in exclude]\n",
    "    \n",
    "    outlier_summary = []\n",
    "    \n",
    "    print(f\"\\nDetecting outliers in {len(numeric_cols)} numeric columns...\")\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        Q1 = df_out[col].quantile(0.25)\n",
    "        Q3 = df_out[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = ((df_out[col] < lower_bound) | (df_out[col] > upper_bound)).sum()\n",
    "        \n",
    "        if outliers > 0:\n",
    "            df_out[col] = df_out[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "            outlier_summary.append({\n",
    "                'Column': col,\n",
    "                'Outliers': outliers,\n",
    "                'Percentage': f\"{(outliers/len(df))*100:.2f}%\"\n",
    "            })\n",
    "    \n",
    "    if outlier_summary:\n",
    "        print(f\"\\nOutliers detected and capped in {len(outlier_summary)} columns:\")\n",
    "        display(pd.DataFrame(outlier_summary).head(10))\n",
    "    else:\n",
    "        print(\"\\nNo outliers detected\")\n",
    "    \n",
    "    print(f\"\\nDataset shape: {df_out.shape}\")\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "# Execute\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"EXECUTING: Outlier Detection\")\n",
    "print(\"=\"*40)\n",
    "df_no_outliers = handle_outliers(df_featured)\n",
    "print(\"\\n RESULTS:\")\n",
    "print(f\"Outliers handled successfully\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4080e70b",
   "metadata": {},
   "source": [
    "### 4. Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b07eed4",
   "metadata": {
    "papermill": {
     "duration": 1.581619,
     "end_time": "2025-12-07T12:16:59.787612",
     "exception": false,
     "start_time": "2025-12-07T12:16:58.205993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_categorical(df):\n",
    "    \"\"\"Encode categorical features\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 4: CATEGORICAL ENCODING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    df_encoded = df.copy()\n",
    "    categorical_cols = df_encoded.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    high_cardinality = [col for col in categorical_cols if df_encoded[col].nunique() > 20]\n",
    "    low_cardinality = [col for col in categorical_cols if df_encoded[col].nunique() <= 20]\n",
    "    \n",
    "    print(f\"\\nCategorical columns: {len(categorical_cols)}\")\n",
    "    print(f\"  High cardinality (>20): {len(high_cardinality)}\")\n",
    "    print(f\"  Low cardinality (<=20): {len(low_cardinality)}\")\n",
    "    \n",
    "    # Label Encoding for high cardinality\n",
    "    if high_cardinality:\n",
    "        print(f\"\\nLabel Encoding {len(high_cardinality)} columns:\")\n",
    "        for col in high_cardinality:\n",
    "            le = LabelEncoder()\n",
    "            df_encoded[f'{col}_Encoded'] = le.fit_transform(df_encoded[col].astype(str))\n",
    "            print(f\"  {col} -> {col}_Encoded ({df_encoded[col].nunique()} unique)\")\n",
    "    \n",
    "    # One-Hot Encoding for low cardinality\n",
    "    if low_cardinality:\n",
    "        print(f\"\\nOne-Hot Encoding {len(low_cardinality)} columns:\")\n",
    "        original_shape = df_encoded.shape[1]\n",
    "        df_encoded = pd.get_dummies(df_encoded, columns=low_cardinality, drop_first=True, dtype=int)\n",
    "        new_cols = df_encoded.shape[1] - original_shape\n",
    "        print(f\"  Created {new_cols} dummy columns\")\n",
    "    \n",
    "    print(f\"\\nDataset shape after encoding: {df_encoded.shape}\")\n",
    "    \n",
    "    return df_encoded\n",
    "\n",
    "# Execute\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"EXECUTING: Categorical Encoding\")\n",
    "print(\"=\"*40)\n",
    "df_encoded = encode_categorical(df_no_outliers)\n",
    "print(\"\\n RESULTS:\")\n",
    "print(f\"Original: {df_no_outliers.shape[1]} cols | After: {df_encoded.shape[1]} cols\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc3cb51",
   "metadata": {},
   "source": [
    "### 5. Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bb98d7",
   "metadata": {
    "papermill": {
     "duration": 0.862819,
     "end_time": "2025-12-07T12:17:01.094945",
     "exception": false,
     "start_time": "2025-12-07T12:17:00.232126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_skewed_features(df, skew_threshold=1.0):\n",
    "    \"\"\"Apply log transformation to highly skewed features\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 5: DATA TRANSFORMATION (SKEWNESS REDUCTION)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    df_trans = df.copy()\n",
    "    numeric_cols = df_trans.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Exclude targets and encoded columns\n",
    "    exclude = ['Future_Price_5Y', 'Good_Investment']\n",
    "    exclude_encoded = [col for col in numeric_cols if '_Encoded' in col or col.startswith('Property_Type_') \n",
    "                       or col.startswith('Furnished_Status_') or col.startswith('Facing_') \n",
    "                       or col.startswith('Owner_Type_') or col.startswith('Availability_Status_')]\n",
    "    exclude.extend(exclude_encoded)\n",
    "    numeric_cols = [col for col in numeric_cols if col not in exclude]\n",
    "    \n",
    "    skewed_features = []\n",
    "    transformed_cols = []\n",
    "    \n",
    "    print(f\"\\nAnalyzing skewness in {len(numeric_cols)} columns...\")\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if df_trans[col].min() >= 0:  # Only for non-negative\n",
    "            skewness = df_trans[col].skew()\n",
    "            if abs(skewness) > skew_threshold:\n",
    "                skewed_features.append((col, skewness))\n",
    "                # Apply log transformation\n",
    "                df_trans[f'{col}_Log'] = np.log1p(df_trans[col])\n",
    "                transformed_cols.append(f'{col}_Log')\n",
    "    \n",
    "    print(f\"\\nFound {len(skewed_features)} skewed features (threshold: {skew_threshold})\")\n",
    "    print(f\"Applied log transformation to: {len(transformed_cols)} features\")\n",
    "    \n",
    "    if skewed_features:\n",
    "        print(\"\\nTop 10 Skewed Features:\")\n",
    "        for feat, skew in sorted(skewed_features, key=lambda x: abs(x[1]), reverse=True)[:10]:\n",
    "            print(f\"  {feat:40s} | Skewness: {skew:.2f}\")\n",
    "    \n",
    "    print(f\"\\nDataset shape: {df_trans.shape}\")\n",
    "    \n",
    "    return df_trans, transformed_cols\n",
    "\n",
    "# Execute\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"EXECUTING: Data Transformation\")\n",
    "print(\"=\"*40)\n",
    "df_transformed, log_features = transform_skewed_features(df_encoded, skew_threshold=1.0)\n",
    "print(\"\\n RESULTS:\")\n",
    "print(f\"Log-transformed features: {len(log_features)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3089d2",
   "metadata": {
    "papermill": {
     "duration": 0.454907,
     "end_time": "2025-12-07T12:17:02.009422",
     "exception": false,
     "start_time": "2025-12-07T12:17:01.554515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERIFYING TARGET COLUMNS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check if targets exist in df_transformed\n",
    "if 'Good_Investment' not in df_transformed.columns:\n",
    "    print(\"Target columns missing! Adding them back...\")\n",
    "    \n",
    "    # Get targets from df_with_targets (which has them)\n",
    "    df_transformed['Good_Investment'] = df_with_targets['Good_Investment']\n",
    "    df_transformed['Future_Price_5Y'] = df_with_targets['Future_Price_5Y']\n",
    "    df_transformed['Investment_Quality_Score'] = df_with_targets['Investment_Quality_Score']\n",
    "    \n",
    "    print(\"Target columns restored:\")\n",
    "    print(f\"   - Good_Investment: {df_transformed['Good_Investment'].nunique()} classes\")\n",
    "    print(f\"   - Future_Price_5Y: {df_transformed['Future_Price_5Y'].min():.2f}L - {df_transformed['Future_Price_5Y'].max():.2f}L\")\n",
    "else:\n",
    "    print(\"Target columns already present\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5515570",
   "metadata": {
    "papermill": {
     "duration": 0.429028,
     "end_time": "2025-12-07T12:17:02.878819",
     "exception": false,
     "start_time": "2025-12-07T12:17:02.449791",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 6. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c234331",
   "metadata": {
    "papermill": {
     "duration": 1.113844,
     "end_time": "2025-12-07T12:17:04.426240",
     "exception": false,
     "start_time": "2025-12-07T12:17:03.312396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def strict_feature_selection(df, target_col, task_type='classification', top_n=20):\n",
    "    \"\"\"More conservative feature selection\"\"\"\n",
    "    \n",
    "    print(f\"\\n STRICT Feature Selection for {task_type}\")\n",
    "    \n",
    "    # AGGRESSIVE exclusions\n",
    "    base_exclude = [\n",
    "        'State', 'City', 'Locality', 'Amenities', \n",
    "        'Public_Transport_Accessibility', 'Parking_Space', 'Security',\n",
    "        'Availability_Status', 'Property_Type', 'Furnished_Status',\n",
    "        'Facing', 'Owner_Type',\n",
    "        'Future_Price_5Y', 'Price_Appreciation_%',\n",
    "        'Good_Investment', 'Investment_Quality_Score',\n",
    "        'Dynamic_Appreciation_Rate'  # Don't leak the calculation\n",
    "    ]\n",
    "    \n",
    "    if task_type == 'classification':\n",
    "        exclude_cols = base_exclude + [\n",
    "            'Price_in_Lakhs', 'Price_per_SqFt',\n",
    "            'Price_in_Lakhs_Log', 'Price_per_SqFt_Log'\n",
    "        ]\n",
    "    else:\n",
    "        exclude_cols = base_exclude.copy()\n",
    "    \n",
    "    available = [col for col in df.columns if col not in exclude_cols]\n",
    "    numerical = df[available].select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    if target_col in numerical:\n",
    "        numerical.remove(target_col)\n",
    "    \n",
    "    # Select only top 20 (not 30) to reduce complexity\n",
    "    correlations = df[numerical].corrwith(df[target_col]).abs().sort_values(ascending=False)\n",
    "    selected = correlations.head(top_n).index.tolist()\n",
    "    \n",
    "    print(f\"   Selected: {len(selected)} features\")\n",
    "    return selected, correlations\n",
    "\n",
    "clf_features, _ = strict_feature_selection(df_with_targets, 'Good_Investment', 'classification', top_n=20)\n",
    "reg_features, _ = strict_feature_selection(df_with_targets, 'Future_Price_5Y', 'regression', top_n=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af327b85",
   "metadata": {
    "papermill": {
     "duration": 0.447249,
     "end_time": "2025-12-07T12:17:05.315256",
     "exception": false,
     "start_time": "2025-12-07T12:17:04.868007",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 7. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99388627",
   "metadata": {
    "papermill": {
     "duration": 0.679192,
     "end_time": "2025-12-07T12:17:06.447127",
     "exception": false,
     "start_time": "2025-12-07T12:17:05.767935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_data(df, target_col, features, test_size=0.2, random_state=42):\n",
    "    \"\"\"Split data into train/test\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 7: DATA SPLITTING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    X = df[features].copy()\n",
    "    y = df[target_col].copy()\n",
    "    \n",
    "    stratify = y if y.dtype in ['int64', 'int32', 'bool'] else None\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=stratify\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTarget: {target_col}\")\n",
    "    print(f\"Features: {len(features)}\")\n",
    "    print(f\"Train: {X_train.shape[0]} ({(len(X_train)/len(X))*100:.1f}%)\")\n",
    "    print(f\"Test: {X_test.shape[0]} ({(len(X_test)/len(X))*100:.1f}%)\")\n",
    "    \n",
    "    if stratify is not None:\n",
    "        print(f\"\\nClass Distribution:\")\n",
    "        print(f\"  Train - 0: {(y_train==0).sum()} | 1: {(y_train==1).sum()}\")\n",
    "        print(f\"  Test  - 0: {(y_test==0).sum()} | 1: {(y_test==1).sum()}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Execute for Classification\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"EXECUTING: Split Data (Classification)\")\n",
    "print(\"=\"*40)\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = split_data(df_transformed, 'Good_Investment', clf_features)\n",
    "print(\"\\n RESULTS:\")\n",
    "print(f\"Training: {X_train_clf.shape} | Test: {X_test_clf.shape}\")\n",
    "\n",
    "# Execute for Regression\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"EXECUTING: Split Data (Regression)\")\n",
    "print(\"=\"*40)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = split_data(df_transformed, 'Future_Price_5Y', reg_features)\n",
    "print(\"\\n RESULTS:\")\n",
    "print(f\"Training: {X_train_reg.shape} | Test: {X_test_reg.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7db7f2",
   "metadata": {},
   "source": [
    "### 8. Handling imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4cc751",
   "metadata": {
    "papermill": {
     "duration": 15.266474,
     "end_time": "2025-12-07T12:17:23.036759",
     "exception": false,
     "start_time": "2025-12-07T12:17:07.770285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handle_imbalance(X_train, y_train):\n",
    "    \"\"\"Apply SMOTE with OPTIMAL strategy for 90%+ accuracy\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 9: HANDLE CLASS IMBALANCE (OPTIMIZED)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    original_dist = y_train.value_counts()\n",
    "    imbalance_ratio = original_dist.min() / original_dist.max()\n",
    "    \n",
    "    print(f\"\\n Original Distribution:\")\n",
    "    print(f\"  Class 0: {original_dist[0]:,} ({(original_dist[0]/len(y_train))*100:.1f}%)\")\n",
    "    print(f\"  Class 1: {original_dist[1]:,} ({(original_dist[1]/len(y_train))*100:.1f}%)\")\n",
    "    print(f\"  Imbalance Ratio: {imbalance_ratio:.3f}\")\n",
    "    \n",
    "    if imbalance_ratio < 0.8:\n",
    "        # Use 0.7 ratio (less aggressive, better generalization)\n",
    "        print(f\"\\n🔧 Applying SMOTE with sampling_strategy=0.7...\")\n",
    "        smote = SMOTE(random_state=42, sampling_strategy=0.7, k_neighbors=5)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "        new_dist = pd.Series(y_resampled).value_counts()\n",
    "        new_ratio = new_dist.min() / new_dist.max()\n",
    "        \n",
    "        print(f\"\\n After SMOTE:\")\n",
    "        print(f\"  Class 0: {new_dist[0]:,} ({(new_dist[0]/len(y_resampled))*100:.1f}%)\")\n",
    "        print(f\"  Class 1: {new_dist[1]:,} ({(new_dist[1]/len(y_resampled))*100:.1f}%)\")\n",
    "        print(f\"  New Ratio: {new_ratio:.3f}\")\n",
    "        print(f\"  Total Samples: {len(X_train):,} → {len(X_resampled):,} (+{len(X_resampled)-len(X_train):,})\")\n",
    "        \n",
    "        return X_resampled, y_resampled\n",
    "    else:\n",
    "        print(f\"\\n Dataset is balanced. No SMOTE needed.\")\n",
    "        return X_train, y_train\n",
    "\n",
    "# Execute\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"EXECUTING: Handle Class Imbalance\")\n",
    "print(\"=\"*40)\n",
    "X_train_clf_balanced, y_train_clf_balanced = handle_imbalance(X_train_clf, y_train_clf)\n",
    "print(\"\\n RESULTS:\")\n",
    "print(f\"Training samples: {len(X_train_clf_balanced)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824ed5f9",
   "metadata": {},
   "source": [
    "### 9. Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e3c36f",
   "metadata": {
    "papermill": {
     "duration": 0.733317,
     "end_time": "2025-12-07T12:17:24.222098",
     "exception": false,
     "start_time": "2025-12-07T12:17:23.488781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 8: CONDITIONAL SCALING\n",
    "# ============================================================================\n",
    "def scale_if_needed(X_train, X_test, model_type):\n",
    "    \"\"\"Scale only for linear models\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"STEP 8: CONDITIONAL SCALING ({model_type.upper()})\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if model_type in ['random_forest', 'xgboost']:\n",
    "        print(f\"\\nScaling: NOT REQUIRED (tree-based)\")\n",
    "        return X_train.copy(), X_test.copy(), None\n",
    "    \n",
    "    elif model_type in ['logistic', 'linear']:\n",
    "        print(f\"\\nScaling: REQUIRED (linear model)\")\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), \n",
    "                                       columns=X_train.columns, index=X_train.index)\n",
    "        X_test_scaled = pd.DataFrame(scaler.transform(X_test), \n",
    "                                      columns=X_test.columns, index=X_test.index)\n",
    "        print(f\"  Mean before: {X_train.mean().mean():.2f} | After: {X_train_scaled.mean().mean():.4f}\")\n",
    "        return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "# Execute for linear models\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"EXECUTING: Scaling for Linear Models\")\n",
    "print(\"=\"*40)\n",
    "X_train_clf_scaled, X_test_clf_scaled, scaler_clf = scale_if_needed(\n",
    "    X_train_clf_balanced,  # Use balanced data\n",
    "    X_test_clf, \n",
    "    'logistic'\n",
    ")\n",
    "X_train_reg_scaled, X_test_reg_scaled, scaler_reg = scale_if_needed(\n",
    "    X_train_reg,  # No SMOTE for regression\n",
    "    X_test_reg, \n",
    "    'linear'\n",
    ")\n",
    "print(\"\\n RESULTS:\")\n",
    "print(f\"Scalers created for linear models\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9ff944",
   "metadata": {
    "papermill": {
     "duration": 0.445426,
     "end_time": "2025-12-07T12:17:26.004534",
     "exception": false,
     "start_time": "2025-12-07T12:17:25.559108",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ***7. ML Model Implementation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e93b8c",
   "metadata": {
    "papermill": {
     "duration": 62.753178,
     "end_time": "2025-12-07T12:18:29.203126",
     "exception": false,
     "start_time": "2025-12-07T12:17:26.449948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CLASSIFICATION_EXPERIMENT = \"Real_Estate_Classification\"\n",
    "REGRESSION_EXPERIMENT = \"Real_Estate_Regression\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MLflow Experiment Names Configured\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Classification Experiment: {CLASSIFICATION_EXPERIMENT}\")\n",
    "print(f\"Regression Experiment: {REGRESSION_EXPERIMENT}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Import required for MLflow signature\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "print(\"\\n✓ MLflow setup complete - Ready to train models\")\n",
    "\n",
    "def train_classification_with_mlflow(X_train, X_test, y_train, y_test, \n",
    "                                     X_train_scaled, X_test_scaled):\n",
    "    \"\"\"Train classification models with MLflow tracking in separate experiment\"\"\"\n",
    "    \n",
    "    # SET CLASSIFICATION EXPERIMENT\n",
    "    mlflow.set_experiment(CLASSIFICATION_EXPERIMENT)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"TRAINING CLASSIFICATION MODELS - Experiment: {CLASSIFICATION_EXPERIMENT}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    results = []\n",
    "    models = {}\n",
    "    \n",
    "    # ========================================================================\n",
    "    # MODEL 1: RANDOM FOREST CLASSIFIER\n",
    "    # ========================================================================\n",
    "    with mlflow.start_run(run_name=\"RandomForest_Classification\") as run:\n",
    "        print(\"\\n Random Forest Classifier...\")\n",
    "        \n",
    "        # Model parameters\n",
    "        rf_params = {\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 10,\n",
    "            'min_samples_split': 20,\n",
    "            'min_samples_leaf': 10,\n",
    "            'max_features': 'sqrt',\n",
    "            'class_weight': 'balanced',\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.log_params(rf_params)\n",
    "        mlflow.log_param(\"model_type\", \"RandomForestClassifier\")\n",
    "        mlflow.log_param(\"task\", \"classification\")\n",
    "        mlflow.log_param(\"experiment\", CLASSIFICATION_EXPERIMENT)\n",
    "        mlflow.log_param(\"train_samples\", X_train.shape[0])\n",
    "        mlflow.log_param(\"test_samples\", X_test.shape[0])\n",
    "        mlflow.log_param(\"n_features\", X_train.shape[1])\n",
    "        \n",
    "        # Train model\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        rf = RandomForestClassifier(**rf_params)\n",
    "        rf.fit(X_train, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        train_pred = rf.predict(X_train)\n",
    "        test_pred = rf.predict(X_test)\n",
    "        test_pred_proba = rf.predict_proba(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "        train_acc = accuracy_score(y_train, train_pred)\n",
    "        test_acc = accuracy_score(y_test, test_pred)\n",
    "        precision = precision_score(y_test, test_pred, zero_division=0)\n",
    "        recall = recall_score(y_test, test_pred, zero_division=0)\n",
    "        f1 = f1_score(y_test, test_pred, zero_division=0)\n",
    "        \n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_test, test_pred_proba[:, 1])\n",
    "        except:\n",
    "            roc_auc = 0.0\n",
    "        \n",
    "        overfitting_gap = train_acc - test_acc\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"train_accuracy\", train_acc)\n",
    "        mlflow.log_metric(\"test_accuracy\", test_acc)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "        mlflow.log_metric(\"overfitting_gap\", overfitting_gap)\n",
    "        \n",
    "        # Log confusion matrix\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        cm = confusion_matrix(y_test, test_pred)\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "        ax.set_title('Random Forest - Confusion Matrix')\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('Actual')\n",
    "        plt.tight_layout()\n",
    "        mlflow.log_figure(fig, \"confusion_matrix_rf.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Log model\n",
    "        signature = infer_signature(X_train, rf.predict(X_train))\n",
    "        mlflow.sklearn.log_model(rf, \"model\", signature=signature)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Model': 'RandomForest',\n",
    "            'Run_ID': run.info.run_id,\n",
    "            'Train_Acc': train_acc,\n",
    "            'Test_Acc': test_acc,\n",
    "            'Overfitting_Gap': overfitting_gap,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1': f1,\n",
    "            'ROC_AUC': roc_auc\n",
    "        })\n",
    "        models['rf'] = rf\n",
    "        \n",
    "        print(f\"   ✓ Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "        print(f\"   ✓ F1: {f1:.4f} | ROC-AUC: {roc_auc:.4f}\")\n",
    "        print(f\"   ✓ Run ID: {run.info.run_id}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # MODEL 2: XGBOOST CLASSIFIER\n",
    "    # ========================================================================\n",
    "    with mlflow.start_run(run_name=\"XGBoost_Classification\") as run:\n",
    "        print(\"\\n XGBoost Classifier...\")\n",
    "        \n",
    "        # Model parameters\n",
    "        xgb_params = {\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 6,\n",
    "            'learning_rate': 0.05,\n",
    "            'min_child_weight': 5,\n",
    "            'subsample': 0.7,\n",
    "            'colsample_bytree': 0.7,\n",
    "            'gamma': 0.5,\n",
    "            'reg_alpha': 1.0,\n",
    "            'reg_lambda': 2.0,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.log_params(xgb_params)\n",
    "        mlflow.log_param(\"model_type\", \"XGBClassifier\")\n",
    "        mlflow.log_param(\"task\", \"classification\")\n",
    "        mlflow.log_param(\"experiment\", CLASSIFICATION_EXPERIMENT)\n",
    "        mlflow.log_param(\"train_samples\", X_train.shape[0])\n",
    "        mlflow.log_param(\"test_samples\", X_test.shape[0])\n",
    "        mlflow.log_param(\"n_features\", X_train.shape[1])\n",
    "        \n",
    "        # Train model\n",
    "        from xgboost import XGBClassifier\n",
    "        xgb = XGBClassifier(**xgb_params)\n",
    "        xgb.fit(X_train, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        train_pred = xgb.predict(X_train)\n",
    "        test_pred = xgb.predict(X_test)\n",
    "        test_pred_proba = xgb.predict_proba(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_acc = accuracy_score(y_train, train_pred)\n",
    "        test_acc = accuracy_score(y_test, test_pred)\n",
    "        precision = precision_score(y_test, test_pred, zero_division=0)\n",
    "        recall = recall_score(y_test, test_pred, zero_division=0)\n",
    "        f1 = f1_score(y_test, test_pred, zero_division=0)\n",
    "        \n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_test, test_pred_proba[:, 1])\n",
    "        except:\n",
    "            roc_auc = 0.0\n",
    "            \n",
    "        overfitting_gap = train_acc - test_acc\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"train_accuracy\", train_acc)\n",
    "        mlflow.log_metric(\"test_accuracy\", test_acc)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "        mlflow.log_metric(\"overfitting_gap\", overfitting_gap)\n",
    "        \n",
    "        # Log confusion matrix\n",
    "        cm = confusion_matrix(y_test, test_pred)\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', ax=ax)\n",
    "        ax.set_title('XGBoost - Confusion Matrix')\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('Actual')\n",
    "        plt.tight_layout()\n",
    "        mlflow.log_figure(fig, \"confusion_matrix_xgb.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Log model\n",
    "        signature = infer_signature(X_train, xgb.predict(X_train))\n",
    "        mlflow.xgboost.log_model(xgb, \"model\", signature=signature)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Model': 'XGBoost',\n",
    "            'Run_ID': run.info.run_id,\n",
    "            'Train_Acc': train_acc,\n",
    "            'Test_Acc': test_acc,\n",
    "            'Overfitting_Gap': overfitting_gap,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1': f1,\n",
    "            'ROC_AUC': roc_auc\n",
    "        })\n",
    "        models['xgb'] = xgb\n",
    "        \n",
    "        print(f\"   ✓ Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "        print(f\"   ✓ F1: {f1:.4f} | ROC-AUC: {roc_auc:.4f}\")\n",
    "        print(f\"   ✓ Run ID: {run.info.run_id}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # MODEL 3: LOGISTIC REGRESSION\n",
    "    # ========================================================================\n",
    "    with mlflow.start_run(run_name=\"LogisticRegression_Classification\") as run:\n",
    "        print(\"\\n Logistic Regression...\")\n",
    "        \n",
    "        # Model parameters\n",
    "        lr_params = {\n",
    "            'max_iter': 2000,\n",
    "            'C': 0.1,\n",
    "            'penalty': 'elasticnet',\n",
    "            'solver': 'saga',\n",
    "            'l1_ratio': 0.5,\n",
    "            'class_weight': 'balanced',\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.log_params(lr_params)\n",
    "        mlflow.log_param(\"model_type\", \"LogisticRegression\")\n",
    "        mlflow.log_param(\"task\", \"classification\")\n",
    "        mlflow.log_param(\"experiment\", CLASSIFICATION_EXPERIMENT)\n",
    "        mlflow.log_param(\"scaled\", True)\n",
    "        mlflow.log_param(\"train_samples\", X_train_scaled.shape[0])\n",
    "        mlflow.log_param(\"test_samples\", X_test_scaled.shape[0])\n",
    "        mlflow.log_param(\"n_features\", X_train_scaled.shape[1])\n",
    "        \n",
    "        # Train model\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        lr = LogisticRegression(**lr_params)\n",
    "        lr.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        train_pred = lr.predict(X_train_scaled)\n",
    "        test_pred = lr.predict(X_test_scaled)\n",
    "        test_pred_proba = lr.predict_proba(X_test_scaled)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_acc = accuracy_score(y_train, train_pred)\n",
    "        test_acc = accuracy_score(y_test, test_pred)\n",
    "        precision = precision_score(y_test, test_pred, zero_division=0)\n",
    "        recall = recall_score(y_test, test_pred, zero_division=0)\n",
    "        f1 = f1_score(y_test, test_pred, zero_division=0)\n",
    "        \n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_test, test_pred_proba[:, 1])\n",
    "        except:\n",
    "            roc_auc = 0.0\n",
    "            \n",
    "        overfitting_gap = train_acc - test_acc\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"train_accuracy\", train_acc)\n",
    "        mlflow.log_metric(\"test_accuracy\", test_acc)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "        mlflow.log_metric(\"overfitting_gap\", overfitting_gap)\n",
    "        \n",
    "        # Log confusion matrix\n",
    "        cm = confusion_matrix(y_test, test_pred)\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges', ax=ax)\n",
    "        ax.set_title('Logistic Regression - Confusion Matrix')\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('Actual')\n",
    "        plt.tight_layout()\n",
    "        mlflow.log_figure(fig, \"confusion_matrix_lr.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Log model\n",
    "        signature = infer_signature(X_train_scaled, lr.predict(X_train_scaled))\n",
    "        mlflow.sklearn.log_model(lr, \"model\", signature=signature)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Model': 'LogisticRegression',\n",
    "            'Run_ID': run.info.run_id,\n",
    "            'Train_Acc': train_acc,\n",
    "            'Test_Acc': test_acc,\n",
    "            'Overfitting_Gap': overfitting_gap,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1': f1,\n",
    "            'ROC_AUC': roc_auc\n",
    "        })\n",
    "        models['lr'] = lr\n",
    "        \n",
    "        print(f\"   ✓ Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "        print(f\"   ✓ F1: {f1:.4f} | ROC-AUC: {roc_auc:.4f}\")\n",
    "        print(f\"   ✓ Run ID: {run.info.run_id}\")\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CLASSIFICATION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    display(results_df[['Model', 'Test_Acc', 'F1', 'ROC_AUC', 'Overfitting_Gap']])\n",
    "    \n",
    "    return results_df, models\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TRAIN CLASSIFICATION MODELS\n",
    "# ============================================================================\n",
    "print(\"\\n CLASSIFICATION TASK\")\n",
    "clf_results, clf_models = train_classification_with_mlflow(\n",
    "    X_train_clf_balanced, X_test_clf,\n",
    "    y_train_clf_balanced, y_test_clf,\n",
    "    X_train_clf_scaled, X_test_clf_scaled\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLASSIFICATION MODELS TRAINED SUCCESSFULLY\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eb3999",
   "metadata": {
    "papermill": {
     "duration": 44.167658,
     "end_time": "2025-12-07T12:19:13.823222",
     "exception": false,
     "start_time": "2025-12-07T12:18:29.655564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# REGRESSION MODEL TRAINING WITH MLFLOW\n",
    "# ============================================================================\n",
    "\n",
    "def train_regression_with_mlflow(X_train, X_test, y_train, y_test,\n",
    "                                 X_train_scaled, X_test_scaled):\n",
    "    \"\"\"Train regression models with MLflow tracking in separate experiment\"\"\"\n",
    "    \n",
    "    # SET REGRESSION EXPERIMENT\n",
    "    mlflow.set_experiment(REGRESSION_EXPERIMENT)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"TRAINING REGRESSION MODELS - Experiment: {REGRESSION_EXPERIMENT}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    results = []\n",
    "    models = {}\n",
    "    \n",
    "    # ========================================================================\n",
    "    # MODEL 1: RANDOM FOREST REGRESSOR\n",
    "    # ========================================================================\n",
    "    with mlflow.start_run(run_name=\"RandomForest_Regression\") as run:\n",
    "        print(\"\\n Random Forest Regressor...\")\n",
    "        \n",
    "        # Model parameters\n",
    "        rf_params = {\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 12,\n",
    "            'min_samples_split': 15,\n",
    "            'min_samples_leaf': 8,\n",
    "            'max_features': 'sqrt',\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.log_params(rf_params)\n",
    "        mlflow.log_param(\"model_type\", \"RandomForestRegressor\")\n",
    "        mlflow.log_param(\"task\", \"regression\")\n",
    "        mlflow.log_param(\"experiment\", REGRESSION_EXPERIMENT)\n",
    "        mlflow.log_param(\"train_samples\", X_train.shape[0])\n",
    "        mlflow.log_param(\"test_samples\", X_test.shape[0])\n",
    "        mlflow.log_param(\"n_features\", X_train.shape[1])\n",
    "        \n",
    "        # Train model\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "        \n",
    "        rf = RandomForestRegressor(**rf_params)\n",
    "        rf.fit(X_train, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        train_pred = rf.predict(X_train)\n",
    "        test_pred = rf.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_r2 = r2_score(y_train, train_pred)\n",
    "        test_r2 = r2_score(y_test, test_pred)\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "        test_mae = mean_absolute_error(y_test, test_pred)\n",
    "        overfitting_gap = train_r2 - test_r2\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"train_r2\", train_r2)\n",
    "        mlflow.log_metric(\"test_r2\", test_r2)\n",
    "        mlflow.log_metric(\"test_rmse\", test_rmse)\n",
    "        mlflow.log_metric(\"test_mae\", test_mae)\n",
    "        mlflow.log_metric(\"overfitting_gap\", overfitting_gap)\n",
    "        \n",
    "        # Log actual vs predicted plot\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        ax.scatter(y_test, test_pred, alpha=0.5)\n",
    "        ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "        ax.set_xlabel('Actual Price (Lakhs)')\n",
    "        ax.set_ylabel('Predicted Price (Lakhs)')\n",
    "        ax.set_title('Random Forest - Actual vs Predicted')\n",
    "        ax.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        mlflow.log_figure(fig, \"actual_vs_predicted_rf.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Log model\n",
    "        signature = infer_signature(X_train, rf.predict(X_train))\n",
    "        mlflow.sklearn.log_model(rf, \"model\", signature=signature)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Model': 'RandomForest',\n",
    "            'Run_ID': run.info.run_id,\n",
    "            'Train_R2': train_r2,\n",
    "            'Test_R2': test_r2,\n",
    "            'Overfitting_Gap': overfitting_gap,\n",
    "            'RMSE': test_rmse,\n",
    "            'MAE': test_mae\n",
    "        })\n",
    "        models['rf'] = rf\n",
    "        \n",
    "        print(f\"   ✓ Train R²: {train_r2:.4f} | Test R²: {test_r2:.4f}\")\n",
    "        print(f\"   ✓ RMSE: {test_rmse:.2f} | MAE: {test_mae:.2f}\")\n",
    "        print(f\"   ✓ Run ID: {run.info.run_id}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # MODEL 2: XGBOOST REGRESSOR\n",
    "    # ========================================================================\n",
    "    with mlflow.start_run(run_name=\"XGBoost_Regression\") as run:\n",
    "        print(\"\\n XGBoost Regressor...\")\n",
    "        \n",
    "        # Model parameters\n",
    "        xgb_params = {\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 5,\n",
    "            'learning_rate': 0.05,\n",
    "            'min_child_weight': 5,\n",
    "            'subsample': 0.7,\n",
    "            'colsample_bytree': 0.7,\n",
    "            'gamma': 0.5,\n",
    "            'reg_alpha': 1.0,\n",
    "            'reg_lambda': 2.0,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.log_params(xgb_params)\n",
    "        mlflow.log_param(\"model_type\", \"XGBRegressor\")\n",
    "        mlflow.log_param(\"task\", \"regression\")\n",
    "        mlflow.log_param(\"experiment\", REGRESSION_EXPERIMENT)\n",
    "        mlflow.log_param(\"train_samples\", X_train.shape[0])\n",
    "        mlflow.log_param(\"test_samples\", X_test.shape[0])\n",
    "        mlflow.log_param(\"n_features\", X_train.shape[1])\n",
    "        \n",
    "        # Train model\n",
    "        from xgboost import XGBRegressor\n",
    "        xgb = XGBRegressor(**xgb_params)\n",
    "        xgb.fit(X_train, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        train_pred = xgb.predict(X_train)\n",
    "        test_pred = xgb.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_r2 = r2_score(y_train, train_pred)\n",
    "        test_r2 = r2_score(y_test, test_pred)\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "        test_mae = mean_absolute_error(y_test, test_pred)\n",
    "        overfitting_gap = train_r2 - test_r2\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"train_r2\", train_r2)\n",
    "        mlflow.log_metric(\"test_r2\", test_r2)\n",
    "        mlflow.log_metric(\"test_rmse\", test_rmse)\n",
    "        mlflow.log_metric(\"test_mae\", test_mae)\n",
    "        mlflow.log_metric(\"overfitting_gap\", overfitting_gap)\n",
    "        \n",
    "        # Log actual vs predicted plot\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        ax.scatter(y_test, test_pred, alpha=0.5, color='green')\n",
    "        ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "        ax.set_xlabel('Actual Price (Lakhs)')\n",
    "        ax.set_ylabel('Predicted Price (Lakhs)')\n",
    "        ax.set_title('XGBoost - Actual vs Predicted')\n",
    "        ax.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        mlflow.log_figure(fig, \"actual_vs_predicted_xgb.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Log model\n",
    "        signature = infer_signature(X_train, xgb.predict(X_train))\n",
    "        mlflow.xgboost.log_model(xgb, \"model\", signature=signature)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Model': 'XGBoost',\n",
    "            'Run_ID': run.info.run_id,\n",
    "            'Train_R2': train_r2,\n",
    "            'Test_R2': test_r2,\n",
    "            'Overfitting_Gap': overfitting_gap,\n",
    "            'RMSE': test_rmse,\n",
    "            'MAE': test_mae\n",
    "        })\n",
    "        models['xgb'] = xgb\n",
    "        \n",
    "        print(f\"   ✓ Train R²: {train_r2:.4f} | Test R²: {test_r2:.4f}\")\n",
    "        print(f\"   ✓ RMSE: {test_rmse:.2f} | MAE: {test_mae:.2f}\")\n",
    "        print(f\"   ✓ Run ID: {run.info.run_id}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # MODEL 3: RIDGE REGRESSION\n",
    "    # ========================================================================\n",
    "    with mlflow.start_run(run_name=\"Ridge_Regression\") as run:\n",
    "        print(\"\\n Ridge Regression...\")\n",
    "        \n",
    "        from sklearn.linear_model import Ridge\n",
    "        \n",
    "        # Model parameters\n",
    "        ridge_params = {\n",
    "            'alpha': 10.0,\n",
    "            'random_state': 42\n",
    "        }\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.log_params(ridge_params)\n",
    "        mlflow.log_param(\"model_type\", \"Ridge\")\n",
    "        mlflow.log_param(\"task\", \"regression\")\n",
    "        mlflow.log_param(\"experiment\", REGRESSION_EXPERIMENT)\n",
    "        mlflow.log_param(\"scaled\", True)\n",
    "        mlflow.log_param(\"train_samples\", X_train_scaled.shape[0])\n",
    "        mlflow.log_param(\"test_samples\", X_test_scaled.shape[0])\n",
    "        mlflow.log_param(\"n_features\", X_train_scaled.shape[1])\n",
    "        \n",
    "        # Train model\n",
    "        lr = Ridge(**ridge_params)\n",
    "        lr.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        train_pred = lr.predict(X_train_scaled)\n",
    "        test_pred = lr.predict(X_test_scaled)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_r2 = r2_score(y_train, train_pred)\n",
    "        test_r2 = r2_score(y_test, test_pred)\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "        test_mae = mean_absolute_error(y_test, test_pred)\n",
    "        overfitting_gap = train_r2 - test_r2\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"train_r2\", train_r2)\n",
    "        mlflow.log_metric(\"test_r2\", test_r2)\n",
    "        mlflow.log_metric(\"test_rmse\", test_rmse)\n",
    "        mlflow.log_metric(\"test_mae\", test_mae)\n",
    "        mlflow.log_metric(\"overfitting_gap\", overfitting_gap)\n",
    "        \n",
    "        # Log actual vs predicted plot\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        ax.scatter(y_test, test_pred, alpha=0.5, color='orange')\n",
    "        ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "        ax.set_xlabel('Actual Price (Lakhs)')\n",
    "        ax.set_ylabel('Predicted Price (Lakhs)')\n",
    "        ax.set_title('Ridge Regression - Actual vs Predicted')\n",
    "        ax.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        mlflow.log_figure(fig, \"actual_vs_predicted_ridge.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Log model\n",
    "        signature = infer_signature(X_train_scaled, lr.predict(X_train_scaled))\n",
    "        mlflow.sklearn.log_model(lr, \"model\", signature=signature)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Model': 'Ridge',\n",
    "            'Run_ID': run.info.run_id,\n",
    "            'Train_R2': train_r2,\n",
    "            'Test_R2': test_r2,\n",
    "            'Overfitting_Gap': overfitting_gap,\n",
    "            'RMSE': test_rmse,\n",
    "            'MAE': test_mae\n",
    "        })\n",
    "        models['lr'] = lr\n",
    "        \n",
    "        print(f\"   ✓ Train R²: {train_r2:.4f} | Test R²: {test_r2:.4f}\")\n",
    "        print(f\"   ✓ RMSE: {test_rmse:.2f} | MAE: {test_mae:.2f}\")\n",
    "        print(f\"   ✓ Run ID: {run.info.run_id}\")\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"REGRESSION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    display(results_df[['Model', 'Test_R2', 'RMSE', 'MAE', 'Overfitting_Gap']])\n",
    "    \n",
    "    return results_df, models\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TRAIN REGRESSION MODELS\n",
    "# ============================================================================\n",
    "print(\"\\n📋 REGRESSION TASK\")\n",
    "reg_results, reg_models = train_regression_with_mlflow(\n",
    "    X_train_reg, X_test_reg,\n",
    "    y_train_reg, y_test_reg,\n",
    "    X_train_reg_scaled, X_test_reg_scaled\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGRESSION MODELS TRAINED SUCCESSFULLY\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579d6c14",
   "metadata": {
    "papermill": {
     "duration": 3.72387,
     "end_time": "2025-12-07T12:19:17.993028",
     "exception": false,
     "start_time": "2025-12-07T12:19:14.269158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CLASSIFICATION VISUALIZATIONS\n",
    "# ============================================================================\n",
    "\n",
    "def visualize_classification_results(clf_models,\n",
    "                                     X_train_clf, X_test_clf,\n",
    "                                     y_train_clf, y_test_clf,\n",
    "                                     X_train_clf_scaled, X_test_clf_scaled,\n",
    "                                     clf_results):\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CLASSIFICATION VISUALIZATIONS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    models_list = ['rf', 'xgb', 'lr']\n",
    "    model_names = ['Random Forest', 'XGBoost', 'Logistic Regression']\n",
    "\n",
    "    # Which set is scaled?\n",
    "    train_data = {\n",
    "        'rf': (X_train_clf, y_train_clf),\n",
    "        'xgb': (X_train_clf, y_train_clf),\n",
    "        'lr': (X_train_clf_scaled, y_train_clf)\n",
    "    }\n",
    "    test_data = {\n",
    "        'rf': (X_test_clf, y_test_clf),\n",
    "        'xgb': (X_test_clf, y_test_clf),\n",
    "        'lr': (X_test_clf_scaled, y_test_clf)\n",
    "    }\n",
    "\n",
    "    train_scores, test_scores = [], []\n",
    "\n",
    "    print(\"\\n Training / Testing Accuracy\")\n",
    "    for model_key, model_name in zip(models_list, model_names):\n",
    "        model = clf_models[model_key]\n",
    "        Xtr, ytr = train_data[model_key]\n",
    "        Xte, yte = test_data[model_key]\n",
    "\n",
    "        train_acc = model.score(Xtr, ytr)\n",
    "        test_acc = model.score(Xte, yte)\n",
    "\n",
    "        train_scores.append(train_acc)\n",
    "        test_scores.append(test_acc)\n",
    "\n",
    "        print(f\"   {model_name}: Train={train_acc:.4f}, Test={test_acc:.4f}\")\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # FIGURE 1 — Train vs Test Accuracy and Loss\n",
    "    # -------------------------------------------------------------\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    fig.suptitle(\"Classification — Train vs Test Performance\", fontsize=16, fontweight='bold')\n",
    "\n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.35\n",
    "\n",
    "    # Accuracy plot\n",
    "    axes[0].bar(x - width/2, train_scores, width, label=\"Train Accuracy\", color=\"#2E86AB\")\n",
    "    axes[0].bar(x + width/2, test_scores, width, label=\"Test Accuracy\", color=\"#F18F01\")\n",
    "    axes[0].set_title(\"Train vs Test Accuracy\")\n",
    "    axes[0].set_ylim([0, 1])\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(model_names, rotation=45)\n",
    "    axes[0].legend()\n",
    "\n",
    "    # LOSS = 1 - accuracy\n",
    "    train_loss = [1-x for x in train_scores]\n",
    "    test_loss  = [1-x for x in test_scores]\n",
    "\n",
    "    axes[1].plot(model_names, train_loss, marker='o', label=\"Train Loss\", color=\"blue\")\n",
    "    axes[1].plot(model_names, test_loss, marker='o', label=\"Test Loss\", color=\"orange\")\n",
    "    axes[1].set_title(\"Train vs Test Loss (1 - Accuracy)\")\n",
    "    axes[1].set_ylim([0, 1])\n",
    "    axes[1].legend()\n",
    "    axes[1].grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # FIGURE 2 — Confusion Matrices\n",
    "    # -------------------------------------------------------------\n",
    "    print(\"\\n Confusion Matrices\")\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    fig.suptitle(\"Confusion Matrices — Classification Models\", fontsize=16, fontweight='bold')\n",
    "\n",
    "    for idx, (model_key, model_name) in enumerate(zip(models_list, model_names)):\n",
    "        model = clf_models[model_key]\n",
    "        Xte, yte = test_data[model_key]\n",
    "        y_pred = model.predict(Xte)\n",
    "\n",
    "        cm = confusion_matrix(yte, y_pred)\n",
    "\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[idx],\n",
    "                    xticklabels=['0', '1'], yticklabels=['0','1'])\n",
    "\n",
    "        axes[idx].set_title(model_name)\n",
    "        axes[idx].set_xlabel(\"Predicted\")\n",
    "        axes[idx].set_ylabel(\"Actual\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n✓ Classification visualizations complete.\\n\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTE CLASSIFICATION VISUALIZATIONS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"EXECUTING: Classification Visualizations\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "visualize_classification_results(\n",
    "    clf_models,\n",
    "    X_train_clf_balanced, X_test_clf, y_train_clf_balanced, y_test_clf,\n",
    "    X_train_clf_scaled, X_test_clf_scaled,\n",
    "    clf_results\n",
    ")\n",
    "\n",
    "print(\"\\n CLASSIFICATION TASK COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6102009",
   "metadata": {
    "papermill": {
     "duration": 5.609911,
     "end_time": "2025-12-07T12:19:24.091334",
     "exception": false,
     "start_time": "2025-12-07T12:19:18.481423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# REGRESSION VISUALIZATIONS\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "def visualize_regression_results(reg_models,\n",
    "                                 X_train_reg, X_test_reg,\n",
    "                                 y_train_reg, y_test_reg,\n",
    "                                 X_train_reg_scaled, X_test_reg_scaled,\n",
    "                                 reg_results):\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"REGRESSION VISUALIZATIONS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    model_keys = ['rf', 'xgb', 'lr']\n",
    "    model_names = ['Random Forest', 'XGBoost', 'Ridge Regression']\n",
    "\n",
    "    train_data = {\n",
    "        'rf': (X_train_reg, y_train_reg),\n",
    "        'xgb': (X_train_reg, y_train_reg),\n",
    "        'lr': (X_train_reg_scaled, y_train_reg)\n",
    "    }\n",
    "    test_data = {\n",
    "        'rf': (X_test_reg, y_test_reg),\n",
    "        'xgb': (X_test_reg, y_test_reg),\n",
    "        'lr': (X_test_reg_scaled, y_test_reg)\n",
    "    }\n",
    "\n",
    "    train_r2, test_r2 = [], []\n",
    "    train_rmse, test_rmse = [], []\n",
    "\n",
    "    print(\"\\n Training / Testing R² and RMSE\")\n",
    "    for model_key, model_name in zip(model_keys, model_names):\n",
    "        model = reg_models[model_key]\n",
    "        Xtr, ytr = train_data[model_key]\n",
    "        Xte, yte = test_data[model_key]\n",
    "\n",
    "        ytr_pred = model.predict(Xtr)\n",
    "        yte_pred = model.predict(Xte)\n",
    "\n",
    "        tr_r2 = r2_score(ytr, ytr_pred)\n",
    "        te_r2 = r2_score(yte, yte_pred)\n",
    "\n",
    "        tr_rmse = np.sqrt(mean_squared_error(ytr, ytr_pred))\n",
    "        te_rmse = np.sqrt(mean_squared_error(yte, yte_pred))\n",
    "\n",
    "        train_r2.append(tr_r2)\n",
    "        test_r2.append(te_r2)\n",
    "        train_rmse.append(tr_rmse)\n",
    "        test_rmse.append(te_rmse)\n",
    "\n",
    "        print(f\"  {model_name}: Train R²={tr_r2:.4f}, Test R²={te_r2:.4f}, Train RMSE={tr_rmse:.2f}, Test RMSE={te_rmse:.2f}\")\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # FIGURE 1 — R² and RMSE (Train vs Test)\n",
    "    # -------------------------------------------------------------\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    fig.suptitle(\"Regression — Train vs Test Performance\", fontsize=16, fontweight='bold')\n",
    "\n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.35\n",
    "\n",
    "    # R² plot\n",
    "    axes[0].bar(x - width/2, train_r2, width, label=\"Train R²\", color=\"#2E86AB\")\n",
    "    axes[0].bar(x + width/2, test_r2, width, label=\"Test R²\", color=\"#F18F01\")\n",
    "    axes[0].set_title(\"Train vs Test R²\")\n",
    "    axes[0].set_ylim([0, 1])\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(model_names, rotation=45)\n",
    "    axes[0].legend()\n",
    "\n",
    "    # RMSE plot (loss)\n",
    "    axes[1].plot(model_names, train_rmse, marker='o', label=\"Train RMSE\", color=\"blue\")\n",
    "    axes[1].plot(model_names, test_rmse, marker='o', label=\"Test RMSE\", color=\"orange\")\n",
    "    axes[1].set_title(\"Train vs Test Loss (RMSE)\")\n",
    "    axes[1].legend()\n",
    "    axes[1].grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # FIGURE 2 — Actual vs Predicted\n",
    "    # -------------------------------------------------------------\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    fig.suptitle(\"Actual vs Predicted — Regression Models\", fontsize=16, fontweight='bold')\n",
    "\n",
    "    for idx, (model_key, model_name) in enumerate(zip(model_keys, model_names)):\n",
    "        model = reg_models[model_key]\n",
    "        Xte, yte = test_data[model_key]\n",
    "\n",
    "        y_pred = model.predict(Xte)\n",
    "\n",
    "        axes[idx].scatter(yte, y_pred, alpha=0.5, color=\"#06A77D\")\n",
    "        axes[idx].plot([min(yte), max(yte)], [min(yte), max(yte)], \"r--\")\n",
    "\n",
    "        axes[idx].set_title(model_name)\n",
    "        axes[idx].set_xlabel(\"Actual\")\n",
    "        axes[idx].set_ylabel(\"Predicted\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # FIGURE 3 — Residual Plots\n",
    "    # -------------------------------------------------------------\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    fig.suptitle(\"Residual Plots — Regression Models\", fontsize=16, fontweight='bold')\n",
    "\n",
    "    for idx, (model_key, model_name) in enumerate(zip(model_keys, model_names)):\n",
    "        model = reg_models[model_key]\n",
    "        Xte, yte = test_data[model_key]\n",
    "\n",
    "        y_pred = model.predict(Xte)\n",
    "        residuals = yte - y_pred\n",
    "\n",
    "        axes[idx].scatter(y_pred, residuals, alpha=0.5, color=\"#D62828\")\n",
    "        axes[idx].axhline(0, color=\"black\", linestyle=\"--\")\n",
    "        axes[idx].set_title(model_name)\n",
    "        axes[idx].set_xlabel(\"Predicted\")\n",
    "        axes[idx].set_ylabel(\"Residuals\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n✓ Regression visualizations complete.\\n\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTE REGRESSION VISUALIZATIONS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"EXECUTING: Regression Visualizations\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "visualize_regression_results(\n",
    "    reg_models,\n",
    "    X_train_reg, X_test_reg, y_train_reg, y_test_reg,\n",
    "    X_train_reg_scaled, X_test_reg_scaled,\n",
    "    reg_results\n",
    ")\n",
    "\n",
    "print(\"\\n REGRESSION TASK COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94888544",
   "metadata": {},
   "source": [
    "## **1. Which evaluation metrics did you consider for a positive business impact and why?**\n",
    "\n",
    "### **Classification (Good Investment Prediction)**\n",
    "\n",
    "We focused on the following metrics:\n",
    "\n",
    "1. **Accuracy** – Measures overall correctness. Essential for ensuring the model reliably classifies investment-worthy properties.\n",
    "2. **Precision** – Critical to reduce false positives (avoiding labeling bad properties as “Good Investment”).\n",
    "3. **Recall** – Ensures the model correctly identifies most genuinely profitable properties.\n",
    "4. **ROC-AUC** – Measures separability between “Good” and “Not Good” investments, improving decision confidence.\n",
    "\n",
    "**Business Impact:**\n",
    "These metrics ensure that investors are not misled into choosing weak investment properties and that profitable opportunities are not missed. High precision and recall directly reduce financial risk.\n",
    "\n",
    "\n",
    "### **Regression (Future Price Prediction – 5 Years)**\n",
    "\n",
    "We used:\n",
    "\n",
    "1. **R² Score** – Measures how well the model explains price variation; crucial for trust in long-term predictions.\n",
    "2. **RMSE (Root Mean Squared Error)** – Punishes large prediction errors; important in real estate where even small deviations can impact financial planning.\n",
    "3. **MAE (Mean Absolute Error)** – Helps understand average deviation in price forecasts in lakhs.\n",
    "\n",
    "**Business Impact:**\n",
    "Accurate future price predictions help users:\n",
    "\n",
    "* Estimate ROI\n",
    "* Compare properties\n",
    "* Make long-term investment decisions\n",
    "* Reduce uncertainty in financial planning\n",
    "\n",
    "## **2. Which ML model did you choose as your final prediction model and why?**\n",
    "\n",
    "### **Final Classification Model:**\n",
    "\n",
    "✔ **XGBoost Classifier**\n",
    "**Reason:**\n",
    "\n",
    "* Highest test accuracy (**0.9582**)\n",
    "* Handles complex, non-linear relationships in real estate data\n",
    "* Robust to outliers and missingness\n",
    "* Provides strong generalization with lower overfitting risk\n",
    "* Supports feature importance and SHAP interpretability\n",
    "\n",
    "### **Final Regression Model:**\n",
    "\n",
    "✔ **Ridge Regression**\n",
    "**Reason:**\n",
    "\n",
    "* Highest R² score (**0.9954**)\n",
    "* Best performance on unseen data\n",
    "* Handles multicollinearity effectively\n",
    "* Produces stable and smooth price predictions\n",
    "* Ideal when dataset has correlated numerical variables\n",
    "\n",
    "\n",
    "## **3. Explain the chosen model and its feature importance using explainability tools.**\n",
    "\n",
    "### **(A) XGBoost Classifier – Explanation**\n",
    "\n",
    "**Model Overview:**\n",
    "XGBoost is a high-performance gradient boosting algorithm that uses sequential decision trees to minimize classification error. It combines boosting, regularization, and tree-based splitting logic, making it highly effective for complex tabular datasets such as real estate.\n",
    "\n",
    "**Why it works well here:**\n",
    "\n",
    "* Captures non-linear interactions like BHK × Locality × Price trends\n",
    "* Learns subtle market patterns\n",
    "* Prioritizes the most influential investment-related features\n",
    "\n",
    "### **Feature Importance (Classification – XGBoost)**\n",
    "\n",
    "Based on SHAP / model importance, top contributors typically include:\n",
    "\n",
    "1. **Price_per_SqFt**\n",
    "2. **Size_in_SqFt**\n",
    "3. **City / Locality**\n",
    "4. **BHK**\n",
    "5. **Age_of_Property**\n",
    "6. **Nearby_Schools / Hospitals**\n",
    "7. **Public_Transport_Accessibility**\n",
    "8. **Amenities**\n",
    "9. **Availability_Status**\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "* Lower price per sqft → higher investment score\n",
    "* Better locality amenities → increased probability of good investment\n",
    "* Newer properties typically score higher\n",
    "* Transport access and BHK count heavily influence investment quality\n",
    "\n",
    "\n",
    "### **(B) Ridge Regression – Explanation**\n",
    "\n",
    "**Model Overview:**\n",
    "Ridge Regression is a linear model with L2 regularization. It reduces model weights for correlated features and prevents overfitting while maintaining interpretability.\n",
    "\n",
    "**Why it performs best:**\n",
    "\n",
    "* Real estate prices often follow linear + domain-driven relationships\n",
    "* Handles correlated numerical features smoothly\n",
    "* Produces stable, consistent future price predictions\n",
    "\n",
    "### **Feature Importance (Regression – Ridge)**\n",
    "\n",
    "Top impactful variables for future price are:\n",
    "\n",
    "1. **Price_in_Lakhs (Current Price)**\n",
    "2. **City & Locality growth trends**\n",
    "3. **Size_in_SqFt**\n",
    "4. **Price_per_SqFt**\n",
    "5. **Amenities score**\n",
    "6. **Age_of_Property**\n",
    "7. **Nearby Facilities (Schools / Hospitals)**\n",
    "8. **Property_Type**\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "* Current price is the strongest driver for future price\n",
    "* Location-based economic growth influences appreciation\n",
    "* Larger properties appreciate more steadily\n",
    "* Amenities & connectivity boost long-term value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb209fb1",
   "metadata": {
    "papermill": {
     "duration": 0.469423,
     "end_time": "2025-12-07T12:19:30.535129",
     "exception": false,
     "start_time": "2025-12-07T12:19:30.065706",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ***8.*** ***Future Work (Optional)***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781e0022",
   "metadata": {
    "papermill": {
     "duration": 0.451674,
     "end_time": "2025-12-07T12:19:31.463945",
     "exception": false,
     "start_time": "2025-12-07T12:19:31.012271",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6400fe00",
   "metadata": {
    "papermill": {
     "duration": 0.489139,
     "end_time": "2025-12-07T12:19:32.403966",
     "exception": false,
     "start_time": "2025-12-07T12:19:31.914827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def export_best_models(clf_results, reg_results, clf_models, reg_models, \n",
    "                       clf_features, reg_features, scaler_clf, scaler_reg):\n",
    "    \"\"\"Export best models and artifacts\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXPORTING BEST MODELS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Debug: Print available models\n",
    "    print(\"\\nAvailable Classification Models:\")\n",
    "    print(clf_results[['Model', 'Test_Acc']])\n",
    "    print(\"\\nAvailable Regression Models:\")\n",
    "    print(reg_results[['Model', 'Test_R2']])\n",
    "    \n",
    "    # Best Classification Model - Get the row directly\n",
    "    best_clf_row = clf_results.loc[clf_results['Test_Acc'].idxmax()]\n",
    "    best_clf_name = best_clf_row['Model']\n",
    "    \n",
    "    # Map model name to key\n",
    "    name_to_key = {\n",
    "        'RandomForest': 'rf',\n",
    "        'XGBoost': 'xgb',\n",
    "        'LogisticRegression': 'lr'\n",
    "    }\n",
    "    \n",
    "    best_clf_key = name_to_key.get(best_clf_name, 'rf')\n",
    "    best_clf = clf_models[best_clf_key]\n",
    "    best_clf_accuracy = best_clf_row['Test_Acc']\n",
    "    \n",
    "    print(f\"\\n✓ Best Classification Model: {best_clf_name} (Accuracy: {best_clf_accuracy:.4f})\")\n",
    "    \n",
    "    # Best Regression Model - Get the row directly\n",
    "    best_reg_row = reg_results.loc[reg_results['Test_R2'].idxmax()]\n",
    "    best_reg_name = best_reg_row['Model']\n",
    "    \n",
    "    # Map model name to key\n",
    "    reg_name_to_key = {\n",
    "        'RandomForest': 'rf',\n",
    "        'XGBoost': 'xgb',\n",
    "        'Ridge': 'lr',\n",
    "        'LinearRegression': 'lr'\n",
    "    }\n",
    "    \n",
    "    best_reg_key = reg_name_to_key.get(best_reg_name, 'rf')\n",
    "    best_reg = reg_models[best_reg_key]\n",
    "    best_reg_r2 = best_reg_row['Test_R2']\n",
    "    \n",
    "    print(f\"✓ Best Regression Model: {best_reg_name} (R²: {best_reg_r2:.4f})\")\n",
    "    \n",
    "    # Export models\n",
    "    with open('best_classification_model.pkl', 'wb') as f:\n",
    "        pickle.dump(best_clf, f)\n",
    "    print(f\"\\n✓ Exported: best_classification_model.pkl\")\n",
    "    \n",
    "    with open('best_regression_model.pkl', 'wb') as f:\n",
    "        pickle.dump(best_reg, f)\n",
    "    print(f\"✓ Exported: best_regression_model.pkl\")\n",
    "    \n",
    "    # Export features\n",
    "    with open('classification_features.pkl', 'wb') as f:\n",
    "        pickle.dump(clf_features, f)\n",
    "    print(f\"✓ Exported: classification_features.pkl ({len(clf_features)} features)\")\n",
    "    \n",
    "    with open('regression_features.pkl', 'wb') as f:\n",
    "        pickle.dump(reg_features, f)\n",
    "    print(f\"✓ Exported: regression_features.pkl ({len(reg_features)} features)\")\n",
    "    \n",
    "    # Export scalers (only if needed)\n",
    "    if best_clf_key == 'lr' and scaler_clf is not None:\n",
    "        with open('classification_scaler.pkl', 'wb') as f:\n",
    "            pickle.dump(scaler_clf, f)\n",
    "        print(f\"✓ Exported: classification_scaler.pkl\")\n",
    "    \n",
    "    if best_reg_key == 'lr' and scaler_reg is not None:\n",
    "        with open('regression_scaler.pkl', 'wb') as f:\n",
    "            pickle.dump(scaler_reg, f)\n",
    "        print(f\"✓ Exported: regression_scaler.pkl\")\n",
    "    \n",
    "    # Export metadata\n",
    "    metadata = {\n",
    "        'classification': {\n",
    "            'model_name': best_clf_name,\n",
    "            'model_key': best_clf_key,\n",
    "            'test_accuracy': float(best_clf_accuracy),\n",
    "            'features_count': len(clf_features),\n",
    "            'needs_scaling': best_clf_key == 'lr',\n",
    "            'experiment_name': CLASSIFICATION_EXPERIMENT\n",
    "        },\n",
    "        'regression': {\n",
    "            'model_name': best_reg_name,\n",
    "            'model_key': best_reg_key,\n",
    "            'test_r2': float(best_reg_r2),\n",
    "            'features_count': len(reg_features),\n",
    "            'needs_scaling': best_reg_key == 'lr',\n",
    "            'experiment_name': REGRESSION_EXPERIMENT\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('model_metadata.pkl', 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "    print(f\"✓ Exported: model_metadata.pkl\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EXPORT SUMMARY:\")\n",
    "    print(f\"  Classification: {best_clf_name} ({best_clf_key}) | Accuracy: {best_clf_accuracy:.4f}\")\n",
    "    print(f\"  Regression: {best_reg_name} ({best_reg_key}) | R²: {best_reg_r2:.4f}\")\n",
    "    print(f\"  Classification Experiment: {CLASSIFICATION_EXPERIMENT}\")\n",
    "    print(f\"  Regression Experiment: {REGRESSION_EXPERIMENT}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return metadata, best_clf, best_reg, best_clf_key, best_reg_key\n",
    "\n",
    "# Execute Export\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"EXECUTING: Export Best Models\")\n",
    "print(\"=\"*40)\n",
    "metadata, best_clf_model, best_reg_model, best_clf_key, best_reg_key = export_best_models(\n",
    "    clf_results, reg_results, clf_models, reg_models,\n",
    "    clf_features, reg_features, scaler_clf, scaler_reg\n",
    ")\n",
    "\n",
    "print(\"\\n MODELS EXPORTED SUCCESSFULLY!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dd37d5",
   "metadata": {
    "papermill": {
     "duration": 2778.174642,
     "end_time": "2025-12-07T13:05:51.042022",
     "exception": false,
     "start_time": "2025-12-07T12:19:32.867380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_shap_analysis(best_clf, best_reg, X_test_clf, X_test_reg, \n",
    "                           clf_features, reg_features, best_clf_key, best_reg_key):\n",
    "    \"\"\"Generate SHAP values for model interpretability with proper interpretation\"\"\"\n",
    "\n",
    "    # ================================\n",
    "    # 1. CLASSIFICATION SHAP ANALYSIS\n",
    "    # ================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"1. CLASSIFICATION SHAP ANALYSIS (Investment Quality)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    if best_clf_key in ['rf', 'xgb']:\n",
    "        explainer_clf = shap.TreeExplainer(best_clf)\n",
    "        shap_values_clf = explainer_clf.shap_values(X_test_clf)\n",
    "\n",
    "        # Handle binary vs multiclass\n",
    "        if isinstance(shap_values_clf, list):\n",
    "            shap_values_clf = shap_values_clf[-1]  # positive class\n",
    "\n",
    "        # Static SHAP Summary Plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.summary_plot(shap_values_clf, X_test_clf, show=False, max_display=15)\n",
    "        plt.title('SHAP Feature Importance - Classification (Good Investment)', \n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.tight_layout()\n",
    "        print(\"   ✓ Saved: shap_classification_summary.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # SHAP Feature Importance Table\n",
    "        shap_importance_clf = pd.DataFrame({\n",
    "            'Feature': clf_features,\n",
    "            'SHAP_Value': np.abs(shap_values_clf).mean(axis=0)\n",
    "        }).sort_values('SHAP_Value', ascending=False)\n",
    "\n",
    "        print(\"\\n Top 10 Important Features (Classification):\")\n",
    "        for idx, row in shap_importance_clf.head(10).iterrows():\n",
    "            print(f\"      {row['Feature'][:40]:40s} | {row['SHAP_Value']:.4f}\")\n",
    "\n",
    "        # Export SHAP (only values, not explainer - explainer can't be pickled)\n",
    "        with open('shap_values_classification.pkl', 'wb') as f:\n",
    "            pickle.dump({'shap_values': shap_values_clf}, f)\n",
    "        print(\"   ✓ Exported: shap_values_classification.pkl\")\n",
    "\n",
    "        # Plotly Visualization\n",
    "        fig_clf = px.bar(\n",
    "            shap_importance_clf.head(20),\n",
    "            x='SHAP_Value',\n",
    "            y='Feature',\n",
    "            orientation='h',\n",
    "            title=\"Top 20 Features Predicting Good Investment Properties\"\n",
    "        )\n",
    "        fig_clf.update_layout(height=600)\n",
    "        fig_clf.show()\n",
    "\n",
    "\n",
    "    # ============================\n",
    "    # 2. REGRESSION SHAP ANALYSIS\n",
    "    # ============================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"2. REGRESSION SHAP ANALYSIS (Future Price Forecasting)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n INTERPRETATION GUIDE:\")\n",
    "    print(\"   • High SHAP for 'Price_in_Lakhs' is EXPECTED and VALID\")\n",
    "    print(\"   • Current price is the strongest predictor of future price\")\n",
    "    print(\"   • Other features show which properties appreciate FASTER\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Convert test data to DataFrame\n",
    "    X_test_reg_df = pd.DataFrame(X_test_reg, columns=reg_features)\n",
    "\n",
    "    # Tree models (RF, XGB)\n",
    "    if \"rf\" in best_reg_key.lower() or \"xgb\" in best_reg_key.lower():\n",
    "        explainer_reg = shap.TreeExplainer(best_reg)\n",
    "        shap_values_reg = explainer_reg.shap_values(X_test_reg_df)\n",
    "\n",
    "    # Linear Regression\n",
    "    elif best_reg_key.lower() == \"lr\":\n",
    "        print(\"⏳ Using KernelExplainer for Linear Regression (this may take a moment)...\")\n",
    "        background = X_test_reg_df.sample(min(50, len(X_test_reg_df)), random_state=42)\n",
    "        explainer_reg = shap.KernelExplainer(best_reg.predict, background)\n",
    "        shap_values_reg = explainer_reg.shap_values(X_test_reg_df, nsamples=100)\n",
    "\n",
    "    else:\n",
    "        print(f\"SHAP not supported for regression model key: {best_reg_key}\")\n",
    "        return None, None\n",
    "\n",
    "    # Summary Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values_reg, X_test_reg_df, show=False, max_display=15)\n",
    "    plt.title(\"SHAP Feature Importance - Regression (Future Price Prediction)\", \n",
    "             fontsize=16, fontweight=\"bold\", pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.close()\n",
    "    print(\"\\n✓ Saved: shap_regression_summary.png\")\n",
    "\n",
    "    # Importance Table\n",
    "    shap_importance_reg = pd.DataFrame({\n",
    "        \"Feature\": reg_features,\n",
    "        \"SHAP_Value\": np.abs(shap_values_reg).mean(axis=0)\n",
    "    }).sort_values(\"SHAP_Value\", ascending=False)\n",
    "\n",
    "    print(\"\\n Top 10 Important Features (Regression):\")\n",
    "    for idx, row in shap_importance_reg.head(10).iterrows():\n",
    "        feat = row['Feature']\n",
    "        val = row['SHAP_Value']\n",
    "        \n",
    "        # Add interpretation for price feature\n",
    "        if 'price' in feat.lower():\n",
    "            print(f\"   {feat[:40]:40s} | {val:>12,.2f} (Base price - expected)\")\n",
    "        else:\n",
    "            print(f\"   {feat[:40]:40s} | {val:>12,.2f}\")\n",
    "\n",
    "    # Export SHAP (only values, not explainer - explainer can't be pickled)\n",
    "    with open('shap_values_regression.pkl', 'wb') as f:\n",
    "        pickle.dump({'shap_values': shap_values_reg}, f)\n",
    "    print(\"\\n✓ Exported: shap_values_regression.pkl\")\n",
    "\n",
    "    # Plotly Visualization\n",
    "    fig_reg = px.bar(\n",
    "        shap_importance_reg.head(20),\n",
    "        x=\"SHAP_Value\",\n",
    "        y=\"Feature\",\n",
    "        orientation=\"h\",\n",
    "        title=\"Top 20 Features for Future Price Prediction (5 Years)\"\n",
    "    )\n",
    "    fig_reg.update_layout(height=600)\n",
    "    fig_reg.show()\n",
    "\n",
    "    # Business Insights\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BUSINESS INSIGHTS FROM SHAP:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Identify top non-price features\n",
    "    non_price_features = shap_importance_reg[\n",
    "        ~shap_importance_reg['Feature'].str.contains('price', case=False, na=False)\n",
    "    ].head(5)\n",
    "    \n",
    "    print(\"\\n Key Drivers of Property Appreciation (Beyond Current Price):\")\n",
    "    for i, (idx, row) in enumerate(non_price_features.iterrows(), 1):\n",
    "        print(f\"   {i}. {row['Feature']}\")\n",
    "    \n",
    "    print(\"\\n Investment Strategy:\")\n",
    "    print(\"   • Focus on properties with high scores in above features\")\n",
    "    print(\"   • These factors drive FASTER appreciation than market average\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    return shap_importance_clf, shap_importance_reg\n",
    "\n",
    "\n",
    "# Execute SHAP Analysis\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"EXECUTING: SHAP Analysis\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "shap_clf, shap_reg = generate_shap_analysis(\n",
    "    best_clf_model, best_reg_model,\n",
    "    X_test_clf, X_test_reg,\n",
    "    clf_features, reg_features,\n",
    "    best_clf_key, best_reg_key\n",
    ")\n",
    "\n",
    "print(\"\\n✅ SHAP analysis completed with proper interpretation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c5c6c8",
   "metadata": {
    "papermill": {
     "duration": 0.456294,
     "end_time": "2025-12-07T13:05:51.963204",
     "exception": false,
     "start_time": "2025-12-07T13:05:51.506910",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a24a1d",
   "metadata": {
    "papermill": {
     "duration": 0.690874,
     "end_time": "2025-12-07T13:05:53.107133",
     "exception": false,
     "start_time": "2025-12-07T13:05:52.416259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 2: LOAD SAVED MODELS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Step 2: Loading saved models and artifacts...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Load models\n",
    "    with open('best_classification_model.pkl', 'rb') as f:\n",
    "        loaded_clf_model = pickle.load(f)\n",
    "    print(\"   ✓ Classification model loaded\")\n",
    "    \n",
    "    with open('best_regression_model.pkl', 'rb') as f:\n",
    "        loaded_reg_model = pickle.load(f)\n",
    "    print(\"   ✓ Regression model loaded\")\n",
    "    \n",
    "    # Load features\n",
    "    with open('classification_features.pkl', 'rb') as f:\n",
    "        loaded_clf_features = pickle.load(f)\n",
    "    print(f\"   ✓ Classification features: {len(loaded_clf_features)}\")\n",
    "    \n",
    "    with open('regression_features.pkl', 'rb') as f:\n",
    "        loaded_reg_features = pickle.load(f)\n",
    "    print(f\"   ✓ Regression features: {len(loaded_reg_features)}\")\n",
    "    \n",
    "    # Load metadata\n",
    "    with open('model_metadata.pkl', 'rb') as f:\n",
    "        loaded_metadata = pickle.load(f)\n",
    "    print(\"   ✓ Metadata loaded\")\n",
    "    \n",
    "    # Load scalers if they exist\n",
    "    try:\n",
    "        with open('classification_scaler.pkl', 'rb') as f:\n",
    "            loaded_clf_scaler = pickle.load(f)\n",
    "        print(\"   ✓ Classification scaler loaded\")\n",
    "    except FileNotFoundError:\n",
    "        loaded_clf_scaler = None\n",
    "        print(\"No classification scaler (tree-based model)\")\n",
    "    \n",
    "    try:\n",
    "        with open('regression_scaler.pkl', 'rb') as f:\n",
    "            loaded_reg_scaler = pickle.load(f)\n",
    "        print(\"   ✓ Regression scaler loaded\")\n",
    "    except FileNotFoundError:\n",
    "        loaded_reg_scaler = None\n",
    "        print(\"No regression scaler (tree-based model)\")\n",
    "    \n",
    "    print(\"\\n All artifacts loaded successfully!\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n ERROR: {e}\")\n",
    "    print(\"Please run the export_best_models() function first!\")\n",
    "    raise\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: DISPLAY MODEL INFORMATION\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADED MODEL INFORMATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n Classification Model:\")\n",
    "print(f\"   Type: {loaded_metadata['classification']['model_name']}\")\n",
    "print(f\"   Test Accuracy: {loaded_metadata['classification']['test_accuracy']:.4f}\")\n",
    "print(f\"   Features: {loaded_metadata['classification']['features_count']}\")\n",
    "print(f\"   Needs Scaling: {loaded_metadata['classification']['needs_scaling']}\")\n",
    "\n",
    "print(f\"\\n Regression Model:\")\n",
    "print(f\"   Type: {loaded_metadata['regression']['model_name']}\")\n",
    "print(f\"   Test R²: {loaded_metadata['regression']['test_r2']:.4f}\")\n",
    "print(f\"   Features: {loaded_metadata['regression']['features_count']}\")\n",
    "print(f\"   Needs Scaling: {loaded_metadata['regression']['needs_scaling']}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: PREPARE TEST SAMPLE\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Step 3: Preparing test sample (20 properties)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_sample_size = 20\n",
    "\n",
    "# Classification test sample\n",
    "clf_test_sample = X_test_clf.head(test_sample_size).copy()\n",
    "clf_test_labels = y_test_clf.head(test_sample_size).copy()\n",
    "\n",
    "# Regression test sample  \n",
    "reg_test_sample = X_test_reg.head(test_sample_size).copy()\n",
    "reg_test_labels = y_test_reg.head(test_sample_size).copy()\n",
    "\n",
    "# Get current prices for regression\n",
    "test_indices = reg_test_sample.index\n",
    "if 'Price_in_Lakhs' in df_transformed.columns:\n",
    "    current_prices = df_transformed.loc[test_indices, 'Price_in_Lakhs'].values\n",
    "else:\n",
    "    # Estimate from future price (assuming ~40% appreciation)\n",
    "    current_prices = reg_test_labels.values / 1.4\n",
    "\n",
    "print(f\"   ✓ Classification sample: {clf_test_sample.shape}\")\n",
    "print(f\"   ✓ Regression sample: {reg_test_sample.shape}\")\n",
    "print(f\"   ✓ Current prices range: ₹{current_prices.min():.2f}L - ₹{current_prices.max():.2f}L\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 5: CLASSIFICATION PREDICTIONS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Step 4: CLASSIFICATION PREDICTIONS (Investment Quality)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Apply scaling if needed\n",
    "if loaded_metadata['classification']['needs_scaling'] and loaded_clf_scaler is not None:\n",
    "    clf_test_scaled = pd.DataFrame(\n",
    "        loaded_clf_scaler.transform(clf_test_sample),\n",
    "        columns=clf_test_sample.columns,\n",
    "        index=clf_test_sample.index\n",
    "    )\n",
    "    clf_predictions = loaded_clf_model.predict(clf_test_scaled)\n",
    "    clf_probabilities = loaded_clf_model.predict_proba(clf_test_scaled)\n",
    "    print(\"Applied scaling (Logistic Regression)\")\n",
    "else:\n",
    "    clf_predictions = loaded_clf_model.predict(clf_test_sample)\n",
    "    clf_probabilities = loaded_clf_model.predict_proba(clf_test_sample)\n",
    "    print(\"No scaling needed (Tree-based model)\")\n",
    "\n",
    "# Create results dataframe\n",
    "clf_results_df = pd.DataFrame({\n",
    "    'Property_ID': range(1, test_sample_size + 1),\n",
    "    'Actual': clf_test_labels.values,\n",
    "    'Predicted': clf_predictions,\n",
    "    'Confidence_Bad': clf_probabilities[:, 0] * 100,\n",
    "    'Confidence_Good': clf_probabilities[:, 1] * 100\n",
    "})\n",
    "\n",
    "# Add labels\n",
    "clf_results_df['Actual_Label'] = clf_results_df['Actual'].map({0: 'Not Good', 1: 'Good'})\n",
    "clf_results_df['Predicted_Label'] = clf_results_df['Predicted'].map({0: 'Not Good', 1: 'Good'})\n",
    "clf_results_df['Match'] = np.where(\n",
    "    clf_results_df['Actual'] == clf_results_df['Predicted'], \n",
    "    'Correct', \n",
    "    'Wrong'\n",
    ")\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = (clf_results_df['Actual'] == clf_results_df['Predicted']).mean()\n",
    "\n",
    "print(f\"\\n Classification Results:\")\n",
    "print(f\"   Test Accuracy: {test_accuracy:.2%}\")\n",
    "print(f\"   Correct: {(clf_results_df['Actual'] == clf_results_df['Predicted']).sum()}/{test_sample_size}\")\n",
    "print(f\"   Incorrect: {(clf_results_df['Actual'] != clf_results_df['Predicted']).sum()}/{test_sample_size}\")\n",
    "\n",
    "print(f\"\\n Sample Predictions (First 10 Properties):\")\n",
    "print(\"=\"*100)\n",
    "display_df = clf_results_df[['Property_ID', 'Actual_Label', 'Predicted_Label', \n",
    "                              'Confidence_Good', 'Match']].head(10)\n",
    "display_df['Confidence_Good'] = display_df['Confidence_Good'].map('{:.1f}%'.format)\n",
    "print(display_df.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 6: REGRESSION PREDICTIONS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Step 5: REGRESSION PREDICTIONS (Future Price Forecast - 5 Years)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Apply scaling if needed\n",
    "if loaded_metadata['regression']['needs_scaling'] and loaded_reg_scaler is not None:\n",
    "    reg_test_scaled = pd.DataFrame(\n",
    "        loaded_reg_scaler.transform(reg_test_sample),\n",
    "        columns=reg_test_sample.columns,\n",
    "        index=reg_test_sample.index\n",
    "    )\n",
    "    reg_predictions = loaded_reg_model.predict(reg_test_scaled)\n",
    "    print(\"Applied scaling (Ridge Regression)\")\n",
    "else:\n",
    "    reg_predictions = loaded_reg_model.predict(reg_test_sample)\n",
    "    print(\"No scaling needed (Tree-based model)\")\n",
    "\n",
    "# Create results dataframe\n",
    "reg_results_df = pd.DataFrame({\n",
    "    'Property_ID': range(1, test_sample_size + 1),\n",
    "    'Current_Price': current_prices,\n",
    "    'Actual_Future': reg_test_labels.values,\n",
    "    'Predicted_Future': reg_predictions\n",
    "})\n",
    "\n",
    "# Calculate appreciation metrics\n",
    "reg_results_df['Actual_Appreciation_%'] = (\n",
    "    (reg_results_df['Actual_Future'] / reg_results_df['Current_Price'] - 1) * 100\n",
    ")\n",
    "reg_results_df['Predicted_Appreciation_%'] = (\n",
    "    (reg_results_df['Predicted_Future'] / reg_results_df['Current_Price'] - 1) * 100\n",
    ")\n",
    "reg_results_df['Error_Lakhs'] = (\n",
    "    reg_results_df['Predicted_Future'] - reg_results_df['Actual_Future']\n",
    ")\n",
    "reg_results_df['Abs_Error_%'] = (\n",
    "    abs(reg_results_df['Error_Lakhs']) / reg_results_df['Actual_Future'] * 100\n",
    ")\n",
    "\n",
    "# Categorize predictions\n",
    "reg_results_df['Accuracy_Rating'] = pd.cut(\n",
    "    reg_results_df['Abs_Error_%'],\n",
    "    bins=[0, 5, 10, 15, 100],\n",
    "    labels=['🟢 Excellent (<5%)', '🟡 Good (5-10%)', '🟠 Fair (10-15%)', '🔴 Poor (>15%)']\n",
    ")\n",
    "\n",
    "# Calculate metrics\n",
    "mae_lakhs = reg_results_df['Error_Lakhs'].abs().mean()\n",
    "mae_percent = reg_results_df['Abs_Error_%'].mean()\n",
    "mape = (abs(reg_results_df['Predicted_Future'] - reg_results_df['Actual_Future']) / \n",
    "        reg_results_df['Actual_Future']).mean() * 100\n",
    "\n",
    "print(f\"\\n Regression Results:\")\n",
    "print(f\"   Mean Absolute Error: ₹{mae_lakhs:.2f} Lakhs\")\n",
    "print(f\"   Mean Absolute % Error: {mae_percent:.2f}%\")\n",
    "print(f\"   MAPE: {mape:.2f}%\")\n",
    "print(f\"   Avg Actual Appreciation: {reg_results_df['Actual_Appreciation_%'].mean():.2f}%\")\n",
    "print(f\"   Avg Predicted Appreciation: {reg_results_df['Predicted_Appreciation_%'].mean():.2f}%\")\n",
    "\n",
    "print(f\"\\n Sample Predictions (First 10 Properties):\")\n",
    "print(\"=\"*120)\n",
    "display_cols = ['Property_ID', 'Current_Price', 'Predicted_Future', \n",
    "                'Predicted_Appreciation_%', 'Abs_Error_%', 'Accuracy_Rating']\n",
    "display_df = reg_results_df[display_cols].head(10)\n",
    "display_df['Current_Price'] = display_df['Current_Price'].map('₹{:.2f}L'.format)\n",
    "display_df['Predicted_Future'] = display_df['Predicted_Future'].map('₹{:.2f}L'.format)\n",
    "display_df['Predicted_Appreciation_%'] = display_df['Predicted_Appreciation_%'].map('{:.1f}%'.format)\n",
    "display_df['Abs_Error_%'] = display_df['Abs_Error_%'].map('{:.1f}%'.format)\n",
    "print(display_df.to_string(index=False))\n",
    "print(\"=\"*120)\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 7: VISUALIZING TEST PREDICTIONS (PLOTLY VERSION - FIXED)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Step 6: Visualizing Test Predictions (Plotly Interactive)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.subplots as sp\n",
    "\n",
    "# Define correct_mask BEFORE using it\n",
    "correct_mask = clf_results_df['Actual'] == clf_results_df['Predicted']\n",
    "\n",
    "# Create 2x2 subplot grid\n",
    "fig = sp.make_subplots(\n",
    "    rows=3, cols=2,\n",
    "    subplot_titles=[\n",
    "        \"Classification Confusion Matrix\",\n",
    "        \"Prediction Confidence Distribution\",\n",
    "        \"Regression: Actual vs Predicted\",\n",
    "        \"Regression Error Distribution\",\n",
    "        \"5-Year Appreciation: Actual vs Predicted\", \"\"\n",
    "    ],\n",
    "    specs=[\n",
    "        [{\"type\": \"heatmap\"}, {\"type\": \"histogram\"}],\n",
    "        [{\"type\": \"scatter\"}, {\"type\": \"histogram\"}],\n",
    "        [{\"colspan\": 2, \"type\": \"bar\"}, None]\n",
    "    ],\n",
    "    vertical_spacing=0.18,\n",
    "    horizontal_spacing=0.12\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# 1️⃣ CONFUSION MATRIX\n",
    "# --------------------------\n",
    "cm = confusion_matrix(clf_results_df['Actual'], clf_results_df['Predicted'])\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=cm,\n",
    "        x=[\"Not Good\", \"Good\"],\n",
    "        y=[\"Not Good\", \"Good\"],\n",
    "        colorscale=\"Blues\",\n",
    "        showscale=True,\n",
    "        text=cm,\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 16}\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# 2️⃣ CONFIDENCE HISTOGRAM\n",
    "# --------------------------\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=clf_results_df[correct_mask]['Confidence_Good'],\n",
    "        name=\"Correct Predictions\",\n",
    "        opacity=0.7,\n",
    "        marker=dict(color='green')\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=clf_results_df[~correct_mask]['Confidence_Good'],\n",
    "        name=\"Wrong Predictions\",\n",
    "        opacity=0.7,\n",
    "        marker=dict(color='red')\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# 3️⃣ REGRESSION ACTUAL VS PREDICTED\n",
    "# --------------------------\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=reg_results_df['Actual_Future'],\n",
    "        y=reg_results_df['Predicted_Future'],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(\n",
    "            size=10,\n",
    "            color=reg_results_df['Abs_Error_%'],\n",
    "            colorscale=\"RdYlGn_r\",\n",
    "            colorbar=dict(title=\"Error %\"),\n",
    "            line=dict(width=1, color=\"black\")\n",
    "        ),\n",
    "        text=[f\"Error: {e:.2f}%\" for e in reg_results_df['Abs_Error_%']],\n",
    "        hovertemplate=\"<b>Actual</b>: ₹%{x:.2f}L<br>\"\n",
    "                      \"<b>Predicted</b>: ₹%{y:.2f}L<br>\"\n",
    "                      \"%{text}<extra></extra>\",\n",
    "        name=\"Properties\"\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Perfect prediction line\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[reg_results_df['Actual_Future'].min(),\n",
    "           reg_results_df['Actual_Future'].max()],\n",
    "        y=[reg_results_df['Actual_Future'].min(),\n",
    "           reg_results_df['Actual_Future'].max()],\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"red\", dash=\"dash\", width=2),\n",
    "        name=\"Perfect Prediction\",\n",
    "        showlegend=True\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# 4️⃣ ERROR DISTRIBUTION\n",
    "# --------------------------\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=reg_results_df['Error_Lakhs'],\n",
    "        marker=dict(color='orange', line=dict(width=1, color='black')),\n",
    "        opacity=0.75,\n",
    "        name=\"Prediction Errors\"\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Add vertical line at zero\n",
    "fig.add_vline(x=0, line_dash=\"dash\", line_color=\"red\", row=2, col=2)\n",
    "\n",
    "# --------------------------\n",
    "# 5️⃣ APPRECIATION BAR CHART\n",
    "# --------------------------\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=reg_results_df['Property_ID'],\n",
    "        y=reg_results_df['Actual_Appreciation_%'],\n",
    "        name=\"Actual Appreciation\",\n",
    "        marker=dict(color='steelblue')\n",
    "    ),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=reg_results_df['Property_ID'],\n",
    "        y=reg_results_df['Predicted_Appreciation_%'],\n",
    "        name=\"Predicted Appreciation\",\n",
    "        marker=dict(color='coral')\n",
    "    ),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# LAYOUT UPDATES\n",
    "# --------------------------\n",
    "fig.update_xaxes(title_text=\"Actual Label\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Predicted Label\", row=1, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text=\"Confidence for 'Good' (%)\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Count\", row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text=\"Actual Future Price (Lakhs)\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Predicted Future Price (Lakhs)\", row=2, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text=\"Prediction Error (Lakhs)\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"Frequency\", row=2, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text=\"Property ID\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"Appreciation (%)\", row=3, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Model Test Predictions — Real Estate Investment Analysis\",\n",
    "        'x': 0.5,\n",
    "        'xanchor': 'center',\n",
    "        'font': {'size': 20, 'family': 'Arial Black'}\n",
    "    },\n",
    "    height=1600,\n",
    "    showlegend=True,\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.08, xanchor=\"center\", x=0.5),\n",
    "    barmode='group'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"✓ Interactive visualization created successfully!\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 8: INVESTMENT RECOMMENDATIONS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Step 7: INVESTMENT RECOMMENDATIONS (Top 5 Properties)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Combine both predictions\n",
    "combined_df = pd.DataFrame({\n",
    "    'Property_ID': range(1, test_sample_size + 1),\n",
    "    'Current_Price': current_prices,\n",
    "    'Predicted_Future_Price': reg_predictions,\n",
    "    'Predicted_Appreciation_%': reg_results_df['Predicted_Appreciation_%'],\n",
    "    'Investment_Quality': clf_predictions,\n",
    "    'Investment_Confidence_%': clf_probabilities[:, 1] * 100\n",
    "})\n",
    "\n",
    "# Calculate investment score (weighted combination)\n",
    "combined_df['Investment_Score'] = (\n",
    "    (combined_df['Investment_Quality'] * 40) +  # 40% weight on quality\n",
    "    (combined_df['Investment_Confidence_%'] * 0.3) +  # 30% weight on confidence\n",
    "    (combined_df['Predicted_Appreciation_%'] * 0.3)  # 30% weight on appreciation\n",
    ")\n",
    "\n",
    "# Get top 5 recommendations\n",
    "top_5 = combined_df.nlargest(5, 'Investment_Score')\n",
    "\n",
    "print(\"\\n TOP 5 INVESTMENT RECOMMENDATIONS:\")\n",
    "print(\"=\"*120)\n",
    "for idx, (_, row) in enumerate(top_5.iterrows(), 1):\n",
    "    print(f\"\\n{idx}. Property #{int(row['Property_ID'])}\")\n",
    "    print(f\"   Current Price: ₹{row['Current_Price']:.2f} Lakhs\")\n",
    "    print(f\"   Future Price (5Y): ₹{row['Predicted_Future_Price']:.2f} Lakhs\")\n",
    "    print(f\"   Expected Appreciation: {row['Predicted_Appreciation_%']:.1f}%\")\n",
    "    print(f\"   Investment Quality: {'GOOD ✓' if row['Investment_Quality'] == 1 else 'NOT RECOMMENDED ✗'}\")\n",
    "    print(f\"   Confidence: {row['Investment_Confidence_%']:.1f}%\")\n",
    "    print(f\"   Overall Score: {row['Investment_Score']:.2f}/100\")\n",
    "    print(\"-\" * 120)\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"MODEL VALIDATION COMPLETE - ALL TESTS PASSED!\")\n",
    "print(\"=\"*120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d06930a",
   "metadata": {
    "papermill": {
     "duration": 0.460078,
     "end_time": "2025-12-07T13:05:54.931316",
     "exception": false,
     "start_time": "2025-12-07T13:05:54.471238",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f60092",
   "metadata": {},
   "source": [
    "1. The project effectively integrates both classification and regression models to support comprehensive real estate investment analysis.\n",
    "2. The classification model provides data-driven insights into whether a property qualifies as a good investment.\n",
    "3. The regression model forecasts the future price of a property over a five-year period using current price and market indicators.\n",
    "4. The combined outputs enable users to evaluate both short-term investment quality and long-term financial growth.\n",
    "5. Extensive feature engineering, preprocessing, and model optimization ensure accurate, stable, and reliable predictions.\n",
    "6. SHAP analysis and feature importance visualizations enhance the interpretability and transparency of the model outputs.\n",
    "7. Market insights and visual analytics help users understand property trends, patterns, and influencing factors.\n",
    "8. The Streamlit application consolidates all functionalities into an accessible, user-friendly interface.\n",
    "9. Single-prediction and bulk-prediction tabs support both individual investors and business users handling large datasets.\n",
    "10. The Feature Importance & SHAP tab improves trust by explaining how each feature contributes to the model’s decisions.\n",
    "11. The solution is scalable, making it suitable for real estate agencies, analysts, and investment platforms.\n",
    "12. Overall, the project delivers a complete, explainable, and practical real estate advisory system that empowers users to make confident, data-backed investment decisions.\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8934495,
     "sourceId": 14030522,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Amazon Delivery ML",
   "language": "python",
   "name": "amazon_delivery"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3046.195143,
   "end_time": "2025-12-07T13:05:59.746182",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-07T12:15:13.551039",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "02333a945a7b49deb8d2c30f6bf05015": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1a24bc15135d4e42a2ef3bc290c0ac0c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c970a7bda10e47cf94aa0461f3628eae",
       "placeholder": "​",
       "style": "IPY_MODEL_02333a945a7b49deb8d2c30f6bf05015",
       "tabbable": null,
       "tooltip": null,
       "value": " 50000/50000 [45:25&lt;00:00, 18.17it/s]"
      }
     },
     "34ce1fd5d1ec4d72a9c99bd75d497e73": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7cf437d6e0fd443db75debe1c604068d",
       "placeholder": "​",
       "style": "IPY_MODEL_c498f373563d4266812b5dfff5af6e94",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "7cf437d6e0fd443db75debe1c604068d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c498f373563d4266812b5dfff5af6e94": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c970a7bda10e47cf94aa0461f3628eae": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dce565750b984be682dbbdee9936dd33": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "de03c1b7b4ef4de69960442d4fcc28c8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e09f401ebb8146a184734253e0291215",
       "max": 50000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f36576cbf3204be198a88a936dcd73d5",
       "tabbable": null,
       "tooltip": null,
       "value": 50000
      }
     },
     "e09f401ebb8146a184734253e0291215": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f14095ef48fb4812b0015d03df2dd17e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_34ce1fd5d1ec4d72a9c99bd75d497e73",
        "IPY_MODEL_de03c1b7b4ef4de69960442d4fcc28c8",
        "IPY_MODEL_1a24bc15135d4e42a2ef3bc290c0ac0c"
       ],
       "layout": "IPY_MODEL_dce565750b984be682dbbdee9936dd33",
       "tabbable": null,
       "tooltip": null
      }
     },
     "f36576cbf3204be198a88a936dcd73d5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

